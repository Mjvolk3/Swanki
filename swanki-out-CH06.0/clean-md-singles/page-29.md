Figure 6.16 (a) A two-link robot arm, in which the Cartesian coordinates $\left(x_{1}, x_{2}\right)$ of the end effector are determined uniquely by the two joint angles $\theta_{1}$ and $\theta_{2}$ and the (fixed) lengths $L_{1}$ and $L_{2}$ of the arms. This is known as the forward kinematics of the arm. (b) In practice, we have to find the joint angles that will give rise to a desired end effector position. This inverse kinematics has two solutions corresponding to 'el-

![](https://cdn.mathpix.com/cropped/2024_05_26_cf46115da84aa2e9c64eg-1.jpg?height=354&width=357&top_left_y=219&top_left_x=679)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_26_cf46115da84aa2e9c64eg-1.jpg?height=364&width=364&top_left_y=219&top_left_x=1127)

(b) bow up' and 'elbow down'.

In the robotics example, the kinematics is defined by geometrical equations, and the multimodality is readily apparent. However, in many machine learning problems the presence of multimodality, particularly in problems involving spaces of high dimensionality, can be less obvious. For tutorial purposes, however, we will consider a simple toy problem for which we can easily visualize the multimodality. The data for this problem is generated by sampling a variable $x$ uniformly over the interval $(0,1)$, to give a set of values $\left\{x_{n}\right\}$, and the corresponding target values $t_{n}$ are obtained by computing the function $x_{n}+0.3 \sin \left(2 \pi x_{n}\right)$ and then adding uniform noise over the interval $(-0.1,0.1)$. The inverse problem is then obtained by keeping the same data points but exchanging the roles of $x$ and $t$. Figure 6.17 shows the data sets for the forward and inverse problems, along with the results of fitting two-layer neural networks having six hidden units and a single linear output unit by minimizing a sum-of-squares error function. Least squares corresponds to maximum likelihood under a Gaussian assumption. We see that this leads to a good model for the forward problem but a very poor model for the highly non-Gaussian inverse problem.

\title{
6.5.2 Conditional mixture distributions
}

We therefore seek a general framework for modelling conditional probability distributions. This can be achieved by using a mixture model for $p(\mathbf{t} \mid \mathbf{x})$ in which

Figure 6.17 On the left is the data set for a simple forward problem in which the red curve shows the result of fitting a two-layer neural network by minimizing the sum-of-squares error function. The corresponding inverse problem, shown on the right, is obtained by exchanging the roles of $x$ and $t$. Here the same network, again trained by minimizing the sumof-squares error function, gives a poor fit to the data due to the multimodality of the data set.
![](https://cdn.mathpix.com/cropped/2024_05_26_cf46115da84aa2e9c64eg-1.jpg?height=470&width=984&top_left_y=1616&top_left_x=640)
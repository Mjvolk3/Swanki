## Explanation of neural network training results with given data distribution

![](https://cdn.mathpix.com/cropped/2024_05_26_53b5c38c9dec90db1928g-1.jpg?height=391&width=493&top_left_y=232&top_left_x=1131)

%

In the given image, 50 data points (blue dots) are uniformly sampled in $x$ over the interval $(-1,1)$, and their corresponding $f(x)$ values are used to train a two-layer network with three hidden units that use tanh activation functions. The red curves illustrate the network function while the dashed curves represent the outputs of the three hidden units.

- #neural-networks, #activation-functions.tanh, #machine-learning

## Universal approximation theorem in neural networks

![](https://cdn.mathpix.com/cropped/2024_05_26_53b5c38c9dec90db1928g-1.jpg?height=391&width=493&top_left_y=232&top_left_x=1131)

%

The universal approximation theorem indicates that two-layer feed-forward networks with a wide range of activation functions can approximate any function defined over a continuous subset of $\mathbb{R}^D$ to arbitrary accuracy. This is valid for both continuous and finite-dimensional discrete spaces.

- #neural-networks, #universal-approximator, #theoryraf დაახლოებით neural-networks
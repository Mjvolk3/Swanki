## What is depicted in Figure 6.11 in terms of the neural network's performance and the decision boundaries?

![](https://cdn.mathpix.com/cropped/2024_05_26_69d949d0ac2b0e71376dg-1.jpg?height=523&width=650&top_left_y=232&top_left_x=955)

%

Figure 6.11 depicts a scatter plot with synthetic data for a two-class classification problem. The neural network, which has two inputs, two hidden units with tanh activation functions, and a single output with a logistic-sigmoid activation function, is evaluated based on its performance in separating the two classes. Key features include:

- **Dashed blue lines**: $z = 0.5$ contours for the hidden units.
- **Solid red line**: $y = 0.5$ decision surface of the neural network.
- **Solid green line**: Optimal decision boundary calculated from the data distributions.

These elements illustrate the network's learned decision boundary and its comparison to the optimal boundary.

- neural-network.classification, activation-function.tanh, decision-boundary.comparison

---

## What do the dashed blue lines, solid red line, and solid green line in Figure 6.11 represent?

![](https://cdn.mathpix.com/cropped/2024_05_26_69d949d0ac2b0e71376dg-1.jpg?height=523&width=650&top_left_y=232&top_left_x=955)

%

In Figure 6.11:

- The **dashed blue lines** represent the $z = 0.5$ contours for each of the hidden units with tanh activation functions.
- The **solid red line** indicates the $y = 0.5$ decision surface, which is the boundary used by the neural network to differentiate between the two classes.
- The **solid green line** denotes the optimal decision boundary computed from the distributions used to generate the synthetic data.

These lines demonstrate how the neural networkâ€™s decision boundary compares to the theoretically optimal boundary.

- neural-network.classification, activation-function.tanh, decision-boundary.optimization
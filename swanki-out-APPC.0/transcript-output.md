# Appendix C. Lagrange Multipliers

Lagrange multipliers, sometimes referred to as undetermined multipliers, are a powerful mathematical technique used to identify the stationary points of a function of several variables when those variables are subject to one or more constraints. For instance, consider trying to find the maximum of a function of two variables, denoted as $f$ of $x_1$ and $x_2$, while being subject to a constraint that relates $x_1$ and $x_2$, which can be expressed as $g$ of $x_1$ and $x_2$ equals zero.

One traditional approach to solve this involves expressing one variable in terms of the other using the constraint equation. For example, if $g$ of $x_1$ and $x_2$ equals zero, we can solve for $x_2$ as a function of $x_1$, say $x_2$ equals $h$ of $x_1$. Substituting this back into $f$, we now have a function of $x_1$ alone, $f$ of $x_1$ and $h$ of $x_1$. We then find the maximum of this new function by differentiating it with respect to $x_1$ and finding where this derivative equals zero. This gives a stationary value for $x_1$, denoted $x_1^\star$, and the corresponding value for $x_2$ is given by $x_2^\star$ equals $h$ of $x_1^\star$. However, this method can be cumbersome and non-symmetric, as it treats $x_1$ and $x_2$ differently.

A more elegant solution is to introduce a parameter, $\lambda$, known as a Lagrange multiplier. To understand this method geometrically, consider a $D$-dimensional variable $\mathbf{x}$ with components $x_1$ through $x_D$. The constraint $g$ of $\mathbf{x}$ equals zero represents a $(D-1)$-dimensional surface in the space of $\mathbf{x}$. At any point on this surface, the gradient of $g$, denoted $\nabla g$ of $\mathbf{x}$, is orthogonal to the surface. This orthogonality can be visualized as follows: if we take a point $\mathbf{x}$ on the constraint surface and a nearby point $\mathbf{x} + \boldsymbol{\epsilon}$ also on the surface, the Taylor expansion of $g$ around $\mathbf{x}$ reveals that the gradient of $g$ must be perpendicular to any small displacement $\boldsymbol{\epsilon}$ on the surface.

When seeking a point $\mathbf{x}^\star$ that maximizes $f$ of $\mathbf{x}$ on the constraint surface, this point must have the property that the gradient of $f$, $\nabla f$ of $\mathbf{x}$, is also orthogonal to the constraint surface. Otherwise, it would be possible to increase $f$ by moving along the constraint surface. Consequently, $\nabla f$ and $\nabla g$ must be parallel (or anti-parallel), implying the existence of a parameter $\lambda$ such that the gradient of $f$ plus $\lambda$ times the gradient of $g$ equals zero.

This leads us to define the Lagrangian function, $L$ of $\mathbf{x}$ and $\lambda$, which is $f$ of $\mathbf{x}$ plus $\lambda$ times $g$ of $\mathbf{x}$. The stationary points of this function, where the gradient with respect to $\mathbf{x}$ equals zero, provide the solution to the constrained optimization problem. Additionally, setting the partial derivative of $L$ with respect to $\lambda$ to zero enforces the constraint $g$ of $\mathbf{x}$ equals zero.

To illustrate this, consider the function $f$ of $x_1$ and $x_2$ equals one minus $x_1$ squared minus $x_2$ squared, with the constraint $g$ of $x_1$ and $x_2$ equals $x_1$ plus $x_2$ minus one equals zero. The corresponding Lagrangian for this problem is $L$ of $\mathbf{x}$ and $\lambda$ equals one minus $x_1$ squared minus $x_2$ squared plus $\lambda$ times ($x_1$ plus $x_2$ minus one). Setting the partial derivatives of $L$ with respect to $x_1$, $x_2$, and $\lambda$ to zero yields a system of equations. Solving these, we find the stationary point $(x_1^\star, x_2^\star)$ is (one-half, one-half) and the value of $\lambda$ is one.

Next, we consider constraints in the form of inequalities, where $g$ of $\mathbf{x}$ is greater than or equal to zero. There are two types of solutions: one where the stationary point lies within the region where $g$ of $\mathbf{x}$ is strictly positive, and the other where it lies on the boundary where $g$ of $\mathbf{x}$ equals zero. In the first case, the constraint is inactive, and we only need to find where the gradient of $f$ equals zero. In the second case, the constraint is active, and the solution is analogous to the equality constraint case, except the Lagrange multiplier $\lambda$ must be non-negative, ensuring that the gradient of $f$ points away from the region where $g$ of $\mathbf{x}$ is positive.

The conditions for this type of optimization are known as the Karush-Kuhn-Tucker (KKT) conditions. They state that $g$ of $\mathbf{x}$ must be greater than or equal to zero, $\lambda$ must be non-negative, and the product of $\lambda$ and $g$ of $\mathbf{x}$ must equal zero.

Finally, the method of Lagrange multipliers can be extended to cases involving multiple equality and inequality constraints. For multiple equality constraints, we introduce multiple Lagrange multipliers and optimize the Lagrangian function, which becomes a sum of $f$ of $\mathbf{x}$ and terms involving each constraint multiplied by its corresponding Lagrange multiplier. For inequality constraints, the multipliers must also satisfy non-negativity conditions.

In summary, Lagrange multipliers provide a versatile and elegant method for solving constrained optimization problems by transforming them into problems of finding the stationary points of an augmented function, the Lagrangian. This method can handle both equality and inequality constraints and can be extended to multiple constraints, making it a fundamental tool in optimization theory.
## What is demonstrated by the left-hand and right-hand plots in Figure 5.15 regarding nonlinear basis functions?

![](https://cdn.mathpix.com/cropped/2024_05_26_f271bce35f2c91024ce0g-1.jpg?height=740&width=1514&top_left_y=221&top_left_x=110)

% 

The left-hand plot shows the original input space $(x_1, x_2)$ with data points from two classes (red and blue). Two 'Gaussian' basis functions $\phi_1(\mathbf{x})$ and $\phi_2(\mathbf{x})$ have centers indicated by green crosses and contours represented by green circles. The right-hand plot displays the corresponding feature space $(\phi_1, \phi_2)$ and the linear decision boundary (black line) obtained using a logistic regression model. This demonstrates how applying nonlinear basis functions can transform a non-linearly separable dataset in the original input space into a linearly separable one in the feature space.

- #machine-learning, #linear-classification, #basis-functions

## How do nonlinear basis functions help in linear classification according to Figure 5.15?

![](https://cdn.mathpix.com/cropped/2024_05_26_f271bce35f2c91024ce0g-1.jpg?height=740&width=1514&top_left_y=221&top_left_x=110)

%

Nonlinear basis functions, such as the 'Gaussian' basis functions shown in the left-hand plot of Figure 5.15, help by transforming the original input space $(x_1, x_2)$ into a feature space $(\phi_1, \phi_2)$. This transformation leads to a scenario where data points become linearly separable, as shown in the feature space plot on the right-hand side. Consequently, a linear decision boundary in the feature space corresponds to a nonlinear decision boundary in the original input space, allowing a logistic regression model to classify the data effectively.

- #machine-learning, #feature-transformation, #logistic-regression
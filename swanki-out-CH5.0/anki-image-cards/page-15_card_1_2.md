## What do the left and right plots in the figure illustrate in the context of probabilistic models for machine learning?

![](https://cdn.mathpix.com/cropped/2024_05_26_3a79e15ed1a634320c5fg-1.jpg?height=702&width=1494&top_left_y=235&top_left_x=147)

%

The left plot shows the class-conditional densities $p(x|\mathcal{C}_1)$ and $p(x|\mathcal{C}_2)$ for two classes, $\mathcal{C}_1$ (blue) and $\mathcal{C}_2$ (red), with respect to a single input variable $x$. The right plot displays the corresponding posterior probabilities $p(\mathcal{C}_1|x)$ and $p(\mathcal{C}_2|x)$. The right plot also shows the decision boundary (green vertical line), which minimizes the misclassification rate under the assumption that the prior probabilities $p(\mathcal{C}_1)$ and $p(\mathcal{C}_2)$ are equal.

- #machine-learning, #probabilistic-models, #decision-boundaries

## How is the decision boundary determined based on the class-conditional densities and posterior probabilities?

![](https://cdn.mathpix.com/cropped/2024_05_26_3a79e15ed1a634320c5fg-1.jpg?height=702&width=1494&top_left_y=235&top_left_x=147)

%

The decision boundary is determined by the point $x$ where the posterior probabilities $p(\mathcal{C}_1|x)$ and $p(\mathcal{C}_2|x)$ are equal, which minimizes the probability of misclassification. It is shown as the green vertical line in the right plot. It assumes equal prior probabilities for the classes $\mathcal{C}_1$ and $\mathcal{C}_2$.

- #machine-learning, #probabilistic-models, #decision-boundaries
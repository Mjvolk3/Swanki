![](https://cdn.mathpix.com/cropped/2024_05_26_bf6b853468e691ca09c4g-1.jpg?height=1250&width=1248&top_left_y=215&top_left_x=409)

In the previous chapter, we explored a class of regression models in which the output variables were linear functions of the model parameters and which can therefore be expressed as simple neural networks having a single layer of weight and bias parameters. We turn now to a discussion of classification problems, and in this chapter, we will focus on an analogous class of models that again can be expressed as single-layer neural networks. These will allow us to introduce many of the key concepts of classification before dealing with more general deep neural networks in later chapters.

The goal in classification is to take an input vector \(\mathrm{x} \in \mathbb{R}^{D}\) and assign it to one of \(K\) discrete classes \(\mathcal{C}_{k}\) where \(k=1, \ldots, K\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces. In this chapter, we consider linear
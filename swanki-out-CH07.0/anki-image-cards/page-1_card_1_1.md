  
    ## Explain the significance of gradient descent in neural network training.
    
    ![](https://cdn.mathpix.com/cropped/2024_05_26_b3ffcc781f5d5ca8b19bg-1.jpg?height=1250&width=1248&top_left_y=215&top_left_x=412)
    
    %
    
    Gradient descent is a fundamental optimization algorithm used to adjust the parameters (weights and biases) of neural networks to minimize an error function. It helps in finding a suitable setting for these parameters based on a set of training data. By iteratively moving in the direction of the negative gradient of the error function, gradient descent seeks to reduce the error and improve the model's performance on the training data.

    - neural-networks, optimization.gradient-descent
  
    ## How do deep neural networks benefit from encoding inductive biases through hierarchical representations?
    
    ![](https://cdn.mathpix.com/cropped/2024_05_26_b3ffcc781f5d5ca8b19bg-1.jpg?height=1250&width=1248&top_left_y=215&top_left_x=412)
    
    %
    
    Deep neural networks benefit from encoding inductive biases through hierarchical representations by allowing the model to capture complex patterns and structures in the data more effectively. This hierarchical organization enables the network to learn and generalize from data in a way that is highly efficient and adaptable to a wide range of practical applications. The layer-wise composition of features reflects the underlying structure of the data, leading to improved performance and reduced requirements for manual feature engineering.

    - neural-networks, machine-learning.inductive-bias
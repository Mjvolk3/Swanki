    
## How does a momentum term affect the learning rate parameter in gradient descent?

![](https://cdn.mathpix.com/cropped/2024_05_26_26df87b0396463dc47e2g-1.jpg?height=679&width=689&top_left_y=217&top_left_x=955)

%

In situations where successive steps of gradient descent are oscillatory, a momentum term has little influence on the effective value of the learning rate parameter $\eta$. It helps achieve faster convergence toward the minimum without causing divergent oscillations. 

- #machine-learning, #optimization.gradient-descent, #momentum

---

## What is illustrated in the diagram related to gradient descent with momentum?

![](https://cdn.mathpix.com/cropped/2024_05_26_26df87b0396463dc47e2g-1.jpg?height=679&width=689&top_left_y=217&top_left_x=955)

%

The diagram shows an abstract view of an error surface (red downward-facing parabola) and a series of weight update vectors ($\Delta \mathbf{w}^{(1)}$, $\Delta \mathbf{w}^{(2)}$, and $\Delta \mathbf{w}^{(3)}$) represented as black arrows. These vectors indicate the direction and magnitude of weight changes at successive iterations, demonstrating how gradient descent with momentum navigates the error landscape to minimize the error function.

- #machine-learning, #optimization.gradient-descent, #visualization
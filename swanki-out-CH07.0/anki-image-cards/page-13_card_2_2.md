## What technique is used to further accelerate convergence in gradient descent, as illustrated in the provided image?

![](https://cdn.mathpix.com/cropped/2024_05_26_26df87b0396463dc47e2g-1.jpg?height=287&width=640&top_left_y=1703&top_left_x=989)

%

The technique used to further accelerate convergence in gradient descent, as illustrated, is Nesterov momentum. This method changes the order of operations by first taking a step based on the previous momentum and then computing the gradient at the new location.

- optimization.gradient-descent, optimization.momentum, nesterov-momentum


## How does Nesterov momentum differ from conventional stochastic gradient descent with momentum, as seen in the image?

![](https://cdn.mathpix.com/cropped/2024_05_26_26df87b0396463dc47e2g-1.jpg?height=287&width=640&top_left_y=1703&top_left_x=989)

%

In conventional stochastic gradient descent with momentum, the gradient is first computed at the current location, and then a step amplified by the previous momentum is taken. In Nesterov momentum, the order is altered: a step is taken based on the previous momentum first, and then the gradient is computed at the new location.

- optimization.gradient-descent, optimization.momentum, nesterov-momentum
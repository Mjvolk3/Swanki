\title{
Primer
}

![](https://cdn.mathpix.com/cropped/2024_05_28_ca03d7ceb8a980af3061g-1.jpg?height=502&width=928&top_left_y=382&top_left_x=129)

Fig. 4 |GNNs for knowledge graphs and molecular property prediction. a, An example of a biomedical knowledge graph with different types of interactions between entities (nodes) that are either proteins, drugs, viruses or diseases. b, Quantum property prediction with a graph neural network (GNN) as a representative task for molecular property prediction. Although accurate b

![](https://cdn.mathpix.com/cropped/2024_05_28_ca03d7ceb8a980af3061g-1.jpg?height=470&width=888&top_left_y=409&top_left_x=1069)

quantum simulations to estimate properties can take hours, GNNs have been successful at predicting quantum properties in fractions of seconds. \(E\), potential energy; \(\omega_{0}\), vibrational mode frequency;SARS-CoV, severe acute respiratory syndrome coronavirus; SARS-CoV-2, severe acute respiratory syndrome coronavirus 2 . unknown relation could be whether an existing drug can be repurposed to treat additional diseases. In this drug discovery context, knowledge graphs offer the opportunity to integrate additional data from many modalities like drugs, phenotypes, diseases, disease exposure, genes or pathways, each with their own types of relations \({ }^{7}\). Outside the biomedical context, GNNs for knowledge graphs have heavily impacted recommender systems used in retail, advertisement and social media \({ }^{66}\).

\section*{Molecular property prediction}

An impactful application of GNNs is to predict (un)desirable properties of small molecules. A prominent example is ligand-based virtual screening, in which GNNs are trained to predict a property and scan large sets of molecules to identify candidates with the most favourable properties. For instance, a directional message passing neural network \({ }^{10}\) combined with 200 additional molecule-level features was used \({ }^{67}\) to predict a molecule's ability to inhibit Escherichia coli bacteria growth and helped discover a new antibiotic. In these settings, active learning is also used to refine the model's predictions based on different rounds of experimental validation.

Although the standard GNNs in Table 1 are often adequate for predicting properties like toxicity; absorption, distribution, metabolism, excretion (ADME); or synthesizability \({ }^{68,69}\), their performance can be improved by including additional prior knowledge about molecules, such as the importance of rings. Therefore, cycles and other subgraph counts are often added to initial node and edge features. Laplacian-based positional encodings have been successful with a standard GNN architecture for predicting mass spectra \({ }^{70}\). Other architecture improvements that have shown promise in property prediction are directional graph networks \({ }^{20}\), in which positional encodings improve message guidance, and subgraph aggregation \({ }^{71}\), in which messages are passed over subsets of the molecular graph. Using information about a molecule's synthesis or generation path as an input can provide additional signals for better generalization and data efficiency \({ }^{72}\).

Architecture specializations were necessary to improve GNNs for predicting quantum mechanical properties (visualized in Fig. 4b), one of the first message-passing applications \({ }^{73}\). Graph transformers were used to pass messages between all nodes while retaining the graph structure. The graph structure was included by encoding it as an initial node feature or using simultaneous message-passing layers for the molecular graph \({ }^{34}\). Large transformer architectures are particularly well-suited to utilize the increasing amounts of data generated with quantum simulations, from GEOM-QM9 and GEOM-DRUGS \({ }^{74}\) to PCQM4Mv2 (ref.12). For quantum properties, GNNs are also able to obtain electronic structures via variational quantum Monte Carlo, increasing speeds and bringing a new level of generalizability to the field \({ }^{75,76}\).

\section*{Graph generation}

GNNs are fundamental building blocks in generative models on graphs. Graph generation has several compelling applications. For example, to find an effective drug for a particular disease, billions of small molecules could be virtually screened, but this would still only access a small fraction of the synthesizable molecular space. Direct generation of candidate molecules with certain desired properties could drastically reduce computational costs, while expanding the number of accessible compounds.

However, although the generation of images (fixed-size vectors) and text (ordered sequence of tokens) is successful with deep learning approaches, the variability - different numbers of nodes and edges - and symmetries of graphs render the generation process particularly challenging. Initial approaches were largely based on the generative modelling frameworks of variational auto encoders \({ }^{77}\) or generative adversarial networks \({ }^{78}\), both of which involve learning the transformation of a fixed-size random vector into a graph. Different approaches to achieve this complex mapping include building a single, large, fixed-size adjacency matrix and masking it \({ }^{79}\); iteratively building a graph by adding nodes or subgraphs \({ }^{80,81}\) (Fig. 5a); or starting from a fixed reference graph and learning to modify it \({ }^{82}\). Diffusion-based models, which learn to gradually map randomly sampled graphs to those of interest, can provide large improvements across different domains \({ }^{83,84}\). A particular challenge for graph generation is performance evaluation.

\section*{Biophysical structure, dynamics and interactions}

3D GNNs can model biophysical structures as they are able to represent 3D point clouds, for example protein residues, and have a physically realistic prior that local interactions are the most relevant, whereas distant forces decay rapidly.
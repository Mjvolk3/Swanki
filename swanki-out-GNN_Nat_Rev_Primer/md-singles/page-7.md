\title{
Primer
}

of message-passing operators in which the messages are constructed and passed based on the relative position of the two nodes.

When working with graphs embedded in three dimensions, such as the 3D structures of a molecule, it is important to consider the symmetry of the task with respect to translations and rotations of the frame of reference (Fig.3a). This translates intoSE(3) invariance or equivariance, in which \(\mathrm{SE}(3)\) is the special euclidean group in three dimensions, that is, the group of rotations and translations in three dimensions.

To design SE(3)-invariant architectures, the coordinates of the nodes cannot be taken as normal input features, because they would cause the model's output to change when the frame of reference is translated. Similarly, taking the relative vectors as edge features is problematic, as they change when the system is rotated. The easiest way to achieve rotation invariance is to extract only the relative distances between pairs of nodes and use these as edge features in message-passing \({ }^{44,45}\).

However, only using distances in message-passing does not yield very expressive architectures. Similarly to arbitrary graphs, more powerful models are obtained using either higher-order representations or multi-hop interactions. Unlike general graphs, for which universality is unattainable due to the intrinsic challenge of graph isomorphism, the grounding of nodes in 3D space makes isomorphism easier on geometric graphs. Both strategies can yield architectures that are theoretically maximally expressive and are able to approximate any continuous equivariant function on a set of points in the 3D space \({ }^{46}\).

In the higher-order strategy \({ }^{11,47,48}\), the hidden representations of nodes contain normal \(\mathrm{SE}(3)\)-invariant scalar features, \(\mathrm{SE}(3)\)-equivariant vectors and higher-order representations. These more complex features can represent physical properties, such as forces and polarizability; however, they have to be handled with specific equivariant operations, such as tensor products. In the multi-hop interaction strategy \({ }^{49,50}\), in addition to distances between pairs of nodes, the angles between pairs of connected edges and dihedral angles between three consecutive edges are used. These additional features enable complex relationships to be distinguished that cannot be easily captured when relying on simple distances.

\section*{Interpretability and uncertainty}

Interpretability. Moving from a simple model based on hand-crafted features or rules to a deep learning solution comes at the cost of the degree of interpretability of the predictions. Instead of using human-interpretable rules, predictions are based on layers of transformations that produce representations without human-understandable meanings. This is also the case for GNNs; however, among architectures, they are inherently more interpretable and explainable, because they learn about relations between human-understandable entities from the nodes used to define the graph. For example, an inference based on a GNN's link prediction in a knowledge graph is easier to explain and interpret than the same inference made by an unstructured model. The additional structure of the graph-based problem formulation can be used for interpretability by inferring which node or subgraph of the input explains a prediction the most. To do so, researchers have developed several techniques similar to the general approaches for neural network interpretability but that also take into account the discreteness and symmetries of the graph structure.

Two of the most common strategies are gradient-based \({ }^{51}\) or perturbation-based methods \({ }^{52}\), both of which try to pinpoint the components of the input that most affects the output. An example of the latter strategy is GNNExplainer \({ }^{53}\), which applies various modifications to the input data to determine which subgraph and features were the most important for the prediction. Another strategy is to build surrogate models \({ }^{54}\), simpler, more interpretable architectures trained to reproduce the inputs and outputs of the base model. Finally, graph generation methods can build simple example structures to maximize the likelihood of a class under the model \({ }^{55}\). More detailed taxonomy and description of the different GNN interpretability approaches are provided in refs. 56,57 .

Uncertainty estimation. Uncertainty estimation in machine learning determines how much a prediction can be trusted. Like interpretability, this is more difficult in a deep learning setting than in classical approaches. For GNNs, uncertainty quantification comes with unique challenges. For example, data uncertainty or epistemic uncertainty can arise from multiple sources with different impact magnitudes for node features or missing or incorrect edges. Similarly, how uncertainty propagates through layers and passed messages to produce a final prediction in GNNs is different from simpler architectures that are comparatively better studied for uncertainty quantification. These challenges mean that traditional deep learning uncertainty estimation methods fail when applied to GNNs in an inductive setting \({ }^{58}\). In the transductive setting, a major difficulty is the missing assumption on independent identically distributed samples. Without this, many of the general uncertainty estimation approaches do not apply. In practice, GNNs are underconfident \({ }^{59}\) in the transductive setting. To address these issues, GNN have been developed with tailored techniques, such as custom Bayesian node updates, to disentangle epistemic and aleatoric uncertainty \({ }^{60}\), and topology-dependent correction steps of the confidence \({ }^{61,62}\).

\section*{Applications}

With the abundance of graph-structured data in science and society, GNNs have found wide applicability, with meaningful impact in many fields. However, due to the range of tasks, it is crucial to consider application-specific information when selecting a model, as there is no one-size-fits-all GNN. An architecture should be chosen that best fits the application along multiple axes, such as scalability, expressivity and data efficiency. For instance, one axis is the trade-off between expressivity and memory usage, a core consideration for large protein-protein interaction graphs. On another axis, chemical priors - for instance, the importance of rings - are crucial to small-molecule property prediction \({ }^{26}\). Finally, in machine-learned interatomic potentials for molecular dynamics, inference speed is one of the main challenges \({ }^{63}\).

Although standard GNNs can address many tasks adequately, there are cases in which simple solutions fail or cannot be used. These cases require additional insights to be built into the architecture. This section demonstrates this with literature examples highlighting some important GNN applications in the life and physical sciences.

\section*{Knowledge graphs}

Knowledge graphs model relational data via nodes that represent different entities and directed edges that symbolize various relationships. For instance, in a biomedical knowledge graph, nodes might be diseases, drug molecules, proteins or viruses (Fig. 4a). The edges could encode relations about whether a drug cures a disease, a drug binds to a protein, a protein is relevant for a disease or similar.

To process knowledge graphs, specialized GNNs have been proposed \({ }^{64,65}\) to handle the heterogeneous types of edges and nodes. Node embeddings from these architectures can predict the probability of unknown relations. In a biomedical context, for example, the
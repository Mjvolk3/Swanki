**Appendix C: Lagrange Multipliers**

Lagrange multipliers, also known as undetermined multipliers, offer a powerful method to find the stationary points of a function with multiple variables while adhering to one or more constraints. This technique is especially useful in optimization problems where we need to maximize or minimize a function subject to specific conditions.

To illustrate, imagine we are tasked with finding the maximum of a function of two variables, which we'll call f of x1 and x2, subject to a constraint that relates these two variables. We express this constraint as g of x1 and x2 equals zero. One straightforward approach might involve solving this constraint equation to express x2 as a function of x1, denoted as h of x1. Substituting this back into the original function transforms it into a function of x1 alone, allowing us to find the maximum by differentiating with respect to x1. However, this method can be cumbersome and may disrupt the natural symmetry between the variables x1 and x2.

A more elegant solution introduces a parameter called a Lagrange multiplier, denoted by lambda. This technique can be motivated from a geometrical perspective. Consider a D-dimensional variable, represented as vector x, with components ranging from x1 to xD. The constraint equation g of x equals zero describes a surface in this D-dimensional space. At any point on this constraint surface, the gradient of the constraint function, denoted as ∇g of x, is orthogonal to the surface. This orthogonality is due to the fact that any small movement along the surface does not change the value of g, implying that the gradient must be perpendicular to the surface.

Next, we seek a point on the constraint surface where the function f of x is maximized. At this point, the gradient of f, denoted as ∇f of x, must also be orthogonal to the constraint surface. If it weren't, we could move along the surface to increase the value of f, contradicting the assumption that we are at a maximum. Thus, the gradients of f and g are parallel or anti-parallel, meaning there exists a parameter lambda such that the gradient of f plus lambda times the gradient of g equals zero. This parameter lambda is the Lagrange multiplier.

We can conveniently define a new function called the Lagrangian, denoted as L of x and lambda, which is the sum of the original function f of x and lambda times the constraint function g of x. The condition for constrained stationarity is obtained by setting the gradient of L with respect to x to zero. Additionally, setting the partial derivative of L with respect to lambda to zero gives us the constraint equation g of x equals zero.

To find the maximum of f of x subject to the constraint g of x equals zero, we define the Lagrangian and find its stationary point with respect to both x and lambda. For a D-dimensional vector x, this gives us D plus one equations to solve for the stationary point x-star and the value of lambda. If we're only interested in the stationary point, we can eliminate lambda from the stationarity equations, hence the term 'undetermined multiplier'.

Consider a simple example where we want to find the stationary point of the function f of x1 and x2 equals one minus x1 squared minus x2 squared, subject to the constraint g of x1 and x2 equals x1 plus x2 minus one equals zero. The corresponding Lagrangian function is L of x and lambda equals one minus x1 squared minus x2 squared plus lambda times x1 plus x2 minus one. Setting the conditions for this Lagrangian to be stationary with respect to x1, x2, and lambda gives us a set of coupled equations. Solving these equations yields the stationary point x1-star and x2-star equals one-half, one-half, with the Lagrange multiplier lambda equal to one.

Now, let's extend our discussion to inequality constraints. Suppose we want to maximize f of x subject to an inequality constraint g of x greater than or equal to zero. There are two possible types of solutions: one where the constrained stationary point lies within the region where g of x is greater than zero, and another where it lies on the boundary where g of x equals zero. In the former case, the constraint is inactive, and the stationary condition is simply the gradient of f of x equals zero. This corresponds to a stationary point of the Lagrangian function with lambda equal to zero. In the latter case, the constraint is active, and the solution lies on the boundary, analogous to the equality constraint scenario, with lambda not equal to zero.

The sign of the Lagrange multiplier matters here because the function f of x is at a maximum only if its gradient points away from the region where g of x is greater than zero. Thus, the gradient of f of x equals negative lambda times the gradient of g of x, with lambda greater than zero. For both cases, the product of lambda and g of x equals zero. The solution to the problem of maximizing f of x subject to g of x greater than or equal to zero is obtained by optimizing the Lagrangian function with respect to x and lambda, subject to the conditions that g of x is greater than or equal to zero, lambda is greater than or equal to zero, and lambda times g of x equals zero. These conditions are known as the Karush-Kuhn-Tucker (KKT) conditions.

Lastly, this technique can be extended to cases with multiple equality and inequality constraints. Suppose we want to maximize f of x subject to multiple equality constraints gj of x equals zero for j ranging from one to J, and multiple inequality constraints hk of x greater than or equal to zero for k ranging from one to K. We introduce Lagrange multipliers for each constraint and optimize the Lagrangian function, which includes terms for all constraints, subject to the conditions that the multipliers for the inequality constraints are non-negative and their product with the respective constraint functions equals zero. This method can also be adapted for constrained functional derivatives. For a detailed discussion, you may refer to the works by Nocedal and Wright.
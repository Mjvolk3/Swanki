## How do entropy values differ between distributions with varying levels of uniformity as illustrated by the histograms in Figure 2.14?

![](https://cdn.mathpix.com/cropped/2024_05_10_86a2845941e286ae4e26g-1.jpg?height=648&width=1510&top_left_y=272&top_left_x=134)

%

The left histogram, with values more concentrated around a smaller number of bins, illustrates a lower entropy value of 1.77, indicating less spread and lower uncertainty. The right histogram, which is more uniformly spread across many bins, shows a higher entropy value of 3.09, indicating greater spread and higher uncertainty. This highlights the principle that the more evenly a distribution is spread across its range, the higher its entropy.

- #entropy, #probability-distributions, #information-theory

## Calculate and explain the entropy value for a uniform distribution over 30 bins as mentioned in the associated text.

![](https://cdn.mathpix.com/cropped/2024_05_10_86a2845941e286ae4e26g-1.jpg?height=648&width=1510&top_left_y=272&top_left_x=134)

%

For a uniform distribution over 30 bins, each bin has an equal probability, $$p(x_i) = \frac{1}{30}.$$ The entropy, H, for such a distribution is calculated using the formula:

$$
\mathrm{H} = -\sum_{i=1}^{30} p(x_i) \ln(p(x_i)) = -\sum_{i=1}^{30} \frac{1}{30} \ln\left(\frac{1}{30}\right) = -\ln\left(\frac{1}{30}\right) = 3.40.
$$

This calculation was also mentioned in the text, confirming that maximum entropy under these conditions is 3.40, characteristic of the high uncertainty or maximum spread in a uniform distribution.

- #entropy-calculation, #uniform-distribution, #information-theory
![](https://cdn.mathpix.com/cropped/2024_05_10_10ceec4bdaa45dd5506eg-1.jpg?height=1248&width=1238&top_left_y=216&top_left_x=412)

In almost every application of machine learning we have to deal with uncertainty. For example, a system that classifies images of skin lesions as benign or malignant can never in practice achieve perfect accuracy. We can distinguish between two kinds of uncertainty. The first is epistemic uncertainty (derived from the Greek word episteme meaning knowledge), sometimes called systematic uncertainty. It arises because we only get to see data sets of finite size. As we observe more data, for instance more examples of benign and malignant skin lesion images, we are better able to predict the class of a new example. However, even with an infinitely large data set, we would still not be able to achieve perfect accuracy due to the second kind of uncertainty known as aleatoric uncertainty, also called intrinsic or stochastic uncertainty, or sometimes simply called noise. Generally speaking, the noise arises because we are able to observe only partial information about the world, and therefore, one way to reduce this source of uncertainty is to gather different kinds of data. This is illustrated
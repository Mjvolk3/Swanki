\title{
4
}

\section*{Creating New Instruments and Research Techniques for Discovering Cell Mechanisms}

\begin{abstract}
... is our method of fractionation like the clumsy undertaking of a car mechanic who attempts to use his crude tools to analyze a watch? I believe that it is almost as bad as that. Nevertheless, we have no alternative and must hope that our tools will become refined as we proceed in the analysis. Meanwhile we have to look out for the signs that guide us in the right direction; we must try to correlate the experimental findings obtained with cell-free systems with the complex physiology of the cell; we must keep in view the metabolic 'Gestalt' of the cell; and finally, we must 'seek simplicity and then distrust it.'
\end{abstract}

(Racker, 1965, p. 89)

"Seeing is believing." So we are often told. You do not, however, have to go far to find instances in which seeing is misleading. For example, look at Figure 4.1 and ask yourself whether the shaded surfaces of the two figures are identical in shape. If you are like most people, you will judge that the shapes differ - one long and narrow, the other closer to square. However, you can convince yourself that they actually have identical dimensions if you rotate one of them 90 degrees and measure the corresponding sides. Your visual system misled you in this case about what actually exists in the world, creating what scientists refer to as an artifact (or artefact, especially in older publications). Although the term artifact is used in ordinary parlance for anything made by humans, in science it denotes evidence produced by instruments and experiments that does not properly reflect the phenomenon under investigation.

Although we usually regard seeing as giving us direct access to what is nearby in the world, it may be better to think of our visual system as an instrument for guiding action and producing judgments about what we are seeing. Research over the past two centuries has revealed some of the complex brain operations that enable our visual system to perform these functions

explanation that shows them to be an artifact. On the other hand, if the results are what you would expect given an existing model of the phenomena, or you are able to advance such a model in which they make sense, that increases the likelihood that you will accept the results as evidence.

This yields the third criterion: coherence with plausible accounts of the phenomena. Relying on it seems paradoxical if you see evidence as foundational, such that models or other accounts are assessed in terms of the evidence offered for them. For what I have just proposed is that evidence is assessed by its fit to an account; specifically, it is more likely to be accepted when it fits with a plausible account than when it does not. As paradoxical as it seems, however, in practice scientists often rely on this criterion. Some may worry that if this is actually how scientists appraise evidence, they are guilty of circular reasoning. However, such reasoning would be viciously circular only if the sole basis for evaluating the account were the results of experimental techniques, which were themselves being evaluated by whether they supported the account. But that is not the only way in which an account can be evaluated; another is whether it coheres with other accounts. As we shall see, the new research instruments and techniques that were crucial for the development of cell biology often suggested the existence of new cell organelles. But did these organelles really exist? When the new organelles were identified as having a role in the emerging mechanistic model of cell function, then researchers were more likely to accept not only the existence of the organelles but also the instruments and techniques that pointed to their existence. When no role for the putative organelle was apparent, the existence of the organelle and the techniques providing evidence for them were subject to question.

I have proposed three criteria by which one might assess a new research instrument or technique merely by the results it generates: the degree to which
- the results exhibit a determinate structure or pattern and are repeatable;
- the results agree with results generated by other techniques or can be calibrated against them; and
- the putative evidence coheres with theoretical accounts that are taken to be plausible.

In practice, each of these three criteria plays an important role in scientists' evaluation of evidence (Bechtel, 1995; Bechtel, 2000; see also Creath, 1988). Having offered this rather abstract sketch of the epistemic problems encountered by scientists when introducing new instruments and techniques and answering the inevitable charges of artifact, I turn now to concrete exemplars from the early years of modern cell biology. Most prominent were two

instruments - the ultracentrifuge and electron microscope - and the associated techniques that enabled scientists to gain unprecedented access to the mechanisms of the cell. I discuss the ultracentrifuge in Section 2 and the electron microscope in Section 3.

\title{
2. THE ULTRACENTRIFUGE AND CELL FRACTIONATION
}

As discussed in Chapter 2, to understand a mechanism scientists try to decompose it into its component parts (structural decomposition) and operations (functional decomposition). Perhaps the most obvious strategy is to isolate certain parts and see what each does. In cells, for example, simply learning what chemical compounds are typically found in each component part may be informative as to what role each part plays in the cell's functioning. If a specific enzyme appears in a particular organelle, for example, then it is likely that the reaction catalyzed by that enzyme is localized there. The critical first step is to isolate the component parts and operations and the challenge is to do so in a way that divides components as they are in the natural system. This is extremely challenging. The living cell is a highly structured milieu in which components are integrated structurally and functionally.

How then to decompose cells into their component parts? Two candidates seemed to be available - mechanical forces and chemical processes. Centrifugal force as generated in a centrifuge, which can separate particles by size and weight, was a promising mechanical candidate. However, it required some means of breaking cell membranes so as to access the contents and combine the contents of multiple cells into one container. One plausible way of breaking cell membranes was to apply chemical agents that would dissolve them. Accordingly, in 1869 the Swiss biologist, Johann Friedrich Miescher, employed first warm alcohol and then the enzyme pepsin to strip away both the cell membrane and the cytoplasm of the cell in an attempt to isolate cell nuclei. He subjected the remaining material to centrifugation to isolate the nucleus from the debris, and then subjected the nucleus itself to chemical analysis. In this manner, Miescher (1871) identified a new group of cellular substances which he named nucleins; when his student Richard Altmann (1889) identified them as acids, he renamed them nucleic acids.

Although various researchers in the early twentieth century, including Otto Warburg, employed centrifugation in attempts to isolate cell structures, the technique did not come into wide use at the time. There are a couple of reasons for this. First, many researchers were skeptical of techniques that disrupted the internal structure of the cell, which they assumed was critical

to its functioning. ${ }^{8}$ Such skepticism is often justified: if a component does manage to function when isolated in this manner, it may function differently than in the intact cell. Second, the centrifugal forces generated by the available centrifuges were too weak to separate most cellular constituents.

Beginning in the 1920s, several researchers set about improving the capacity of the centrifuge. Swedish physical chemist Theodor ("The") Svedberg turned to centrifugation in the course of his work on colloids. Svedberg and Herman Rinde (1924) built an electrically driven centrifuge from the components of a cream separator. In addition to generating higher rotational speeds and hence greater centrifugal forces, there was a window through which the process of sedimentation during centrifugation could be observed. ${ }^{9}$ It was this potential for direct observation that led Svedberg to refer to his centrifuge as an ultracentrifuge. However, the meaning of the term shifted and it came to be used for any centrifuge running at high speeds in a vacuum or near vacuum.

Independently, Elime Henriot, working in Belgium, achieved high rotational speeds with an air turbine centrifuge. Jesse Beams, together with his graduate student Edward Pickels at the University of Virginia, modified Henriot's design by introducing a larger rotor enclosed in a vacuum chamber and suspended by a steel wire (see Beams, 1938). Pickels continued to innovate with centrifuges at the International Health Division of the Rockefeller Foundation, where he collaborated with Johannes Bauer in the development of a centrifuge that could be used to separate filterable viruses. Subsequently, Pickels (1942) went on to design an electrically driven ultracentrifuge which investigators found far easier to use.

By the early 1930s the centrifuge as an instrument had been sufficiently developed that it was available for use by biologists such as Martin Behrens (1932) and Robert Bensley and Normand L. Hoerr to attempt to isolate cell structures. The results of these studies were impressive. In 1934 Bensley and Hoerr could reliably produce a fraction which they claimed, based primarily on information about the size of the component particles, was mitochondrial in nature. In the late 1930s and early 1940s, Claude could produce four distinct fractions that were clearly different in both appearance and chemical
\footnotetext{
${ }^{8}$ Schneider and $\operatorname{Kuff}$ (1964, pp. 19-20) commented, "The attitude of most cytologists was in large part responsible for the lack of consideration of isolation methods. They argued, on the basis of their observations of living cells under the microscope, that disruption of the cell wall produced immediate and irreversible changes in the cellular components."

${ }^{9}$ Svedberg's initial success was in sedimenting haemoglobin and demonstrating that it was a homogeneous molecule, not a heterogeneous colloid (Svedberg \& Fåhraeus, 1926). This and subsequent studies helped demonstrate that proteins were in fact macromolecules.
}

\title{
Discovering Cell Mechanisms
}

constitution. This was sufficient to attract attention to the technique of cell fractionation. In the next chapter, I will discuss these and other results in detail and explore how they contributed to the body of knowledge that came to characterize cell biology. It is important to note that already the first criterion for adequacy of a new technique was satisfied - the technique was generating a determinate and replicable pattern of results.

Yet, the developers of techniques for cell fractionation, and especially their critics, were very concerned about whether centrifugation procedures created artifacts. As I noted, the most commonly cited criterion in evaluating results from a new technique is correspondence with evidence from other procedures. In the case of cell fractionation researchers sought to relate the results to those obtained in microscopy by comparing the appearance of the fractionation products with the appearance of organelles in whole cells. Early fractionation studies failed this test - the isolated, centrifuged organelles did not look at all like they did under the light microscope. This was particularly true of mitochondria. Hogeboom, Schneider, \& Palade (1948) explored alternative media in large part to try to make isolated mitochondria retain their typical elongated shape and to stain with Janus Green B, a traditional mitochondrial stain. Thus, the initial failures did not lead researchers to abandon the technique but to revise it so that it yielded evidence comparable to older techniques. This is what I referred to previously as seeking consilience of results in order to calibrate the new technique.

What exactly was the technique of cell fractionation? For Latour, a technique could be characterized as a black box when the steps involved are taken for granted as part of established practice. Cell fractionation had achieved this status by the late 1950s and 1960s, and recipes for performing it were widely available in manuals of experimental procedure. While the techniques retained a close resemblance to those initially developed by Bensley and Hoerr and by Claude, they also incorporated some substantive changes that put their mark on the results obtained. These changes were evident both in the methods for preparing cell materials and in the centrifugation regimes employed to fractionate them.

There were two major approaches to preparing materials for fractionation. (Details varied, depending on the material selected - such as bacteria, algae, or liver tissue - and which cell constituents were of interest.) In the aqueous approach, the material was put in an aqueous medium (e.g., $0.88 \mathrm{M}$ sucrose solution when mitochondria were targeted) and shearing forces were used to break the cell membranes. The resulting material is called a homogenate (because the cells have been broken and their contents blended)

or suspension (because many particles - intact organelles or fragments of organelles - are suspended in the solution). In the nonaqueous approach, water was removed from the material by means of freeze-drying, and then milling and grinding were used to break the cell membranes. Although the nonaqueous method was the first to be employed (by Bensley and Hoerr, for example), and manifested considerable advantages, especially for studying the nucleus, its disadvantages made it less suitable for studying cytoplasmic structures. Perhaps most critical was that the drying process utilized chemical agents that could inactivate many of the enzymes researchers were targeting. Since, following Claude, the aqueous techniques came to be preferred in studies of cytoplasmic organelles, I will focus on them and especially on two key elements: the means of breaking cell membranes and the media in which the contents were maintained.

\title{
Breaking Cell Membranes
}

Membranes - the cell membrane (plasma membrane) as well as various internal membranes - are relatively tough and difficult to break. Moreover, it is important to break the cell membrane without destroying the membrane's surrounding internal organelles, because breaking these could cause a redistribution of the enzymes residing in the organelles. Although its existence was not even anticipated as the earliest work on cell fractionation was being pursued, the lysosome provides a particularly vivid example of the hazards of breaking internal membranes. The lysosome contains hydrolytic enzymes, such as acid phosphatase, which are destructive of other cell organelles. Breaching the lysosome membrane would result in the rapid destruction of the other cell organelles under investigation. The desire not to destroy internal membranes largely ruled out use of the Waring blender, which was a standard tool for preparing homogenates in biochemistry. (In the dominant paradigm in biochemistry, the cell was regarded as essentially a sac of enzymes. To bring enzymes into solution for study, it was desirable to break any membranes that kept them isolated within organelles.) In addition to the objective of breaking all and only cell membranes, a further concern was the speed of operation; any delay between the initial breaking of cells and centrifugation could result in changes in the cell contents. So the goal was relatively straightforward - quickly break all cell membranes but not the membranes of any internal organelles.

Early researchers explored several different means of breaking the cell membrane. Emphasizing gentleness, in his early studies Claude gently rubbed

cells against each other with mortar and pestle. However, this procedure left many whole cells in the aqueous medium, which then could not be separated from cell nuclei. This fraction was therefore essentially useless and had to be discarded, posing a serious difficulty when Claude wanted to characterize the distribution of a given enzyme by stating the amount in later, usable fractions relative to the amount in the whole cell. Two biochemists at Wisconsin, Conrad Elvehjem and Van Potter, developed a coaxial homogenizer in which finely cut or minced tissue was placed in a tube with some buffered salt solution and a closely-fitting pestle rotated either by hand or with a motor while being worked up and down in the tube. Although this may seem like a gentle procedure, it is not. It breaks the membrane through the shearing force resulting from the faster moving material near the pestle rubbing against the slower moving material near the wall of the tube. As a result, the Elvehjem-Potter homogenizer was quite effective at breaking cell membranes, but it also broke some internal membranes (Potter \& Elvehjem, 1936). By the estimate of de Duve (1971), it damaged 15\% of lysosomes and peroxisomes and $10 \%$ of mitochondria. It is interesting to consider how de Duve could reach such a judgment. As we will see, he made a fundamental assumption - that a given enzyme originated in a single organelle. Thus, he inferred that the fraction with the greatest concentration was the locus of the enzyme in living cells, and that any of the enzyme found in other fractions represented contamination. This was plausible insofar as the amount of an enzyme found in one fraction generally greatly exceeded that in any other. In this instance, it was the determinateness of the results that supported the judgment of what was artifact and what was evidence of an underlying phenomenon.

Several factors affect the degree of disintegration of cells achieved with the Potter-Elvehjem homogenizer, including the clearance between tube and pestle, the speed of rotation, and the number of times the pestle is moved up and down. This led to a striking number of variations in the basic design of the homogenizer, some of which are illustrated in Figure 4.2. Campbell and Epstein (1966, p. 19) commented, "it is probably true that almost every worker has his own individual preference." A variety of other means of breaking cell membranes were explored in the 1940s and 1950s, including colloid mills (employed by Mirsky to isolate chromosomes, as in Mirsky \& Ris, 1951), ultrasonic or sonic vibrations, and osmotic shock. Most investigators, though, adopted the Elvehjem-Potter homogenizer or variants on it. This was largely because it provided a successful compromise between the destructiveness of the Waring blender and the insufficient disruption of membranes from simple rubbing. It produced impressive, interpretable results (clear differentiation of

![](https://cdn.mathpix.com/cropped/2024_07_05_d065de578bbffae82f80g-1.jpg?height=968&width=867&top_left_y=208&top_left_x=329

ChatGPT figure/image summary: The image provided appears to be a diagram showing various types of homogenizers used in cell biology for lysing cells and breaking down their membranes to isolate and study subcellular components. Each homogenizer consists of a tube and a closely fitting pestle that can be inserted into the tube. The pestle is designed to rotate and move up and down to create shearing forces that disrupt the cellular membranes. There are several different designs, illustrating the diversity in the equipment used for this purpose:

- A: Shows a pestle with a cross-shaped end, fitting into a tube with a flared opening.
- B: Depicts a classical pestle and tube with a constriction in the middle of the tube.
- C: Features a long thin pestle with what may be a textured end, inserted into a straight tube.
- D: Displays various pestle and tube lengths and diameters without additional features.
- E: Presents a pestle with a helical or coiled structure at its base, likely to increase the shearing forces.
- F: Simple straight pestles and tubes with no additional features.
- G: Another variation of the pestle and tube with a bulb in the middle of the tube.
- H: Features a pestle with a T-shaped handle and a conical tube.
- I: Illustrates a more complex design that seems to incorporate a gear or handle for mechanical turning of the pestle, which also has a coiled base, inserted into a tube with a side arm feature, potentially for the addition or removal of samples or solutions.

These devices are used for the mechanical disruption of cell walls and membranes in laboratories, which is a crucial step in the process of cell fractionation. The process allows scientists to study organelles and other cellular components in detail, contributing essential insights into cell biology and biochemistry.)

Figure 4.2. Examples of different homogenizers used to break cells prior to cell fractionation. In each, a pestle (right) is inserted and moved up and down in a tube (left). Reprinted from Vincent Allfrey (1959), The isolation of subcellular components, in J. Brachet and A. E. Mirsky (eds.), The Cell: Biochemistry, Physiology, and Morphology, Vol. 1. New York: Academic Press, Figure 1 on p. 214.

enzymes between different fractions). Thus, de Duve and Berthet concluded as early as 1954:

Absolute preference should be given to homogenizers of the type described by Potter and Elvehjem (1936). Simple rubbing in a mortar, as recommended by Claude (1946), disrupts only a fraction of the cells, and mechanical choppers such as the Waring Blender cause excessive damage to the particulate components of the cells. The Potter-Elvehjem instrument is, of course, not entirely free of these drawbacks and should be used with discrimination. (p. 232)

\title{
Choice of Media
}

Once the cell membranes are broken, the properties of the aqueous medium into which the cell contents disperse become critical. The desired medium,

\title{
Discovering Cell Mechanisms
}

as Vincent Allfrey stated, "should so approximate the soluble phase of the cytoplasm that the cell particulates remain morphologically, structurally, and functionally intact." But, he added, "Such a medium has not yet been devised, and in practice all isolation media introduce more or less serious alterations in both structure and function" (1959, p. 202).

The challenge, then, was to figure out which medium would most closely correspond to the environment in living cells. Initially a saline solution seemed physiologically realistic, and Claude employed it in his early fractionation studies. However, this medium caused clumping and agglutination of the cytoplasmic particulates and failed to preserve the morphological integrity of organelles when compared with micrographs of whole cells. As noted previously, this led Hogeboom and his collaborators to try a hypertonic $0.88 \mathrm{M}$ sucrose medium, which succeeded in preserving the rod-like appearance of mitochondria (Hogeboom et al., 1948). However, the crucial function of ATP synthesis (suspected to be localized in mitochondria) was lost. Accordingly, one of the collaborators, Schneider (1948), explored some of the more commonly used sucrose concentrations. He showed that with an isotonic $0.25 \mathrm{M}$ sucrose solution, the mitochondrial fraction would carry out oxidative phosphorylation (though the resemblance of its particles to intact mitochondria was somewhat compromised). ${ }^{10}$ Soon the medium in most fractionation studies was selected from the class of isotonic sucrose solutions. ${ }^{11}$

The logic of the argument for preferring this fractionation technique is noteworthy. The claim that the mitochondrion was the locus of oxidative phosphorylation was based primarily on fractionation studies, but the justification for doing fractionation in sucrose and especially in isotonic sucrose was that it produced a fraction that possessed the enzymes for oxidative phosphorylation and could carry out the reactions that realized that function. The

${ }^{10}$ A similar dependence of functional activity on sucrose concentration was found in studies of isolated thymus nuclei - nuclei isolated in $0.25 \mathrm{M}$ sucrose synthesized protein and RNA but those prepared in $0.4 \mathrm{M}$ sucrose did not (Allfrey, 1959, p. 206). Allfrey also reported a number of other factors that affect the appearance and function of isolated nuclei, including $\mathrm{pH}$ and ionic strength, and provided another example of a form/function tradeoff: "It is a curious fact that when calf thymocyte nuclei are isolated in $0.25 \mathrm{M}$ sucrose- $0.003 \mathrm{M} \mathrm{CaCl}_{2}$, they are granular in appearance, but active in many synthetic systems; if they are isolated in a hypertonic medium which preserves optical homogeneity they lose their synthetic capacity" (1959, p. 208).

11 Witter, Watson, and Cottone (1955) compared electron micrographs so as to examine the fine structure of mitochondrial fractions isolated using various sucrose media and concluded that $0.44 \mathrm{M}$ sucrose with $\mathrm{pH}$ regulated to 6.2 with citrate produced the best resolution. Based on similar studies, Novikoff (1956a) favored $0.25 \mathrm{M}$ sucrose with $7.3 \%$ polyvinylpyrrollidone at a $\mathrm{pH}$ of 7.6 to 7.8. In general, the strategy was to tweak the method until it generated the clearest or most useful results for the current purpose.

\title{
Creating New Instruments and Research Techniques
}

procedure for isolating mitochondria was being calibrated to make its results correspond to those obtained by related techniques. Moreover, the fit with the plausible theory that oxidative phosphorylation was localized in the mitochondrion was a central consideration in evaluating the technique. The procedure for developing the evidence certainly was not independent of the theoretical claim it was supposed to support. Clearly, the technique was being calibrated by existing techniques and evaluated by its ability to produce evidence that fit a plausible theory.

\section*{Centrifugation Regimes}

Once material is prepared, it is ready to be placed into the centrifuge and spun. Many questions of procedure remained, however. How fast should the contents be spun, and for how long? For Bensley and Hoerr, who started with the objective of isolating mitochondria, the strategy was pretty straightforward: centrifuge long enough to separate what seemed to be a reasonably pure mitochondrial preparation. However, what came to be the dominant approach involved the separation of four different fractions. This involved successive centrifugation runs in which the sediment at each stage was removed and the remaining unsedimented material (called the supernatant) was subjected to centrifugation at a yet higher speed (see Figure 4.3). ${ }^{12}$ Why this procedure? Through it, as we will see in the next chapter, Claude isolated what appeared to be chemically distinct components that could be linked to four different components of living cells: the nucleus (in the sediment separated out by the first centrifugation run on the original homogenate), mitochondria (in the sediment of the second and third runs), microsomes (in the sediment of the fourth run), and cell sap or cytosol (soluble protoplasmic material remaining in the supernatant after the fourth run).

12 The following is Claude's description of his procedure: "The suspension was immediately centrifuged for one minute at 2000 r.p.m. in a horizontal centrifuge. This step was found to remove most of the liver fragments, the cells which had remained intact, the free nuclei and the red corpuscles. The supernatant fluid, or extract proper, which contained practically all the organic components equal to, or smaller than, three $\mu$ diameter, was spun at 18,000 r.p.m. in the high speed centrifuge, for exactly five minutes. At that speed, a five minute run was sufficient to bring down practically all the large secretory granules. The sediment was saved for further purification in the centrifuge. The small particles, which had remained in the supernate, were sedimented by a long run purified in the higher speed centrifuge, the 'long run' in this case being five minutes centrifugation at $18,000 \times$ gravity. The procedure consisted in suspending the material in water and sedimenting it again at high speed, four times in succession" (1941, p. 267).

Technical problems in centrifugation, as well as variations in materials and goals of the research, led researchers to vary such details as frequency of resolubilizing, length of runs, and speed of centrifugation. For example, due to differences in their initial distribution and other chance factors, some lighter particles sediment along with the heavy particles. In addition, during centrifugation particles will encounter the walls of the centrifugation tube where they may adhere or agglutinate and set up convection currents as they travel down the walls. Small particles may become entrapped and carried down with the larger particles. Empirical explorations revealed that these difficulties could, in part, be overcome by repeated resolubilization and recentrifugation of the sediment. Note again, though, that general expectations regarding the results guided the refinement of the technique.

Are there only four distinct constituents of cells that could be separated by fractionation? There were reasons to suspect more. One of the most powerful reasons was that cytologists already knew of organelles that were not distinguishable in the four basic fractions. For example, de Duve commented, "It may be recalled that the fate in differential centrifugation of such formations as secretory granules, centrosomes, and the Golgi body is entirely unsettled at the present time" (de Duve \& Berthet, 1954, p. 250). Shortly after Claude's early work, other researchers developed regimes to isolate additional fractions. Hubert Chantrenne, for example, used the Henroit-Huguenard centrifuge to obtain five fractions whose chemical composition blended into each other. He concluded, "it seems that one can partition the granules in as many groups as one wishes, with no experiments or observations indicating that there exist precise demarcation lines between the different group of particles" (Chantrenne, 1947, p. 445, as quoted in Rheinberger, 1997, p. 64). Novikoff et al. (1953) separated ten fractions and studied distributions of DNA, RNA, nitrogen, and several enzymes in them. As we will see in Chapter 6, procedures for separating additional fractions played a critical role in the 1950s in the discovery of additional cell organelles.

Nonetheless, the four-fraction approach remained dominant. Perhaps the best explanation for this is that these fractions were identified with cell structures for which functions were already suspected. In particular, although it is noteworthy that the scheme provided no place for the Golgi apparatus, some researchers still suspected this structure to be an artifact. Claude in fact collaborated with Palade in a pair of papers in 1949 arguing that it was indeed an artifact. (These papers are discussed in the last section of this chapter.) Until clear evidence as to the function of the Golgi apparatus was developed in the 1960s, it remained possible to doubt its existence and to accept fractionation procedures which did not give it a place.


![](https://cdn.mathpix.com/cropped/2024_07_05_6984272003135d7cd6e6g-1.jpg?height=424&width=1100&top_left_y=204&top_left_x=210

ChatGPT figure/image summary: The first image (Figure 4.1) is an optical illusion created by Roger N. Shepard to illustrate how our visual perception can be tricked. The image appears to show two table-like shapes, and the reader is asked to consider whether the shaded surfaces of these shapes are identical. Although they may appear to be different due to the perspective and context of the drawing, the shaded surfaces are typically designed to be the same shape in such illusions—this effect is used to demonstrate how context and perspective can influence perception.

The second image (Figure 4.3) is a schematic representation of a typical procedure for creating fractions by multiple centrifugations at increasing speeds. The graph might indicate sequential steps in centrifugation, where after each step, a specific fraction is obtained, and the remaining suspension is subjected to further centrifugation at a higher speed to separate more components based on their densities. This type of fractionation is commonly used in biochemistry and cell biology to isolate different organelles and molecular complexes from cells.)

Figure 4.1. Are the shaded surfaces identical in shape? Adapted from Roger N. Shepard (1990), Mind Sights. New York: Freeman, p. 48.

(Bechtel, 2001) but most of us are blissfully unaware of these. Fortunately, our visual system is very well adapted to the environment in which we live; accordingly, illusions as dramatic as the one generated by Figure 4.1 are rare.

Like our visual system, the instruments used by scientists procure information by performing complex operations on phenomena of interest. They are thus prone to generating artifacts. Because, unlike the visual system, these instruments have not been honed by natural selection over a long phylogenetic history, the risk of artifact is acute. As a result, evidence advanced using new instruments and new techniques for using existing instruments ${ }^{1}$ often is vigorously contested. Competing scientists question whether the evidence really reflects the phenomena of nature. The controversies surrounding new instruments and techniques are often the most bitter in science. Eventually they dissipate. The instruments and techniques for using them are refined so that the community of scientists agrees they are producing reliable information about the structure of the phenomena. Scientists publish manuals of standardized procedures and the community of investigators employs common tools for procuring evidence. ${ }^{2}$ The instruments and techniques for using them become, in the language of Latour (1987), black boxes.

${ }^{1}$ We typically are unaware not only that our senses are instruments but that there are procedures for using them. There are exceptional contexts in which we must train ourselves in the procedures for seeing particular objects - for example, to see an afterimage by first staring at a colored patch and then shifting to looking at a white wall. But with scientific instruments, the techniques for using them are often as important as the instrument itself in determining what evidence is produced. To keep this in focus, I usually will distinguish between instruments and the techniques for their use.

2 In biochemistry, for example, volumes in the series Methods in Enzymology began to be published in 1955 under the editorship of Sidney Colowick and Nathan Kaplan. In 1964 an annual publication, Methods in Cell Physiology, began to appear under the editorship of David M. Prescott. In 1973 the title changed to Methods in Cell Biology and has subsequently expanded to include multiple volumes per year.

![](https://cdn.mathpix.com/cropped/2024_07_05_3d55a20b51fcb0c16092g-1.jpg?height=1321&width=998&top_left_y=207&top_left_x=261

ChatGPT figure/image summary: The image you've provided is a schematic representation of a typical procedure for cell fractionation by successive differential centrifugations. The process is used to separate and purify the different components of a cell. Here's what is depicted in the steps of the diagram:

1. Homogenization: This step involves breaking open the cells in a solution, typically isotonic sucrose, to preserve the organelles. The device shown could be a homogenizer which is used to physically break up the cells.

2. First centrifugation (700 x g, 10 minutes): The initial centrifugation step separates larger cellular debris, like intact cells and nuclei, from the rest of the cellular components by spinning at relatively low speed. These large components sediment at the bottom of the tube, forming Sediment I, while the other components remain in the supernatant.

3. Second centrifugation (5000 x g, 10 minutes): The supernatant from the first centrifugation is further centrifuged at a higher speed. This results in the sedimentation of the mitochondria, which are then found in Sediment II.

4. Redispersion: The mitochondria from Sediment II are then redispersed, likely to wash them and remove any remaining contaminants.

5. Third centrifugation (24,000 x g, 10 minutes): Upon spinning at an even higher speed, further separation occurs. After this step, the pellet (Sediment II after washing) contains purified mitochondria.

6. Fourth centrifugation (54,000 x g, 60 minutes): Finally, an even higher speed centrifugation is performed. This step results in a separation where microsomes are found in Sediment III, and the remaining supernatant contains soluble materials from the homogenate.

Each step involves centrifuging the substance at a certain speed (measured in times gravity [x g]) for a certain duration (measured in minutes). The different speeds allow for separation of components based on their size and density, with heavier and larger particles sedimenting out of the suspension first. This diagram is used to illustrate a standard lab procedure for fractionating cells' organelles for further study of their structure and function.)

Figure 4.3. A schematic representation of a typical procedure for creating fractions by multiple centrifugations at increasing speeds. Reprinted from E. D. P. De Robertis, W. W. Nowinski, and F. Saez (1960), General Cytology (Third ed.). Philadelphia: W. B. Saunders Company, Figure 4.1 on p. 79, with permission from Elsevier.

\title{
Interpreting Fractionation Results
}

The immediate goal of cell fractionation is the determination of the chemical, especially enzymatic, composition of different cell organelles, which could then in turn support claims about the functions performed by those organelles. Typically after fractionation, however, investigators found a particular enzyme

\title{
Discovering Cell Mechanisms
}

distributed across more than one fraction. One interpretation of such results is that the same enzyme occurs in multiple organelles. Claude, however, viewed the fraction in which it was in highest concentration as the likely true locus and the smaller amounts found in other fractions as contamination resulting from inexactness in the process of fractionation. de Duve construed this as a major insight on Claude's part and expressed it in the postulate " $a$ given enzyme belongs to a single intracellular component in the living cell" (de Duve \& Berthet, 1954, p. 239). To this de Duve himself added a second postulate: "all members of a given subcellular population have the same enzymatic composition," which he labeled the postulate of biological homogeneity (1963-4, p. 52). ${ }^{13}$ Commenting on the value of these assumptions, de Duve wrote,

granting these two postulates, especially that of biochemical homogeneity, we can now use the enzymes as markers for their host particles and conduct tissue fractionation experiments very much like any other type of chemical fractionation. We may, at least in the initial analytical phase of the work, forget all about morphological features and treat suspensions of ground cells or tissues as mixtures of different populations of physical entities to be identified, characterized, resolved, and purified, with as sole guides the enzymes. If we can reach the final preparative phase and achieve sufficient purification, then the test of our working hypotheses will come, for morphological examination will show whether our deductions were in fact valid or not. (1963-4, p. 53)

Not everyone accepted these postulates. They constitute major assumptions about how cells are organized, and many investigators found it suspicious to invoke them in the very interpretation of fractionation studies. Why should cells follow a design principle in which each chemical constituent is limited to a particular organelle?

Cell fractionation was not the only way to localize chemicals in cell structures. When selective stains became available, histochemists used uptake of the stain to determine the locus of substances in slices or even whole mounts of tissues. David Glick, a pioneer in developing quantitative histochemical methods, called for use of such methods to vindicate the results of fractionation studies: "it would be best to seek proof of the specific localization by an independent method, i.e., one not dependent on centrifugal separations" (1953, p. 451). Glick can here be seen to be requesting consilience with other techniques before accepting cell fractionation results. In the case of some
\footnotetext{
${ }^{13}$ Claude was characterized by de Duve and Beaufay as altering the question investigators were asking from "what is in ...?" to "where is . . ?"
}

cell constituents, such as DNA, cytochemical techniques such as the Feulgen stain were sufficient to demonstrate a unique locus. de Duve agreed that cytochemical staining could provide a valuable test of the results of fractionation, but was unwilling to restrict fractionation studies to results that could be vindicated by histochemistry.

By the 1950 s cell fractionation was generally regarded as providing reliable evidence of the enzyme composition of cell organelles and thus of their function. Yet skeptics remained. James Danielli, for example, doubted that the results of fractionation were reflective of the activities in living cells: "so far there has been an almost complete lack of proof that the bodies isolated are in the same condition as in the intact cells" (1953, p. 7-8). F. K. Sanders offered a similarly negative assessment:

except in certain well-defined cases, little evidence has been offered that such fractions are in fact identical with known cellular structures. Moreover, it cannot be assumed that because a certain enzyme is found in isolated 'nuclei' or 'mitochondria' it is, in fact, present in this location in the living cell. Cells are highly complex colloidal systems, in which the distribution of substances between the different parts of the system is likely to be altered by procedures far milder than those [in cell fractionation]. (1951, p. 24) ${ }^{14}$

In the next chapter we will see that even while grounds for skepticism remained, and before a theoretical understanding of the process of fractionation had developed, researchers relied on cell fractionation in developing models of cell mechanisms. That this technique was producing (a) determinate results that (b) to some extent corresponded to results from other techniques and (c) fit into a developing mechanistic model of the cell was

${ }^{14}$ Even as enthusiastic an advocate of cell fractionation as de Duve expressed caution: "As a bridge between the fields of cytology and biochemistry, it [differential centrifugation] offers tremendous possibilities which even the most carefully worked out techniques of cytochemistry could never have been expected to fulfill. It must be remembered, however, that the application of differential centrifugation is fraught with many technical difficulties and open to a large number of errors. The methods that have been worked out today represent significant improvements over the earlier ones, but much remains to be done to augment their accuracy and selectivity. For this purpose it is important to have in mind the theoretical basis of the technique as well as the various factors of practical nature which have been found to affect the results.... The limitations of differential centrifugation become particularly severe when the technique is applied to the study of tissue enzymes. It is now quite clear that the observed partitions provide only the roughest sort of information concerning the true intracellular distributions of enzymes. They can only be considered as clues which have to be followed by many additional experiments in order to arrive at their real significance" (de Duve \& Berthet, 1954).

\title{
Discovering Cell Mechanisms
}

reason enough for many investigators to employ it as a basic tool in their attempts to understand cells.

\section*{3. THE ELECTRON MICROSCOPE AND ELECTRON MICROSCOPY}

Whereas cell fractionation enabled scientists to decompose cells into component parts in a way that permitted investigation of their function, the goal of microscopy was to visualize the component parts as they existed within the cell. (A secondary use of microscopy was to examine the fractionated parts of the cells to determine whether they corresponded to parts as they occurred in the intact cell.) Despite the advances that had been made using light microscopy in the nineteenth century, the wavelength of light presented a principled limit to the resolution that could be attained and rendered this instrument inadequate for studying the internal structure of cells. A number of variations on light microscopy developed in the 1940s, such as phase contrast ${ }^{15}$ and ultraviolet microscopy, extended its usefulness (for reviews, see Baker, 1951; Wyckoff, 1959), but the newly invented electron microscope offered the greatest opportunity. The wavelength of an electron depends upon its voltage, but even the relatively low-powered $50 \mathrm{kV}$ microscopes developed in the 1930 s and 1940 s theoretically permitted resolution down to 5-10 $\AA$ $(50-100 \mathrm{~m} \mu$ ), in contrast to the limits of approximately $2,500 \AA(.25 \mu)$ with a light microscope using white light and $1000 \AA$ with ultraviolet microscopes (Cosslett, 1955). Figure 4.4 shows schematically the difference in resolution between the light and electron microscope.

The electron microscope relies on the same general optical principles as the light microscope. Each requires a source of illumination, a condenser lens for controlling it, an objective lens, and one or more additional lenses for providing high magnification. The main differences between light and electron microscopes stem from the use of an electron beam, rather than a beam of light, for the illumination. One important consequence is that researchers had to adjust to not looking through the microscope, but instead viewing an image on a fluorescent screen or in a photograph. Another is that the image results not from the absorption of light of various wavelengths, but from the scattering of electrons. Scattering depends not on the chemical constitution of the object imaged, but on the amount of matter present - the
\footnotetext{
15 In phase contrast microscopy, phase differences in the light refracted from various structures in the cell are converted into differences in amplitude, which can be seen as differences in image contrast. This technique is particularly useful for in vivo studies.
}

![](https://cdn.mathpix.com/cropped/2024_07_05_04e93c00f0989932db48g-1.jpg?height=1049&width=705&top_left_y=207&top_left_x=403

ChatGPT figure/image summary: The image provided appears to be a schematic representation of a cell and its various organelles and structures, as seen through electron microscopy. The diagram includes labels for the following cellular components:

- Secretion vesicles
- Chromatin within a nucleus
- Nuclear sap
- Nucleolus
- Mitochondria
- Lipid molecules
- Cell membrane
- Basophil substance
- Golgi complex
- Cell center (possibly referring to the centrosome)

The image illustrates the greater level of detail about cell structures that can be observed using electron microscopy compared to light microscopy. With electron microscopy, the internal composition of the cell can be visualized with much higher resolution, revealing intricate subcellular organelles and molecular complexes. This type of imaging has been especially significant in the field of cell biology and has greatly advanced our understanding of cell structure and function.)

Figure 4.4. A representation of cell structure detail available in light versus electron micrographs. The center shows what can typically be seen with a light microscope, whereas the outer region reveals details of various organelles that could be seen in electron micrographs of the 1950s. Reprinted from E. D. P. De Robertis, W. W. Nowinski, and F. Saez (1960), General Cytology (Third ed.). Philadelphia: W. B. Saunders Company, Figure 3.9 on p. 71, with permission from Elsevier.

product of the number of atoms and their physical density or atomic number. In 1924, Louis de Broglie discovered the key theoretical concept on which electron microscopy depended, namely, that electrons have wave properties with wavelength inversely proportional to electron velocity. Two years later Hans Busch showed how to focus the electron beam by means of magnetic lenses. From these foundations, Max Knoll, Ernst Ruska, and Bodo von Borries developed transmission electron microscopes in the 1930s. In 1933 Ruska introduced the first true compound electron microscope, greatly increasing its resolving power. He then joined the firm of Siemens and Halske and, with

\title{
Discovering Cell Mechanisms
}

von Borries, designed the first commercially available electron microscope, which went into production in 1939.

The onset of World War II cut off access of the U.S. and its allies to the Siemens microscope, but Vladimir K. Zworykin, head of electronics research at RCA, had himself already begun work on an electron microscope. ${ }^{16} \mathrm{He}$ brought Ladislaus Marton, a Hungarian physicist working in Belgium who had just fled the Nazis, to the Camden, NJ, laboratory in 1938. While at the Free University of Brussels, Marton had built two microscopes of his own in the early 1930s, ${ }^{17}$ and with Zworykin he designed the RCA EM-A microscope. The EM-A had serious limitations in maintaining a vacuum, making it a poor candidate for commercial development. In early 1940, Zworykin hired James Hillier to replace Marton (who, with support from the Rockefeller Foundation, moved to Stanford to pursue technological improvements and chemical and biological applications of the electron microscope). As a graduate student of Eli F. Burton at the University of Toronto, Hillier was developing his own design for an electron microscope. At RCA, he designed what became the commercially viable RCA EM-B. Its availability was initially highly restricted during the war and only those with AA-1 priority were able to get on the waiting list to purchase one. ${ }^{18}$

Even during the war, opportunities emerged for biologists to use the new microscope and develop techniques appropriate for the study of cell structures. Pioneering these techniques was not for the faint-hearted. The most straightforward challenge was the need to master the numerous manual adjustments that were required to align the lenses to produce focused images. Two other challenges required considerably more effort and innovation. First, a way of preparing specimens sufficiently thin to be penetrated by the electron beam was needed. Second, specimens had to be transformed so as to survive conditions within the electron microscope and produce useful images.

16 For details on the development of the electron microscope, see Rasmussen (1997). Rasmussen hypothesized that RCA's interest in developing an electron microscope resulted from the fact that during the war it was blocked from taking advantage of the protocols it had developed for television: "it is possible that the prestige to be won during the war for RCA's cathode-ray technique via the electron microscope may have been one reason that RCA embarked on what must at first have seemed an unremunerative program in scientific patronage" (p. 31).

${ }^{17}$ Marton himself pioneered in imaging biological material - in 1934 he produced and published in Nature the first electron micrograph of a tissue section from a plant leaf fixed with $\mathrm{OsO}_{4}$ (Marton, 1934). Although his specimen was too thick to produce a very detailed image, Marton showed that biological material would not be destroyed by the electron beam. Nonetheless, various investigators continued to worry about that possibility in succeeding decades.

18 The RCA EM-B and a subsequent model, the RCA EMU, which became available in 1944, were the major tools for biological electron microscopy for the subsequent ten years until the Siemens Elmiskip-1 provided an alternative.

\title{
Obtaining Sufficiently Thin Specimens
}

The need for thin specimens stems from the fact that electrons had to pass through the specimen to create an image. If an electron encounters an obstacle, it is deflected (or loses energy). The relatively low-powered electron beam employed in the microscopes available in the 1940s and 50 s could penetrate only approximately $0.1 \mathrm{~mm}$ of biological tissue, a distance less than the thickness of a typical cell. This meant that the tissue had to be altered in some way before an image could be generated. Three strategies were pursued to produce sufficiently thin specimens: (a) make micrographs of structures that were already sufficiently thin; (b) section the specimens; or (c) induce specimens to spread thinly. ${ }^{19}$ All three approaches were pursued by early electron microscopists trying to image biological material.

(a) MICROGRapHS OF THIN Biological MaterialS. Many of the earliest electron microscopists restricted themselves to thin biological materials. Two groups of researchers were noteworthy for pursuing this strategy. In order to interest biologists in using the RCA electron microscope, Zworykin had secured a National Research Council postdoctoral fellowship. It was awarded to Thomas Anderson, a recent physical chemistry Ph.D. from the California Institute of Technology. During his fellowship, Anderson worked with Stuart Mudd, head of microbiology at the University of Pennsylvania, and Wendell Stanley, from the department of plant and animal pathology at the Rockefeller Institute's Princeton Laboratory. In his first research with the electron microscope, Anderson studied bacterial and viral specimens provided by Mudd and Stanley, which were sufficiently thin for electron microscopy (Stanley \& Anderson, 1941). ${ }^{20}$ After completing his fellowship, Anderson remained at the University of Pennsylvania, and in the 1940s focused much of his work on bacteriophages.

Francis O. Schmitt's group at MIT, which included Cecil Hall, Marie Jakus, and Richard Bear, also employed thin materials. In the early 1940s they were the only biologists with their own electron microscope (brought with a $\$ 70,000$ grant from the Rockefeller Foundation), which they used to
\footnotetext{
19 Shadow casting, whereby a heavy metal such as gold or chromium is deposited on the surface of a specimen at an oblique angle, which generates a shadow in the micrograph whose length corresponds to the height of the material on the specimen (Williams \& Wyckoff, 1946), was another method sometimes employed with success to image viruses and bacteria.

20 At approximately the same time, Helmut Ruska, Ernst Ruska's brother, was appointed director of a laboratory for visiting scientists at Siemens and published his own micrographs of bacteriophages (Ruska, 1941).
}

\title{
Discovering Cell Mechanisms
}

investigate virus and protein structure. They prepared their material by fragmenting cells and drying the contents. Schmitt's group produced micrograph images of a number of cell fibers including collagen (Schmitt, Hall, \& Jakus, 1942), tropomyosin (Schmitt, Hall, \& Jakus, 1943), sperm tails, cilia, and flagella (Schmitt, 1944-5). The micrographs revealed a high degree of unusual and unexpected periodic and asymmetric order in the construction of these fibers.

In general, working with thin material did not create any additional risk of artifact in electron microscopy (although risks loomed from the need to work in vacuum and to fix specimens). As noted, the electron micrographs often produced unexpected results since the resolution of electron micrographs was far beyond that of other imaging techniques. Here the definitiveness of the patterns found and their reliable production seems to have been sufficient to convince the investigators that they were not artifacts.

(в) CUTTING THIN SECTIONS. The traditional means of preparing thin material for light microscopy was to cut thin slices from a tissue block with a microtome, such as the Cambridge Rocking Microtome, designed in 1885 by Charles Darwin's son Horace. The problem was that the microtomes designed for light microscopy could at best cut sections $1-2 \mathrm{~mm}$ thick, nearly an order of magnitude too thick for electron microscopy. During the 1940s numerous laboratories tried to create a microtome that could cut sufficiently thin sections, initially with little success. The problem is familiar to anyone who has tried to thinly slice a food like bread: it rips and tears. ${ }^{21}$

One approach, ${ }^{22}$ suggested by theoretical calculations, was to use high speeds to reduce the stress on the knife edge and thereby produce cleaner slices (O'Brien \& McKinley, 1943). Fullam, together with Albert Gessler, the director of research at Interchemical, developed a microtome that worked at $57,000 \mathrm{rpm}$ and produced a cutting speed of 1,100 feet per second (Fullam

21 Anderson (1956, p. 217) offered a more technical description of the problem: "The processes occurring at the cutting edge of the knife, for example, are poorly understood, but a knife edge with a large included angle must introduce large shearing stresses both in the section and in the face of the block as it rakes across its surface. . . . as it starts to float off onto the water in the trough, every section is visibly compressed in the direction of the movement of the knife. This compression is partly reversed as the section spreads out on the surface of the aqueous acetone or dioxane used to support it, but the uniformity of the spreading, especially on a micro scale is not known."

22 Other approaches included cutting wedge-shape sections which would on one edge be sufficiently thin for electron microscopy (von Ardenne, 1939) and reducing the transmission ratio on standard microtomes (see Bretschneider, 1952, for a review of various microtomes introduced in the period).

\title{
Creating New Instruments and Research Techniques
}

\& Gessler, 1946), which they used to cut thin sections from rubber, acrylic resins, and nylon. ${ }^{23}$ Could this be made to work for biological specimens? Claude, together with Fullam, began to explore this in 1944-5, reporting that

if the cutting blade is mounted on a disc and rotated at 50,000 r.p.m. then sections can be made less than $1 \mu$ in thickness. It has been necessary to modify the usual techniques for preparation of the tissues. The only satisfactory fixative so far found is a dilute solution of osmic acid brought rapidly into contact with the cell by perfusion. A new type of embedding material has been used which will sublime at reduced pressure leaving the section ready for electron microscopy free from complicating foreign matter. (Annual Report, 1944-5, p. 76)

These slices, however, were still too thick for electron microscopy. In the Annual Report of his laboratory the following year, Claude indicated partial progress in creating still thinner slices: "During the year an experimental microtome, designed to give sections of $0.1 \mu$ has been built which gives promise of achieving the objective. An entirely satisfactory blade has not yet been found and further investigation of methods of fixation and embedding of tissues to render them suitable for thin sections is in progress" (p. 86). ${ }^{24}$ Although Claude and Fullam (1946) published some of the micrographs from their research, the pictures revealed distortions that were due to an insufficiently hard embedding agent and an insufficiently sharp blade.

Subsequently Claude collaborated with Joseph Blum, an engineer with the Rockefeller Institute, to develop a low-speed microtome in which the specimen passed the knife only once as it revolved on a disc or wheel, thereby reducing the potential for tearing the specimen. The microtome also included a liquid-filled trough mounted next to the knife into which the newly cut sections could float (see Palade, 1971). But, as Claude reported in his Harvey Lecture (1948), that did not solve the problems with fixation and embedding or of needing a sufficiently sharp blade for cutting. The embedding problems were largely resolved by Sanford Newman, Emil Borysko, and Max

${ }^{23}$ Rasmussen reported, "This ultramicrotome was first offered commercially, with an advertisement suggesting that purchasers were making a safe investment because the device could easily be converted into an ultracentrifuge. The section quality was not remarkably better than that delivered by slower microtomes, and the clouds of flying downlike sections it produced had to be collected with a fine butterfly net" (1997, p. 112).

24 Two other groups, Daniel Pease and Richard Baker at the University of Southern California, and L. H. Bretschneider in the Netherlands, were working without knowledge of Claude and Fullam's work due to a two-year delay in the appearance of his Harvey Lecture. Pease and Baker adapted a standard Spencer 820 microtome while Bretschneider adapted the Cambridge Rocking Microtome. Both groups ultimately faced the same problem as Claude - the lack of good embedding and adequate knives. (See Pease \& Porter, 1981.)

Swerdlow (1949a; 1949b), researchers at the National Bureau of Standards who introduced methacrylic resins as embedding agents as well as heat as the tool for advancing the tissue block toward the knife. Harrison Latta and Frank Hartmann (1950) solved the sharpness problem by introducing glass knifes. ${ }^{25}$ In the wake of these developments, Keith Porter, together with Blum, took up Claude's effort to develop an adequate microtome (Porter \& Blum, 1953). ${ }^{26}$ They were hardly alone in pursuit of a suitable microtome. In 1954 the New York Academy of Sciences held a workshop at which designers from throughout the East Coast met, and about a dozen examples were on display. But Porter and Blum succeeded in developing a microtome that not only cut reliably thin sections but also was easy to use. ${ }^{27}$ Its dominance was established when Ivan Sorvall, Inc., of Norwalk, Connecticut, began to produce it commercially a year later. The chief competitor was a European microtome developed by Fritiof Sjöstrand (1953a) and manufactured by L. K. B. Produkter AB, Stockholm. As a result of these advances, by the early 1950s

25 Robertson (1987) reminisced on Latta's introduction of glass knifes: "I remember clearly one Monday morning that Harrison came into the lab with a milk bottle from home and stated that he was going to make a glass knife. We all laughed at this strange idea, but I followed him up to the machine shop where he got a hammer and smashed the milk bottle and chose a small piece to use as a knife. He mounted a fragment on a dummy steel knife using a black glue, and he and Frank Hartmann proceeded to cut very thin sections that one could use without taking out the plastic. I believe this was the first time anybody had succeeded in getting high-quality sections of biological material routinely thin enough to use for direct study without removing the plastic" (p. 139). Pease added more flavor to the early investigation of knives: "it is amusing to recall the mystique that soon developed in defining and finding the 'perfect' stain-free glass that would make ideal fracture edges. The idea developed that very old glass was apt to be better than new glass (I had a prized piece of broken, heavy, plate glass salvaged from a pre-prohibition bar widow, still with some old gold lettering on it)" (1987, p. 51).

26 The microtome they produced was similar in some ways to the Cambridge Rocking Microtome, although Porter contended that the similarity was "simply accidental [since we] never had one of the Cambridge instruments in the laboratory at Rockefeller, and I have never seen one" (Porter, 1987). Instead, Porter and Blum credited Stanley Bennett: "In some respects also the microtomes reported on here are similar to an instrument, devoid of moveable bearings, which was observed in experimental stages of construction in H. S. Bennett's laboratory (University of Washington) in 1951. In it, the specimen was supported on the end of a bar and brought past the cutting edge by simply flexing the bar. It has no provision for avoiding the knife on the return stroke and for this and other reasons did not prove satisfactory"(Porter \& Blum, 1953, pp. 687-8). Porter (1987) credited Joseph Blum with adding the gimbal for universal motion of the bar and a mechanical advance.

27 Pease and Porter quoted the comments of Irene Manton twenty-five years later: "It was my privilege, soon after arrival in New York, to attend a meeting of the New York Academy of Sciences at which an array of devices for thin sectioning were displayed, some crude, others almost comically complex, but only the Porter-Blum behaved perfectly, cutting a clean ribbon of serial sections of the right thickness to order from a methacrylate block" (Pease \& Porter, 1981, p. 290s).

\title{
Discovering Cell Mechanisms
}

How is such agreement reached? Naively, one might think such agreement results from a detailed understanding of how the instruments and techniques operate. But, just as most of us do not understand the operation of our visual system, scientists frequently do not understand how their instruments and techniques work, at least at the level of detail that is required to counter concerns about artifacts. Golgi staining is an extreme but illustrative example. Camillo Golgi introduced the silver nitrate stain in the 1880s and, in the hands of Ramon y Cajal, it provided much of the evidence that established that neurons are discrete cells. What made it effective is that it stains only some of the neurons in a preparation, thereby making clear that the dendrites and axons of one neuron are not continuous with those of another. Yet, more than a century later scientists still do not understand why it stains only a few neurons in a preparation. Even though in most cases scientists do figure out how their instruments and techniques operate, much of that knowledge may not be obtained until long after the instruments and techniques have been put into common use. Moreover, as Ian Hacking pointed out, changes in the understanding of how the evidence is procured has little effect on its acceptance:

Visual displays are curiously robust under changes of theory. You produce a display, and have a theory about why a tiny specimen looks like that. Later you reverse the theory of your microscope, and you still believe the representation. Can theory really be the source of our confidence that what we are seeing is the way things are? (1983. p. 199)

These reflections pose an important question: how do scientists assess their instruments and techniques to determine whether they are providing information about the phenomena of interest or merely artifacts? This is a question about what I will call the epistemology of evidence. I will be exploring that question in this chapter in the context of discussing the instruments and research techniques that proved critical to the ability of cell biology to enter into the no-man's-land between cytology and biochemistry in the 1940s and beyond.

As the previous chapter demonstrated, before 1940 cytology and biochemistry had developed largely in isolation from each other. Many researchers in both fields recognized the potential for fruitful interaction, but made little headway due to the lack of research tools that could yield detailed images of cytoplasmic structures and help relate them to biochemical functions. By 1940, though, researchers were employing new instruments and developing techniques for using them toward these ends. As often happens, the instruments with the greatest impact had been developed in other fields of science. Biologists therefore had to develop specialized techniques for turning these

thin sections could be routinely produced and use of electron microscopy for studying cells exploded. Bretschneider (1952) reviewed thirty-seven articles published since 1950, and a year later Dalton identified twenty-five additional ones, providing a sense of how quickly the field took off once thin sections could be prepared.

Because the use of microtomes to prepare specimens for light microscopy was an established practice, and looking at slices of tissues was not itself problematic, once adequate microtomes were available, they generated few epistemic concerns. Distortions due, for example, to tearing by the knife edge could usually be readily recognized in the resulting micrograph. There were, though, concerns about how to interpret the structures seen in thin-section micrographs. These issues arose especially in the context of the endoplasmic reticulum, which I will discuss in Chapter 6. Before the breakthrough in developing thin sections for electron microscopy, though, important results had already been achieved with the electron microscope using the third technique, tissue culturing of cells.

(c) GRoWING TISSUes in cUltURe. The third approach to preparing thin specimens drew upon a technique developed for a very different purpose the growing of cells in culture. Ross Granville Harrison developed and Alexis Carrel further refined this technique to allow observers to track cell development. Tissue culture involves taking small pieces of an embryo and placing it in a medium of plasma and embryonic juice. To prepare the cultured specimen for microscopy it is deposited on a coverglass, inverted onto a slide that allows for excavation, and sealed with paraffin. In this environment, cells can grow and their development can be observed. Porter, working in Claude's laboratory, was teaching himself tissue culture techniques to provide material into which Claude could insert particles he was isolating by fractionation to test whether they would affect cell development. Porter recognized that tissue culturing provided specimens sufficiently thin for electron microscopy. He commented, "Although not a peer among the microscopists at the time, I was experienced enough to perceive that such diaphanous cells might be suitable for electron microscopy, at least in their thinner margins" (Porter, 1987).

The technique Porter developed for preparing tissue-cultured cells for electron microscopy was demanding. Cells are selected under a light microscope and

an area of the film surrounding these, a little larger than the mesh disc, is marked out. This area of film is then cut from the surrounding film with a fine sharp instrument or a pair of watchmaker's forceps. Thus freed, the bit of film with

\title{
Discovering Cell Mechanisms
}

cultured cells is gently peeled away from the glass until only a small corner remains attached. Kept under water in this way the thin sheet of plastic retains its smooth extended form so that adhering cells are not distorted. The small wire mesh disc, immersed beforehand in the washing bath, is now slipped under the film and the two are so manipulated that the film is spread over the screen's surface. They are then lifted from the bath, drained of water, and placed to dry over phosphorous pentoxide. (Porter, Claude, \& Fullam, 1945, p. 236)

Despite the authors' claim to have developed "relatively simple means" to make micrographs of cultured cells, the procedure was extremely delicate and not widely adopted. ${ }^{28}$ It yielded, though, a dramatic result: an image of the cytoplasm eight years before comparable images were available with thin slices. As I will discuss in the next chapter, it generated a line of research on a new structure, first identified in these micrographs as a lace-like reticulum. When thin-sectioning techniques were developed, Palade and Porter relied on their experience with micrographs of tissue-cultured cells to interpret the thin-section differently than most of their peers.

There were, though, reasons to be dubious of the micrographs of tissuecultured cells. Tissue-cultured cells are grown in abnormal conditions and the very spreading which made them suitable for electron microscopy could also engender artifacts. ${ }^{29}$ This was especially true of the lace-like reticulum that was first observed in these preparations. Porter, however, appeared to have no doubts as to the reality of this structure. Because it did not correspond to anything seen in light microscopy, his confidence could not be based on correspondence with other techniques. Rather, what convinced him that this was not an artifact seems to have been the power of the image itself together

28 Porter commented, "The preparation of adequate specimens was, at first, a discouraging process, but not totally so. I found that most cells able to grow in vitro would grow on Formvar-coated coverslips and that the Formvar film could be peeled from a glass surface and transferred under water to the EM grids then in use. When the grid, held between the points of watchmaker's forceps, was removed from the water and drained on filter paper, the Formvar film stretched over and adhered to the grid surface. The technique required a steady hand as well as determination and endurance. Approximately $50 \%$ of the specimens were satisfactory" (Porter, 1987, pp. 5960). He continued, "When the time came to put the specimen into the electron microscope, not much of worth was expected. To our everlasting delight, however, the first specimen was surprisingly good and served to introduce the observers to more structural information than had been expected or could be interpreted" (p. 60).

29 Novikoff commented on tissue culturing as a potential source of artifacts: "it reveals the capacities of the cultured cells, but perhaps not the actualities of those cells in the organized structure of the multicellular organism. The specialized milieu in which they are grown is quite different from that encountered naturally by cells embedded in tissue mucopolysaccharide or wedged in tightly among neighboring cells, as in epithelium - cells always under the controlling neural, hormonal and neurohumoral influence of the organism" (1959, p. 2).

with the fact that he could link the structure to speculation that cells must have a cytoskeleton that determined their shape (Needham, 1942).

\title{
Altering the Specimen to Survive Microscopy and Generate an Image
}

Beyond preparing a sufficiently thin specimen, electron microscopy required transforming the specimen into a condition that could both withstand the conditions in the electron microscope and produce a distinct image. In the electron microscope, specimens are subjected to vacuum, or near vacuum, conditions. The reason is that the electron microscope image is created by collisions of the electron beam with molecules, and collisions with air molecules would result in a haze in the final image. Unless researchers removed the major constituent of a cell - its water - prior to placing the cell in vacuum conditions, the water would vaporize and disrupt cell structures as it escaped. The removal of water could both shrink and distort the shape of the specimen and move or remove other cell constituents, raising serious risks of generating artifacts. In addition to dehydrating the cell, it was necessary to stabilize the structure in the cell to preserve the morphology against disruption resulting from either the removal of water or metabolic processes. This was most commonly achieved through fixation, a chemical treatment that both displaces the water and creates new chemical bonds that stabilize structures in the cell. Finally, because the resulting image was due to scattering of electrons, and because cellular materials are all of roughly the same density and will therefore scatter electrons roughly equally, it was necessary to stain those structures so that they would scatter more electrons. The chemical bonds created by many fixatives, though, also enhance scattering of electrons and thus served as a stain for electron microscopy. Thus, fixation was the central process in preparing the specimen for electron microscopy.

Because fixation involves the displacement of the water ${ }^{30}$ and the creation of new chemical bonds in the specimen, the process radically alters conditions in the cell. The charge of artifact accompanied the use of fixation since its introduction in the late nineteenth century. These charges were fueled by the investigations of Fischer (1899), who applied different fixatives to homogenous solutions of proteins such as albumose, gelatin, egg albumin, and peptone, and generated structures that were filamentous, reticular, or granular and resembled structures observed in fixed cells. The charge of artifact was further encouraged by the fact that the same item had a different
\footnotetext{
30 A major concern here was the fact that the fixative had to spread through the cell, creating currents which could displace soluble material or extract it from the cell altogether upon washing.
}

\title{
Discovering Cell Mechanisms
}

appearance when prepared with different fixatives, or even when prepared with the same fixative using different techniques to apply it. Further compounding the problem is that the process by which fixatives worked - for example, what substances in the cell they created bonds with - was generally unknown. ${ }^{31}$ This made it difficult to determine whether what was seen in a fixed cell was a structure that was originally in the cell or an artifact.

Although some scientists rejected all use of fixatives, most were convinced that they could in fact reveal real structure in living tissue. Lacking a theoretical understanding of how various fixatives worked, however, these investigators typically had to assess fixatives by their results. ${ }^{32}$ But what should the results look like? In light microscopy, investigators could sometimes compare the image of the fixed cell with that of living cells. Observation of the same structures in the unfixed cell provided evidence that the fixative was producing reliable results. Yet, the interest in fixation and staining was to observe structures that could not be seen in living cells. In such a case, researchers had to judge whether the image was likely to correspond to preexisting structures without independent access to that structure. Often, they relied on the plausibility of the image produced. Porter and Kallman (1953), for example, set out some criteria for judging from a micrograph whether the cell had been well fixed:

Probably the optimum requirements to be made are that the cell should not detectably change its shape under the action of the fixative and that its cytoplasm should not, under high resolution, show discontinuities and lacunae of irregular size and angular form. Such structures with associated surfaces, and density differences, if present in the ground substance of the living cell,

${ }^{31}$ Porter commented on the lack of understanding of how osmium tetroxide functions despite previous studies: "Such studies as those of Hardy (1899), Monckeberg and Bethe (1899), Fischer (1899), Mann (1902), Hofmann (1912), Berg (1927), and Baker (1945) have provided only round indications of the reactions of various fixatives, including osmium tetroxide with proteins and fats" (Porter \& Kallman, 1953, p. 127). Six years later Isidore Gersh commented, "It is an impressive commentary on our ignorance of the mechanism of fixation even now, that we do not know why osmium solutions should result in a homogeneous-appearing nucleus, while other fixatives like formol-Zenker result in nuclei with crisp chromatin clumps, or why Bouin's fixative preserves mitochondria only rarely, while potassium dichromate seldom fails to preserve them" (1959, p. 34). Gersh provided the following example of what was known about the operation of osmium: "Osmium oxidizes aliphatic and aromatic double bonds and sulfhydryl groups, alcoholic hydroxyl groups, and some amines. It also has an affinity for certain nitrogenous groups. Sites of oxidation of adjacent hydroxyl or ethylenic groups of adjacent molecules are thought to be bridged by reduced osmium" (1959, p. 38).

32 Baker comments, "Nowhere in cytological technique has empiricism run riot so freely as in the invention of fixative mixtures" (1942, p. 4).

\title{
Creating New Instruments and Research Techniques
}

would most probably scatter light in dark field microscopy, whereas under such examination the ground substance is normally quite clear and nonrefractile. (pp. 127-8)

Early electron microscopists drew upon the armory of fixatives already employed in light microscopy. In his early studies of electron microscopy with separated fractions, Claude explored the potential of a variety of fixatives (formaldehyde, potassium dichromate, osmium tetroxide). Likewise, he and Porter, in their first study with tissue-cultured cells also tried several fixatives (chromic acid, acid formaldehyde, Flemming's mixture) and concluded that osmium tetroxide $\left(\mathrm{OsO}_{4}\right)$ vapors or solutions generated the clearest and most detailed images. Formaldehyde, they found, failed to show cytoplasmic structures, while chromic acid caused the cytoplasm to shrink and revealed only very small granules in it, and Flemming's solution yielded granules and mitochondria with a very coarse appearance. They claimed that alcohol, acetic acid, and freeze-drying (see below) were even worse in producing images resembling living cells. The standard, thus, was whether the resulting micrographs looked the way researchers expected them to look.

From the outset of electron microscopy, osmium tetroxide was by far the most widely used fixative. It had a long history of use in light microscopy, having been first introduced by Max Shultze in 1865 in his studies of the marine protozoan, Noctiluca. Strangeways and Canti (1927) had compared it with several other classical fixatives and concluded that it produced the least detectable change, the most evident being an increase in light scattering by the nucleus, which they attributed to the development of a "fine precipitate" (p. 9).

One way of responding to the worries about chemical fixation relied on the use of an alternative preparation technique, freeze-drying, which we have already briefly encountered in the work of Altmann and also of Bensley. The technique had been pioneered by Altmann (1890) who froze small pieces of tissue and kept them over sulfuric acid in vacuo at a temperature of $-20^{\circ} \mathrm{C}$ for some days. The water dissipates directly into a gaseous state so that there is no intervening liquid phase that might distort the cell, as there is in chemical fixation. The result is a progressive dehydration of the tissue. Bensley and Gersh revived and improved Altmann's technique, in large part by freezing to even colder temperatures. ${ }^{33}$ Bensley and Gersh (1933a, p. 212) argued that
\footnotetext{
${ }^{33}$ Gersh (1932) described the principles underlying the technique: "In the freeze-drying method . . . the system is cooled very rapidly to such a low temperature $\left[-40^{\circ} \mathrm{C}\right]$ that the cohesive effects existing between ambient molecules at the instant of cooling predominate over all others; ideally all molecules are literally frozen in their tracts; they have no mobility; only occasionally does the translational energy of a surface molecule become great enough for it to escape; then it
}

\title{
Discovering Cell Mechanisms
}

this approach was unlikely to produce any serious distortion of the location of substances in the cell:

It may be assumed that the redistribution of substances in the cells frozen rapidly at a temperature of liquid air is only such as is determined by the formation of ice crystals. Apart from this pushing aside of the protoplasmic constituents by ice crystals, the distribution of protoplasm itself will remain for the most part unaltered.

In addition, freeze-drying would not remove the water soluble constituents of the cell. As they noted, the technique did run the risk of forming ice crystals that could dislocate other cell structures, but such artifacts were usually sufficiently gross that they were easily recognized. The disadvantages of freezedrying were that it was more difficult to perform and produced specimens that were extremely fragile.

Comparison of micrographs of freeze-dried cells with chemically fixed ones indicated that freeze-drying did not produce significantly different results from ordinary chemical drying. ${ }^{34}$ Fernández-Morán (1952), for example, explored whether freeze drying yielded superior images of nerve fibers. Previous studies using chemical fixation had detected thin filaments in nerve axons and had indicated the existence of concentric laminated structures formed by thin membranes on the sheath. Fernández-Morán indicated the factors that gave rise to his concerns about these micrographs: "since the preparation techniques employed involve fixation, dehydration and in many cases embedding and sectioning of the nerve fibres, the existence of these elements cannot be definitely established until the artefacts introduced by such manipulations have been adequately evaluated" (p. 282). In this case, he concluded, "A comparative study of fresh frozen sections from the same nerve segment, which were subjected either to freezing-drying or to parallel osmium fixation, shows that the axon and sheath structures revealed in both are nearly identical, but better preservation and contrast was achieved in the osmium fixed sections. For routine examinations osmium fixation of the thin fresh frozen sections would therefore appear to the method of choice" (p. 291).
\footnotetext{
leaves the surface for the highly evacuated vapor phase and is trapped in a liquid nitrogen trap or expelled through the pumping system. In the freeze-drying method the pumping is continued until all the water molecules have escaped in this way" (p. 206). The tissue was embedded in paraffin while still in vacuum, and then cut into section and mounted on slides.

34 Some researchers concluded that, if anything, freeze-drying produced greater artifacts: "Electron microscope studies of tissue culture preparations have shown that freeze-drying gives an incorrect picture of the structure of protoplasm, and it is reported that the shrinkage and distortion of certain micro-organisms such as Bacillus megatherium are even more marked after freeze-drying than after air-drying" (Drummond, 1950, pp. 92-3).
}

In this case, the correspondence of results produced by two independent techniques was the basis for resolving the chief worries about chemical fixation. It is important to note that in this case it was not a preestablished technique that provided confidence in chemical fixation but another technique being developed simultaneously. Moreover, as the quotations from FernandezMorán make clear, more than correspondence was at stake since he concluded that osmium yields better preservation. That judgment must have been based on independent expectations of what the image should look like, not simply on the comparison of the images produced by the two techniques. The detailed images resulting from chemical fixation evidently set the standard for freeze-drying. (For a contemporaneous review of chemical and freeze-drying techniques, see Bell, 1952.)

Although the comparison with freeze-drying resolved the general worry over chemical fixation, another concern stemmed from the fact that different chemical fixatives produced markedly different results. Bretschneider, for example, tested eighteen fixatives on the radicular cells of a plant and offered the following assessment: "The highest degree of coagulation was found to be produced only by mixtures of formalin and osmium tetroxide, preferably in combination with chromic acid or potassium dichromate. When these substances are used, the diameter of the plasma and karyolymph filaments (elementary parts of the finer structure) [is] reduced to the minimum. This condition is presumed to resemble the natural condition of plasma as closely as possible. All other fixation substances, especially acid, markedly flocculent and rapidly diffusing ones, were found to be unsuitable for electronmicroscopic purposes" (Bretschneider, 1952, p. 313; see also Afzelius, 1962).

Very quickly, osmium tetroxide won favor as the fixative of choice for electron microscopy. Again, we should inquire about the basis of this judgment. One clear virtue of osmium tetroxide was that the micrographs produced with it exhibited greater detail than those made with other fixatives. (See Figure 4.5, which schematically compares osmium, permanganate, and freeze-dried preparations.) Such detail could of course have been artifactual, at least in principle, but investigators found it compelling. The images were generally consistent with those produced with light microscopy and with freeze-drying. Moreover, as we shall see in the next chapter, the structures shown in the micrographs generally fit readily into developing accounts of cell mechanisms. The one clear exception to this will be the focus of the last section of this chapter.

Settling on osmium still left open important questions about fixation methodology. In the early 1950s, both Porter and Palade began to explore variations in the use of osmium tetroxide. As part of their effort to understand

\title{
Discovering Cell Mechanisms
}

![](https://cdn.mathpix.com/cropped/2024_07_05_5095e984f168cec8db82g-1.jpg?height=293&width=458&top_left_y=206&top_left_x=577

ChatGPT figure/image summary: The image you provided appears to be a schematic representation of a cell's internal structures, specifically focusing on the mitochondrion (labeled "M"), endoplasmic reticulum (labeled "ER"), and ribosomes (indicated by the letter "r"). This diagram is used to illustrate the differences in appearance of these organelles based on the preparation technique used: osmium fixation, potassium permanganate staining, and freeze-drying. The lines and dots around the organelles likely symbolize the clarity, contrast, or presence of certain structures as revealed by the different preparation methods when viewed under electron microscopy.

The mitochondrion, shown as an oval with inner folds, is the site of cellular respiration and energy (ATP) production. The endoplasmic reticulum is a network of membrane-bound tubes and sacs that function in protein and lipid synthesis as well as transport of materials within the cell. The dots labeled "r" represent ribosomes, which are the sites of protein synthesis within the cell. Overall, the image is a simplified representation meant to compare how the three different preparation techniques affect the visualization of cellular structures.)

Osmium fixation

![](https://cdn.mathpix.com/cropped/2024_07_05_5095e984f168cec8db82g-1.jpg?height=318&width=368&top_left_y=590&top_left_x=383

ChatGPT figure/image summary: The first image is a schematic diagram showing the effects of staining with osmium fixation on cellular structures such as mitochondria (M), endoplasmic reticulum (ER), and ribosomes (r). The structures appear well-defined and preserved.

The second image represents the effects of staining with permanganate on similar cellular structures. The contrast or preservation levels may be different compared to the osmium fixation.

The third image depicts the effects of freeze-drying on the same cellular components. The visualization might present less detail or a different pattern in comparison to the chemical fixatives.

The images are likely simplifications intended to demonstrate the differences in structural details that are visible when cells are prepared using each of these three methods.)

Permanganate

![](https://cdn.mathpix.com/cropped/2024_07_05_5095e984f168cec8db82g-1.jpg?height=333&width=352&top_left_y=591&top_left_x=801

ChatGPT figure/image summary: The image provided appears to be a schematic illustration of cellular components, specifically the ultrastructure of a cell as seen under an electron microscope. The shapes within the image likely represent cellular organelles or structures such as mitochondria (M), endoplasmic reticulum (ER), and ribosomes (r), which have been treated with three different preparation techniques: osmium fixation, permanganate staining, and freeze-drying. This illustration is meant to visually compare the effects of different staining or preparation methods on cellular components as observed in electron microscopy, as discussed in the text provided.)

Frozen dried

Figure 4.5. A schematic comparison of the effects of three different preparations staining with osmium, staining with permanganate, and freeze-drying -on mitochondria (M), endoplasmic reticulum (ER), and ribosomes (r). Reproduced with permission from G. H. Haggis, D. Michie, K. B. Roberts, and P. M. B. Walker (1964), Introduction to molecular biology. New York: John Wiley and Sons, Inc, Figure 5.1 on p. 114.

the operation of osmium vapors, Porter and Kallman explored the effect of extending the time of fixation from ten minutes to sixteen hours. They reported that with longer fixation "the formed bodies (the majority limited by membranes) become more sharply defined, while the diffuse and frequently fibrous components of the ground substance are removed" (Porter \& Kallman, 1952). The following year Porter and Kallman (1953) proposed that this was due to the materials of the matrix being decomposed to a state that allows them to diffuse out of the cell. Finally, they used osmium tetroxide to treat various homogenous materials, both lipid and protein, that were similar to those thought to occur in cells. With albumin and fibrinogen they reported the initial formation of a gel followed by return to a liquid state and argued that this corresponded to the effect in osmium fixed cells of an initial staining of a variety of materials followed by removal of the fibrous components.

Palade (1952b) undertook a systematic study of osmium fixation at different $\mathrm{pH}$ values. He found, for example, that unbuffered osmium resulted in acidity upon first contact with cell tissue, but that osmium buffered with

veronal caused the cytoplasm to appear more homogeneous. The resulting micrographs most closely resembled those Porter had produced with tissuecultured cells. Palade's buffered solution of $1-2 \% \mathrm{OsO}_{4}$ in a neutral Michaelis buffer became known as "Palade's Pickle" and was widely adopted. Dalton (1955) added chromium salts to cross-link proteins and stabilize them.

Although Palade's technique for staining cells with $\mathrm{OsO}_{4}$ was widely adopted, that did not mean that there were no controversies over its application. Sjöstrand took issue with Palade on several aspects of the technique. For example, while faulting the time lag in Palade's application of the fixative (which, he claimed, caused tissues to deteriorate), Sjöstrand also dismissed the importance Palade had placed on maintaining the $\mathrm{pH}$ of the specimen:

We owe a good deal to Palade (1952) who modified the osmium fixation to give a decent preservation of the fine structure of cells. His introduction of the veronalacetate buffer as a medium for the osmium tetroxide has improved the quality of fixation for several tissues. However, the $\mathrm{pH}$ of the osmium tetroxide seems not to be so important as he originally claimed. Variation of $\mathrm{pH}$ within a wide range from $\mathrm{pH} 4$, and in some cases even $\mathrm{pH} 2$, to $\mathrm{pH} 8$ does not substantially affect the result in many tissues, e.g., the kidney (Rhodin, 1954), the intestinal epithelium (Zetterqvist, 1956) and the sensory epithelium of the inner ear (Wersäll, 1956). Most of the great differences of the quality of fixation described by Palade (1952) in his study at a low resolution seem to represent various degrees of post-mortem changes. (1956a, p. 459)

To speed fixation, Sjöstrand injected fixative into live animals and immersed cut tissue in fixative as quickly as possible. He also claimed that the tonicity of the fixative, which Palade argued was not critical, was very important in preventing swelling:

The osmium tetroxide solution of Palade is strongly hypotonic and in several cases . . . hypotonic solutions of osmium tetroxide produce a more or less marked swelling of the cells. The importance of using isotonic solutions has therefore been stressed (Sjöstrand, 1953a). It has even been possible to obtain as good a preservation of the cell structure by using isotonic non-buffered solutions of osmium tetroxide as for instance a solution of isotonic sodium chloride. (Sjöstrand, 1956a, p. 459)

Rasmussen traced the methodological dispute between Palade and Sjöstrand in part to a difference in the use to which they put electron micrographs. For Palade, the use was primarily qualitative - to provide morphological perspective on the biochemical information generated, for example, from fractionation and functional analysis of mitochondria and mitochondrial fragments.

\title{
Discovering Cell Mechanisms
}

Sjöstrand's interest, in contrast, was quantitative - to determine with precision the size of cellular components so as to support models of the molecular architecture of membranes. This difference was manifest in their respective papers at the Third International Conference on Electron Microscopy, held in London in July, 1954. Sjöstrand asserted,

The quantitative attitude is stressed because it is necessary for the identification and classification of the different components . . . The electron microscope is an efficient measuring device making an exact quantitative description possible, provided it is mastered to give high resolution. (1956b, p. 35)

Rasmussen argued that for this reason Sjöstrand showed only micrographs of mitochondria arrayed longitudinally. In that position the membrane would be perpendicular to the electron beam, and the micrograph would reveal the true width of the membrane. Palade, in contrast, judged the methods of electron microscopy insufficiently reliable to support quantitative conclusions:

It seems, therefore, that the dimensions and spacings shown in fixed material by the electron microscope cannot be considered sufficiently true to nature to permit us to deduce the chemical composition and molecular architecture of a certain structure by finding out, as recently proposed (Sjöstrand, 1953a, b), which particular kind of molecule would best fit a given spacing. (1956c, p. 132)

The dispute between Sjöstrand and Palade also turned on the question of how to evaluate the accuracy of micrographs. For Palade, the critical criterion was consilience with results from other techniques, both other forms of microscopy and fractionation studies. ${ }^{35}$ Palade claimed that Sjöstrand's criterion was the detail in the micrograph itself:

Sjöstrand is of the opinion that fixation tends, in general, to disorganize the cytoplasm s;, in comparison with the situation in vivo, no structure is added, but structure may be subtracted. Accordingly the best fixative is considered to

35 Rasmussen (1995) maintained that Palade's strategy undercut the ability of electron microscopy to conflict with biochemical results: "Palade removed virtually all potential for conflict between the evidence from electron microscopy and that from biochemistry. The two experimental approaches spoke to separate domains: the former to questions of arrangement; the latter to questions of molecular structure and function" (p. 400). Sjöstrand, on the other hand, by proposing to use precise measurements so as to assess the molecular structure of membranes, set up the potential for results that conflicted with biochemistry. The fact that Palade's approach supported and did not threaten biochemists, according to Rasmussen, turned them into allies and helped insure greater success for his approach over that of Sjöstrand: "I would suggest that it was the different kind of relationship that Porter and Palade designed with biochemistry that ultimately made electron microscopic cytology of the Rockefeller school the dominant configuration" (p. 419).

instruments to their own use. Two techniques were especially successful in opening up the territory between biochemistry and cytology to exploration: (a) obtaining micrographs showing the fine structure of cells using the electron microscope; and (b) chemical analysis of cell fractions separated by means of the ultracentrifuge. This chapter will focus on development of these techniques in the 1940s and 1950s, focusing on the epistemic issues raised by their introduction (beyond those that had already been resolved in the fields in which the instruments had been developed). The surge of discoveries and new understandings of cell mechanisms made possible by these new instruments and techniques will then be the focus of Chapters 5 and $6 .^{3}$

\title{
1. THE EPISTEMOLOGY OF EVIDENCE: JUDGING ARTIFACTS
}

I used a compelling visual illusion to demonstrate that the concern with artifacts arises even before we turn to the use of instruments to procure evidence in science. The problem is much accentuated when instruments and the techniques for using them require manipulating and altering the phenomenon under investigation. Sometimes, as amply illustrated in cell fractionation, the manipulations and alterations are severe. A major goal of cell fractionation is to pinpoint the particular organelle in which each enzyme is found and, therefore, determine the biochemical reaction associated with the organelle. Using cell fractionation to pursue this goal requires radically disrupting the cell. Force is applied to break its tough plasma membrane, and the contents are dispersed into a medium different from that in living cells. The contents are then subjected to forces several thousand times that of gravity in the centrifuge. The underlying assumption is that different cell constituents will settle in order of their mass, with the components of greatest mass settling first. Typically, the process involves several iterations in which the contents may be resolubilized in yet different media and subjected again to centrifugation. (Figure 4.3 in Section 2 illustrates a common version of the procedure.) Each iteration generates what is called a fraction of the original material that is then assayed for the reactions it supports and hence what enzymes it contains.
\footnotetext{
3 Additional instruments and techniques that will be important at various points in the historical analysis that follows include techniques of histochemistry and cytochemistry, which used various stains to detect the location and sometimes quantify the amounts of DNA, RNA, and certain enzymes; the use of radioactive tracers to follow the migration of substances through the cell; various forms of spectroscopy that enabled the detection of reaction products; and several new types of microscopes, including phase contrast and fluorescent microscopes. A number of investigators also explored techniques for micromanipulation or microsurgery, which allowed them to dissect parts of cells, remove parts, and inject substances into them.
}

\title{
Creating New Instruments and Research Techniques
}

be that which leaves the specimen with a maximum of organization. Sjöstrand's view undoubtedly applies in numerous cases, but it supposes that there is always more order in a living than in a fixed specimen, which may not necessarily be true. (1956c, p. 133)

This controversy further illustrates how investigators evaluate techniques by their results and, in particular, by whether those results conform to expectations generated by theories of the phenomenon. For Palade, buffered osmium was the preferred fixative because it produced results that fit well with biochemistry and his own use of cell fractionation. As we will see in Chapter 6, this is particularly true of two new structures that appeared in the micrographs made with the new fixative - the cristae of the mitochondria and the ribosomes on the endoplasmic reticulum. For Sjöstrand what was critical was generating micrographs that seemed to support very precise measurements of the observed structures.

It is worth noting that even as most researchers had come to accept the reliability of micrographs produced with osmium fixation, there were still doubters. No less an investigator than Jean Brachet expressed his worries about relying on micrographs produced with osmium for definitive accounts of such cell structures as the endoplasmic reticulum:

One cannot help but admire the very beautiful photographs which have been published, during the past few years, in order to establish and demonstrate the fine structure of the ground cytoplasm; however, the interpretation of the findings is still open to question and the unpleasant problem of the possible artifacts must be raised. It is a matter of some concern that almost all of the work we have just described has been performed on buffered osmium tetroxide fixed tissue; it is hard to believe, from the daily experience of ordinary cytology, that one single fixative can give a trustworthy image of the cell. Careful studies by Frederic (1956), with the so-called 'anoptral' microscope, failed to show the existence of a reticulum in living cells; such a reticulum became conspicuous after osmium fixation only.... It is not impossible that the nice double membrane structure, which is so conspicuous in the ergastoplasm and which will be found again in other cell constituents, is nothing but a fixation artifact. Nor can we exclude the possibility that many structures, which do not fix osmium to any large extent, are invisible in present day electron microscopy. (1957, pp. 40-1)

\section*{4. A CASE STUDY OF AN ARTIFACT CHARGE}

To conclude this chapter, it will be useful to look in some detail at a specific case in which investigators raised the charge of artifact. The case involves the Golgi apparatus, which, as discussed in the previous chapter, had long

been the target of claims of being an artifact. Two of the central figures in developing techniques for electron microscopy, Claude and Palade, joined the legacy of those arguing that it was artifact. ${ }^{36}$ The case is ironic, because both researchers returned to work on the Golgi apparatus several years later, and Palade played a central role in revealing the function of the Golgi (see Chapter 6). But in 1949 they charged that it was an artifact of osmium fixation. The case they made is a model argument for demonstrating an artifact. Because the conclusion was ultimately rejected even by the authors, it reveals just how epistemologically difficult the evaluation of artifacts is.

In the first of their two papers, Palade and Claude (1949a) applied ethanol to homogenates created from a wide variety of cell types from several different species. They claimed it transformed lipid inclusions that occurred in the area where the Golgi apparatus was typically found into myelin figures. Such figures were known to be very polymorphic and unstable, an interesting property given the variable appearance of the Golgi apparatus. In cells, they argued, such myelin figures are not able "to expand freely, but were forced to adapt themselves to the spaces available between the masses of precipitated cytoplasm" (p. 44). Moreover, the appearance of the myelin figures varied with the concentration of ethanol. Under some conditions, their appearance was very similar to that of the Golgi apparatus: "For a certain range of ethanol concentrations, generally comprised between 40 and 55\%, the morphology and topography of the intracellular myelin figures are surprisingly similar to those assumed by the Golgi apparatus in corresponding cells" (p. 49). Palade and Claude summarized their results with ethanol:

Myelin figures can duplicate faithfully the numerous and different forms ascribed at various times to the Golgi apparatus. Thus they can take the appearance of massive, or canalicular networks, scattered strands, canaliculi, polymorphic bodies, "poly-systems" and "mono-systems". . . These facts strongly suggest that the Golgi apparatus may be a myelin figure or a complex of myelin figures artificially induced in cells by given cytological techniques. Such a hypothesis could reconcile the different aspects presented by cells of the same time when examined in the fresh condition or after the application of recognized cytological procedures. (p. 52)

In normal preparations, however, ethanol is not introduced until after staining with osmium tetroxide or silver nitrate, at which point the Golgi apparatus
\footnotetext{
36 According to Porter (Interview, 1987, University of Maryland, Baltimore County), Claude had become convinced that the Golgi apparatus was an artifact and initiated the project. Palade had recently been recruited into the laboratory by Claude and was drawn into the project of establishing that the Golgi apparatus was indeed an artifact.
}

is already visible. ${ }^{37}$ Accordingly, in their second paper Palade and Claude (1949b) explored the effects of the more common fixation and staining procedures employed for the Golgi apparatus. They also applied the usual Golgi techniques to lipid droplets of known chemical composition and on fractions isolated by cell fractionation. In all cases they found evidence that lipid droplets with high phospholipid content developed into myelin figures. In the homogenate preparations, the myelin figures exhibited "the same polymorphism, growth particularities, and typical intracellular distribution as the figures produced by ethanol and described in the previous paper" (p. 81). Among the morphological features exhibited was the division into two regions that a number of investigators had observed. In addition, Palade and Claude proposed to account for the success of Beams and King (1934) in segregating the Golgi apparatus through centrifugation: "The high-speed centrifuging experiments prove only that the lipid droplets can be displaced within the cell and that their specific gravity is less than that of the cytoplasm" (p. 92). Further, they attributed the evidence suggesting a role in cell secretion to the fact that "The haphazard growth of a myelin figure may occasionally bring it in contact with a bile capillary, secretion granules, or neutral fat drops. Likewise, developing myelin figures often penetrate between zymogen granules or mucus droplets which, subsequently, will seem embedded in the meshes of a Golgi apparatus" (pp. 92-3).

Palade and Claude also observed the formation process over time, and concluded that the wave of acidity moving through the preparation corresponded to the formation of the myelin figures. They also proposed a mechanism by which the acid could create the myelin figures through hydrolysis of the phospholipid molecules into more soluble substances that could then be rearranged into myelin figures. Once the acid had acted, Palade and Claude hypothesized, various electrolytes in the fixation media facilitated growth and stabilization of the myelin figures. Finally, further increase in electrolytes stopped the formation of new myelin figures and the slow-diffusing osmium tetroxide then fixed the figures.

Altogether, Palade and Claude provided as compelling a case for a structure being an artifact as one could desire. Not only were they able to generate images that resembled the Golgi apparatus and trace their development from phospholipid inclusions, they also offered a theoretical account
\footnotetext{
37 Palade and Claude noted, though, "it may be recalled that Golgi himself, in a modification of his original procedure, proposed the use of an ethanol mixture ( $32 \%$ final ethanol concentration) as a fixative, and recommended it specifically for its better, quicker, and more constant results in the demonstration of the apparatus" (1949b, p. 72).
}

\title{
Discovering Cell Mechanisms
}

of how the artifact was generated. They could account for the reliable generation of definite images (as well as the variability in appearance, which was a factor pointing to it being an artifact). In this case, the limited means of demonstrating the Golgi further helped their case since, once they had shown how the preferred means of establishing its existence could be generating an artifact, there was no countervailing independent evidence. Thus, they could effectively counter the primary evidence offered for the reality of the Golgi apparatus. They also could answer the claims suggesting a functional role for the Golgi apparatus, thereby undercutting the role of the theoretical account of cell operation in justifying the microscopical evidence.

Coming near the end of the long history of contention over the reality of the Golgi apparatus, Palade and Claude's study did little to alter opinions. Yet, it was a model for establishing that something is an artifact, and at least in their laboratory had the effect of prohibiting discussion of the Golgi apparatus over the next fifteen years. ${ }^{38}$

\section*{5. EQUIPPED WITH NEW INSTRUMENTS AND TECHNIQUES TO ENTER TERRA INCOGNITA}

According to the criteria set out earlier in this chapter, both cell fractionation and electron microscopy established themselves in the 1940s and 1950s as credible sources of evidence about component parts and operations within cells. Both provided patterns of results that were determinate and repeatable. Each secured consilience of the results it generated with those obtained in other ways and, especially, with each other. Finally, the results of each technique cohered well with emerging mechanistic models that accounted for a variety of cellular phenomena. The one major exception was when they delivered evidence on the Golgi apparatus, but this might be in part explicable as due to delays in finding a place for the Golgi apparatus in the mechanisms of the cell. In Chapter 6, I will show that once it was shown to comprise a mechanism that packaged proteins for secretion, doubts about its reality soon vanished.

38 The second edition of General Cytology (de Robertis et al., 1954) was one of the few sources to mention Palade and Claude's work, doing so at the conclusion of a general summary of research on the Golgi apparatus in a section in fine print. Noting that Palade and Claude's work would suggest that the Golgi region was the locus of phospholipid droplets, they commented, "The confirmation of this concept would bring about the elimination of the name Golgi apparatus and perhaps its change into Golgi substance" (p. 148).

\title{
Creating New Instruments and Research Techniques
}

Neither classical cytology, relying primarily on light microscopy, nor biochemistry, relying on homogenates, had been able to enter the terra incognita in which resided the mechanisms responsible for cell phenomena. The new tools of cell fractionation and electron microscopy, however, provided the necessary resources. Electron microscopy made it possible to identify component parts in the cell much smaller than those identifiable by light microscopy. Cell fractionation made it possible to examine the operations of units of larger scale than could survive the homogenation process. Moreover, the techniques could be used together - fractions could be examined in the electron microscope, providing a means of relating the operational units identified by fractionation with component parts identified in electron microscopy of whole cells. The challenge for the scientists was to produce new results with these tools. In the next two chapters I turn to how scientists met this challenge.

\title{
Discovering Cell Mechanisms
}

Other evidence is used to associate the fraction with a particular organelle. The overall procedure supports an inference as to what reactions and enzymes are located in which organelles. However, given the rather violent treatment the material undergoes, one must consider the possibility that it is the violent treatment itself that partly or wholly determines where material ends up. If so, the results are at least in part artifactual and do not provide clear evidence as to where the enzymes originated.

Because centrifugation is inherently a violent and disruptive process, one might wonder whether cell fractionation is exceptional in raising worries about artifacts. In the next section, I will show that many of the concerns target the procedures needed to prepare cells for centrifugation rather than the centrifugation procedure itself. For now it will suffice to indicate that many of the same kinds of issues arose even with electron microscopy, which superficially may seem to be a benign observational technique. The direct act of looking at a micrograph conceals what was done to generate the micrograph in the first place. In their handbook on electron microscopy, Mercer and Birbeck emphasized that in making a micrograph, a researcher is creating something artificial. The challenge is to make something artificial that can be traced sufficiently to the characteristics of the original specimen to support conclusions about that specimen:

The final micrograph contains elements contributed both by the original object and by the preparative technique applied to it. An important part of the interpretation of the micrograph thus turns on a consideration of these latter factors. There are three distinct successive steps to be considered in the preparation of biological materials for microtomy: fixation, dehydration and embedding. To these may be added a fourth, that of staining, which may however be carried out after sections have been cut. To obviate sterile discussions concerning 'artefacts', the electron microscopists should recognize at once the nature of these operations. Their object is the preparation of an artefact of a type which can be examined in the electron microscope, and which bears a sufficiently well-understood relation to some structure originally present in an organism to enable this structure to be deduced from an electron micrograph of the artefact. The interpretation of the micrograph thus rests on an analysis of these steps and requires an understanding of the physical and chemical effects of each. (Mercer \& Birbeck, 1972, p. 4-5)

4 The concern with artifact is not limited to electron micrographs but, as I had occasion to note several times in the previous chapter, was widespread for light microscopy as well. Robert Bensley identified both the reasons for suspecting artifacts and instances in which critics voiced the concern, sometimes in ways that turned out to be correct, sometimes not, in the history of microscopy: "The methods used by cytologists in the preparation of cells for microscopic

In this passage, Mercer and Birbeck tried to shift the discussion of artifacts to the more common sense in which an artifact is anything made by humans. The concern, however, as they make clear, is whether the micrograph can be interpreted so as to obtain good information about cell structure. That is, is the micrograph a reliable source of evidence or a mere artifact from which it is impossible to recover the structure of the cell with which the investigator began?

The concern with artifacts was widespread when cell fractionation and electron microscopy were first being introduced in the 1940 s and 1950s. Writing at the close of the 1950s, at which point the techniques had acquired general acceptance, Alex Novikoff referred to a number of concerns about artifacts that had been prevalent:

We can all recall the categorical assertion that hope for significant information concerning the in vivo function of subcellular particles was lost the moment the cell was disrupted or homogenized. Or, that the use of aqueous media like sucrose could yield only misleading particles, especially worthless nuclei. Or, that oxidative phosphorylation could never be retained once the organized structure of the complete mitochondrion was broken. Or, that electron microscopy was one huge blunder, based as it was on osmium artifacts. Or, that quantitative microspectrophotometry of stained tissue sections was deprived of meaning by the marked structural heterogeneity of the subcellular particles. Or, that enzyme destruction by fixative, and diffusion of reaction product during incubation, made staining methods worthless for demonstrating the intracellular in situ localization of enzymes, particularly important ones. (1959, p. 1)

By the time he was writing in 1959, though, Novikoff was putting all these worries in the past tense, referring to them as the "essentially destructive comments" by "gloomy critics" (p. 1). How in a relatively short time were these
\footnotetext{
examination are complicated and often brutal. They involve the treatment of the tissue with reagents which either violently precipitate proteins or slowly render them insoluble, followed by a series of solvents both aqueous and organic. The possibility of change is implicit in every step of this process, and it is the prime responsibility of the cytologist to inquire whether the structures which he displays in his preparation actually existed in the living cell, and if not, by what chemical process they have been produced. It is not surprising that during the history of this science, the results of investigation have been frequently challenged, sometimes on theoretical grounds based on biochemical experience, but often as the result of experiment. The list of structures which have been so challenged is a long one and includes most of the visible structures of the cell. To this list belong, among others, mitochondria, Nissl bodies of nerve cells, myofibrils of smooth and striated muscle, and the chromatin granules of the resting nuclei. All of these have been shown, by subsequent research, to be pre-existent in the living cell. On the other hand, artefacts have been produced by fixation. To this category belong the network of fibres described by the adherents of the reticular theory of the structure of the protoplasm, and the long filaments produced by Flemming in support of his filar theory" (1951, pp. 1-2).
}

\title{
Discovering Cell Mechanisms
}

concerns laid to rest? Despite the comments of Mercer and Birbeck cited earlier, it was not by developing a detailed understanding of how the techniques worked. An understanding of how, for example, the fixative osmium reacted with tissue preparations, would not be available until much later. If researchers lack a theoretical understanding of the procedures by which putative evidence is generated, how can they establish its reliability? The alternative is to focus on the results themselves.

At first, this suggestion to examine the results of a technique in order to vindicate it would seem to lead nowhere. If skeptics are questioning the evidential status of results, how can further consideration of them resolve the concerns? In fact, this strategy is not as circular as it might appear. To see this, assume for a moment that you are pioneering a procedure for procuring evidence. What might you demand of the results of the procedure before accepting them as evidence of something in the world?

The first thing you would likely do is examine whether there was a distinguishable structure or pattern in the results. If the results seemed randomly distributed, you would conclude that you had generated noise rather than picking up a signal from an underlying phenomenon. Likewise, if cell fractionation studies had shown that enzymes were scattered helter-skelter through the cell, scientists would not have been impressed. As it turned out, there was a pattern in the results, with particular enzymes appearing in high concentrations in one fraction and low in others, and other enzymes having higher concentrations in one of the other fractions. Such a pattern suggested that the distribution of enzymes in the fractions resulted in some way from their distribution in the original organelles within the cell. Likewise, a critical indicator that electron micrographs were revealing of cell structure was that visual patterns suggestive of structure appeared in them.

Of course, the structure could have been generated by the technique itself. That is just what the charge of artifact asserts. However, often in practice the burden of proof is on the person alleging that the results are an artifact. If a plausible account of how the technique itself could generate the structure is offered, that, as we shall see, is taken as a reason to suspect artifactual findings. But scientists are impressed by structure in their results, and when they find it, they are inclined to interpret it as reflecting the phenomenon itself and not an artifact of how the evidence was produced. ${ }^{5}$
\footnotetext{
${ }^{5}$ I asked Keith Porter (interview, 1987, University of Maryland, Baltimore County) whether, when he examined his first micrograph of a cell (see Figure 5.1), he had any doubt about whether it was revealing something about the cell. He responded no - the structure in the micrograph was just too impressive.
}

If your new procedure generates structured results, you will want to repeat the procedure before announcing them publicly. No matter how clear a pattern you obtained the first time, if attempts at replication fail, it is highly likely that the structure was merely a result of some unnoticed feature of the way you performed the procedure, not the underlying phenomenon. Likewise, if the pattern disappears with seemingly trivial variations in the procedure, that again points to the results being due to the procedure, not the underlying phenomenon. Moreover, if other researchers are not able to replicate your results, they will be suspicious that something unnoticed or unreported in your procedure was producing the result. (Suspicions do not always lead to a negative outcome. Researchers know it can be tricky to replicate results on a phenomenon that, though genuine and perhaps even important, is fragile or dependent on very particular circumstances. At the other extreme, thankfully rare, a result that is not replicated may turn out to have been fraudulent. Questions of the repeatability of results are not taken lightly in science and can take considerable time and effort to resolve.) Bearing in mind that adjudication is not always easy, we can summarize this first criterion as holding that results should exhibit a determinate pattern and be repeatable.

If your results survive this first test, your second step would likely involve comparing them to results obtained through older, already established techniques. This is akin to comparing a crime witness's account to that of other witnesses and other sources of evidence. When at least some aspects of the results produced by the new technique can be related to what was already known by other techniques, that does help establish the credibility of the new technique. This is a straightforward application of what William Whewell called the consilience of induction: "The consilience of Induction takes place when an Induction, obtained from one class of facts, coincides with an Induction obtained from another different class. This Consilience is a test of the truth of the theory in which it occurs" (1840, p. xxxix, Aphorism xiv). ${ }^{6}$

There are limitations on how far consistency can take investigators toward evaluating a new instrument. One limitation stems from the fact that the purpose of developing new instruments and techniques is to secure data about phenomena for which existing techniques are inadequate. This means that, for at least some cases in which you apply the new technique, there will be
\footnotetext{
${ }^{6}$ Cytologist John Baker issued a particularly strong version of this requirement, insisting that all results with fixation of cells be compared to results from unfixed cells: "the only evidence of what is good or bad is comparison with the living cell. This point cannot be too strongly stressed: subjective ideas of what final result is good and what bad should never be allowed to form in the mind except on the solid ground of comparison with what is visible in the cell while still unaffected by reagents" (1942, p. 4. The passage is repeated in the second edition, 1950).
}

no existing technique providing comparable results. You may nonetheless find certain contexts in which you can apply both techniques. That would reduce the concern about error but would not irrefutably establish that the technique is reliable with respect to the all new phenomena for which data is sought. ${ }^{7}$

There is, however, a more serious limitation to this approach to vindicating new techniques. The critical assumption in appealing to the consilience of different inductions is that the different inductions are independent of each other. When they are completely independent, though, they may not naturally align with each other. It often takes a great deal of manipulation to get a new technique to produce evidence corresponding to what has been procured previously with other techniques. This often requires taking the older technique as a reference point and altering the new technique so as to generate corresponding results. In this way, the older technique is used to calibrate the new one (Bechtel, 2002b). Although the techniques are not independent in this procedure, successful calibration provides evidence that the new technique is capturing the same underlying phenomenon as the older technique and thereby indicates that there is an objective phenomenon on which both techniques are aligning.

Suppose that your new procedure reliably produces results that are consistent, or can be made consistent, with older techniques in areas of overlap. That is, you have to some extent satisfied the second criterion. Now let us focus instead on results that extend beyond what other techniques provide, because these are where the new technique might advance inquiry. Imagine that these new results do not make sense. They do not suggest or fit into any plausible theoretical account (mechanistic model) of the phenomenon under investigation. Moreover, you are not able to generate such an account. In this situation, you are likely to be suspicious of the new results and seek an
\footnotetext{
7 A failure of correspondence, moreover, may also be due to artifacts created by the older technique. Cosslett emphasized this point in a chapter on electron microscopy as a technique: "It is also necessary to plead for open-mindedness in comparing electron micrographs with the results of the established optical methods. Long familiarity with them unconsciously instills the feeling that they show exactly 'what the specimen really looks like.' In the case of stained specimens obviously, but to some extent also in non-stained preparations, this cannot be true, since there is always the possibility of artifact even in optical preparations. The greater danger of artifact formation in electron microscopy should not blind one to the fact that it gives pictures that are valid within their own limitations. It is only necessary to realize that these are different limitations from those of optical methods. In short, each technique provides evidence of value, each gives a partial view of the constitution and structure of the specimen under examination, and it is the task of the research worker to correlate critically all the clues and deduce from them a coherent answer to the puzzle set by Nature" (1955, p. 524).
}


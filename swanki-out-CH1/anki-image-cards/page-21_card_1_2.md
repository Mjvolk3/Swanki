### Card 1

![](https://cdn.mathpix.com/cropped/2024_05_18_dc0381fb1e39cc4997a4g-1.jpg?height=996&width=1470&top_left_y=225&top_left_x=171)
  
%
  
What does Figure 1.16 illustrate regarding the computational requirements for training state-of-the-art neural networks over time?

%

Figure 1.16 illustrates the number of compute cycles needed to train state-of-the-art neural networks over time. The plot shows two distinct phases of exponential growth in computational requirements, with computation measured in petaflop/s-days. The vertical axis is on a logarithmic scale, and the plot marks key milestones such as the Perceptron, LeNet-5, AlexNet, and AlphaGo Zero. The two phases are termed the "First Era" and the "Modern Era," indicating periods with different doubling times for computational requirements.

- #machine-learning, #neural-networks, #computational-growth

### Card 2

![](https://cdn.mathpix.com/cropped/2024_05_18_dc0381fb1e39cc4997a4g-1.jpg?height=996&width=1470&top_left_y=225&top_left_x=171)

What do the different doubling times in Figure 1.16 signify?

%

The different doubling times in Figure 1.16 signify two epochs of growth in the computational requirements to train neural networks. The "First Era" shows a doubling time consistent with Moore's Law, approximately two years, reflecting slower growth. In contrast, the "Modern Era" exhibits a much steeper increase, with a doubling time of about 3.4 months, indicating a rapid escalation in computational demand due to advancements in deep learning techniques.

- #machine-learning, #neural-networks, #exponential-growth

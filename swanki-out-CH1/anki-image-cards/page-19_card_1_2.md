### 1st Card

What does Figure 1.15 represent, and how does information flow through this neural network?

![](https://cdn.mathpix.com/cropped/2024_05_18_a86eb08e4ac380f84a91g-1.jpg?height=493&width=669&top_left_y=230&top_left_x=975)

%

Figure 1.15 represents a simplified schematic of a feed-forward neural network. It shows three levels or layers of units: input units on the left, hidden units in the middle, and output units on the right. Each input unit is connected to every hidden unit, and each hidden unit is connected to every output unit. The arrows symbolize the direction of data flow through the network, moving from the input units through the hidden units and finally to the output units.

- #computer-science, #machine-learning.neural-networks, #diagram

### 2nd Card

What is the significance of using differentiable activation functions in the neural network depicted in Figure 1.15?

![](https://cdn.mathpix.com/cropped/2024_05_18_a86eb08e4ac380f84a91g-1.jpg?height=493&width=669&top_left_y=230&top_left_x=975)

%

Using differentiable activation functions in the neural network is significant because it allows the error function's derivatives with respect to the network parameters to be evaluated. This enables the use of gradient-based optimization methods to train the network by adjusting the parameters to minimize the error function, thus allowing for networks with more than one layer of parameters to be effectively trained.

- #computer-science, #machine-learning.activation-functions, #gradient-descent
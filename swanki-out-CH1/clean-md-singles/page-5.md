![](https://cdn.mathpix.com/cropped/2024_05_18_f994bbac8ad9a581d276g-1.jpg?height=777&width=1521&top_left_y=222&top_left_x=148)

Figure 1.3 Synthetic face images generated by a deep neural network trained using unsupervised learning. [From https://generated.photos.]

\title{
1.1.4 Large language models
}

One of most important advances in machine learning in recent years has been the development of powerful models for processing natural language and other forms of sequential data such as source code. A large language model, or LLM, uses deep learning to build rich internal representations that capture the semantic properties of language. An important class of large language models, called autoregressive language models, can generate language as output, and therefore, they are a form of generative AI. Such models take a sequence of words as the input and for the output, generate a single word that represents the next word in the sequence. The augmented sequence, with the new word appended at the end, can then be fed through the model again to generate the subsequent word, and this process can be repeated to generate a long sequence of words. Such models can also output a special 'stop' word that signals the end of text generation, thereby allowing them to output text of finite length and then halt. At that point, a user could append their own series of words to the sequence before feeding the complete sequence back through the model to trigger further word generation. In this way, it is possible for a human to have a conversation with the neural network.

Such models can be trained on large data sets of text by extracting training pairs each consisting of a randomly selected sequence of words as input with the known next word as the target output. This is an example of self-supervised learning in which a function from inputs to outputs is learned but where the labelled outputs are obtained automatically from the input training data without needing separate human-
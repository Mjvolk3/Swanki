![](https://cdn.mathpix.com/cropped/2024_06_13_855eab66de586ca6078dg-1.jpg?height=367&width=554&top_left_y=207&top_left_x=733)

Figure 2.13: Logistic regression on the 3-class, 2-feature version of the Iris dataset. Adapted from Figure of 4.25 [GÃ©r19]. Generated by iris_logreg.ipynb.

This maps $\mathbb{R}^{C}$ to $[0,1]^{C}$, and satisfies the constraints that $0 \leq \operatorname{softmax}(\boldsymbol{a})_{c} \leq 1$ and $\sum_{c=1}^{C} \operatorname{softmax}(\boldsymbol{a})_{c}=$ 1. The inputs to the softmax, $\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})$, are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function. To see this, let us divide each $a_{c}$ by a constant $T$ called the temperature. ${ }^{8}$ Then as $T \rightarrow 0$, we find

$$
\operatorname{softmax}(\boldsymbol{a} / T)_{c}= \begin{cases}1.0 & \text { if } c=\operatorname{argmax}_{c^{\prime}} a_{c^{\prime}} \\ 0.0 & \text { otherwise }\end{cases}
$$

In other words, at low temperatures, the distribution puts most of its probability mass in the most probable state (this is called winner takes all), whereas at high temperatures, it spreads the mass uniformly. See Figure 2.12 for an illustration.

\title{
2.5.3 Multiclass logistic regression
}

If we use a linear predictor of the form $f(\boldsymbol{x} ; \boldsymbol{\theta})=\mathbf{W} \boldsymbol{x}+\boldsymbol{b}$, where $\mathbf{W}$ is a $C \times D$ matrix, and $\boldsymbol{b}$ is a $C$-dimensional bias vector, the final model becomes

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\operatorname{Cat}(y \mid \operatorname{softmax}(\mathbf{W} \boldsymbol{x}+\boldsymbol{b}))
$$

Let $\boldsymbol{a}=\mathbf{W} \boldsymbol{x}+\boldsymbol{b}$ be the $C$-dimensional vector of logits. Then we can rewrite the above as follows:

$$
p(y=c \mid \boldsymbol{x} ; \boldsymbol{\theta})=\frac{e^{a_{c}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}
$$

This is known as multinomial logistic regression.

If we have just two classes, this reduces to binary logistic regression. To see this, note that

$$
\operatorname{softmax}(\boldsymbol{a})_{0}=\frac{e^{a_{0}}}{e^{a_{0}}+e^{a_{1}}}=\frac{1}{1+e^{a_{1}-a_{0}}}=\sigma\left(a_{0}-a_{1}\right)
$$

so we can just train the model to predict $a=a_{1}-a_{0}$. This can be done with a single weight vector $\boldsymbol{w}$; if we use the multi-class formulation, we will have two weight vectors, $\boldsymbol{w}_{0}$ and $\boldsymbol{w}_{1}$. Such a model is over-parameterized, which can hurt interpretability, but the predictions will be the same.
\footnotetext{
8. This terminology comes from the area of statistical physics. The Boltzmann distribution is a distribution over states which has the same form as the softmax function.
}

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license
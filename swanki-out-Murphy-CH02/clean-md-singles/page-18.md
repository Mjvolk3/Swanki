![](https://cdn.mathpix.com/cropped/2024_06_13_2a2dfc8685cf6835049eg-1.jpg?height=521&width=1271&top_left_y=191&top_left_x=380)

Figure 2.9: Illustration of the binomial distribution with $N=10$ and (a) $\theta=0.25$ and (b) $\theta=0.9$. Generated by binom_dist_plot.ipynb.

where the symbol means "is sampled from" or "is distributed as", and Ber refers to Bernoulli. The probability mass function (pmf) of this distribution is defined as follows:

$$
\operatorname{Ber}(y \mid \theta)= \begin{cases}1-\theta & \text { if } y=0 \\ \theta & \text { if } y=1\end{cases}
$$

(See Section 2.2.1 for details on pmf's.) We can write this in a more concise manner as follows:

$$
\operatorname{Ber}(y \mid \theta) \triangleq \theta^{y}(1-\theta)^{1-y}
$$

The Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of $N$ Bernoulli trials, denoted $y_{n} \sim \operatorname{Ber}(\cdot \mid \theta)$, for $n=1: N$. Concretely, think of tossing a coin $N$ times. Let us define $s$ to be the total number of heads, $s \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=1\right)$. The distribution of $s$ is given by the binomial distribution:

$$
\operatorname{Bin}(s \mid N, \theta) \triangleq\binom{N}{s} \theta^{s}(1-\theta)^{N-s}
$$

where

$$
\binom{N}{k} \triangleq \frac{N!}{(N-k)!k!}
$$

is the number of ways to choose $k$ items from $N$ (this is known as the binomial coefficient, and is pronounced "N choose k"). See Figure 2.9 for some examples of the binomial distribution. If $N=1$, the binomial distribution reduces to the Bernoulli distribution.

\title{
2.4.2 Sigmoid (logistic) function
}

When we want to predict a binary variable $y \in\{0,1\}$ given some inputs $\boldsymbol{x} \in \mathcal{X}$, we need to use a conditional probability distribution of the form

$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\operatorname{Ber}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022
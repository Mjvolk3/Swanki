![](https://cdn.mathpix.com/cropped/2024_06_13_7978c08eaaee0a4861dag-1.jpg?height=331&width=449&top_left_y=240&top_left_x=429)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_13_7978c08eaaee0a4861dag-1.jpg?height=329&width=434&top_left_y=241&top_left_x=1143)

(b)

Figure 2.14: Linear regression using Gaussian output with mean \(\mu(x)=b+w x\) and (a) fixed variance \(\sigma^{2}\) (homoskedastic) or (b) input-dependent variance \(\sigma(x)^{2}\) (heteroscedastic). Generated by linreg_1d_hetero_tfp.ipynb.

\title{
2.6.3 Regression
}

So far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form

\[
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right)
\]

where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.

It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:

\[
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid \boldsymbol{w}^{\top} \boldsymbol{x}+b, \sigma^{2}\right)
\]

where \(\boldsymbol{\theta}=\left(\boldsymbol{w}, b, \sigma^{2}\right)\). See Figure 2.14(a) for an illustration of this model in 1d. and Section 11.2 for more details on this model.

However, we can also make the variance depend on the input; this is called heteroskedastic regression. In the linear regression setting, we have

\[
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid \boldsymbol{w}_{\mu}^{\top} \boldsymbol{x}+b, \sigma_{+}\left(\boldsymbol{w}_{\sigma}^{\top} \boldsymbol{x}\right)\right)
\]

where \(\boldsymbol{\theta}=\left(\boldsymbol{w}_{\mu}, \boldsymbol{w}_{\sigma}\right)\) are the two forms of regression weights, and

\[
\sigma_{+}(a)=\log \left(1+e^{a}\right)
\]

is the softplus function, that maps from \(\mathbb{R}\) to \(\mathbb{R}_{+}\), to ensure the predicted standard deviation is non-negative. See Figure 2.14(b) for an illustration of this model in 1d.

Note that Figure 2.14 plots the \(95 \%\) predictive interval, \([\mu(x)-2 \sigma(x), \mu(x)+2 \sigma(x)]\). This is the uncertainty in the predicted observation \(y\) given \(\boldsymbol{x}\), and captures the variability in the blue dots. By contrast, the uncertainty in the underlying (noise-free) function is represented by \(\sqrt{\mathbb{V}\left[f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta})\right]}\), which does not involve the \(\sigma\) term; now the uncertainty is over the parameters \(\boldsymbol{\theta}\), rather than the output \(y\). See Section 11.7 for details on how to model parameter uncertainty.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license
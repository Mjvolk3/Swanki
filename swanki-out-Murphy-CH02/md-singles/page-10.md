![](https://cdn.mathpix.com/cropped/2024_06_13_398d6182f58c2c67baf7g-1.jpg?height=329&width=498&top_left_y=239&top_left_x=751)

Figure 2.4: Illustration of a mixture of two 1d Gaussians, \(p(x)=0.5 \mathcal{N}(x \mid 0,0.5)+0.5 \mathcal{N}(x \mid 2,0.5)\). Generated by bimodal_dist_plot.ipynb.

To prove this, let us suppose, for simplicity, that \(X\) and \(Y\) are both discrete rv's. Then we have

\[
\begin{aligned}
\mathbb{E}_{Y}[\mathbb{E}[X \mid Y]] & =\mathbb{E}_{Y}\left[\sum_{x} x p(X=x \mid Y)\right] \\
& =\sum_{y}\left[\sum_{x} x p(X=x \mid Y=y)\right] p(Y=y)=\sum_{x, y} x p(X=x, Y=y)=\mathbb{E}[X]
\end{aligned}
\]

To give a more intuitive explanation, consider the following simple example. \({ }^{3}\) Let \(X\) be the lifetime duration of a lightbulb, and let \(Y\) be the factory the lightbulb was produced in. Suppose \(\mathbb{E}[X \mid Y=1]=5000\) and \(\mathbb{E}[X \mid Y=2]=4000\), indicating that factory 1 produces longer lasting bulbs. Suppose factory 1 supplies \(60 \%\) of the lightbulbs, so \(p(Y=1)=0.6\) and \(p(Y=2)=0.4\). Then the expected duration of a random lightbulb is given by

\[
\mathbb{E}[X]=\mathbb{E}[X \mid Y=1] p(Y=1)+\mathbb{E}[X \mid Y=2] p(Y=2)=5000 \times 0.6+4000 \times 0.4=4600
\]

There is a similar formula for the variance. In particular, the law of total variance, also called the conditional variance formula, tells us that

\[
\mathbb{V}[X]=\mathbb{E}_{Y}[\mathbb{V}[X \mid Y]]+\mathbb{V}_{Y}[\mathbb{E}[X \mid Y]]
\]

To see this, let us define the conditional moments, \(\mu_{X \mid Y}=\mathbb{E}[X \mid Y], s_{X \mid Y}=\mathbb{E}\left[X^{2} \mid Y\right]\), and \(\sigma_{X \mid Y}^{2}=\mathbb{V}[X \mid Y]=s_{X \mid Y}-\mu_{X \mid Y}^{2}\), which are functions of \(Y\) (and therefore are random quantities). Then we have

\[
\begin{aligned}
\mathbb{V}[X] & =\mathbb{E}\left[X^{2}\right]-(\mathbb{E}[X])^{2}=\mathbb{E}_{Y}\left[s_{X \mid Y}\right]-\left(\mathbb{E}_{Y}\left[\mu_{X \mid Y}\right]\right)^{2} \\
& =\mathbb{E}_{Y}\left[\sigma_{X \mid Y}^{2}\right]+\mathbb{E}_{Y}\left[\mu_{X \mid Y}^{2}\right]-\left(\mathbb{E}_{Y}\left[\mu_{X \mid Y}\right]\right)^{2} \\
& =E_{Y}[\mathbb{V}[X \mid Y]]+\mathbb{V}_{Y}\left[\mu_{X \mid Y}\right]
\end{aligned}
\]

To get some intuition for these formulas, consider a mixture of \(K\) univariate Gaussians. Let \(Y\) be the hidden indicator variable that specifies which mixture component we are using, and let
\footnotetext{
3. This example is from https://en.wikipedia.org/wiki/Law_of_total_expectation, but with modified notation.
}
\title{
Probability: Univariate Models
}

\subsection*{2.1 Introduction}

In this chapter, we give a brief introduction to the basics of probability theory. There are many good books that go into more detail, e.g., [GS97; BT08].

\subsection*{2.1.1 What is probability?}

Probability theory is nothing but common sense reduced to calculation. - Pierre Laplace, 1812

We are all comfortable saying that the probability that a (fair) coin will land heads is $50 \%$. But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time. ${ }^{1}$

The other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials [Jay03; Lin06]. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss.

One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies. For example, we might want to compute the probability that the polar ice cap will melt by 2030 CE. This event will happen zero or one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify our uncertainty about this event; based on how probable we think this event is, we can decide how to take the optimal action, as discussed in Chapter 5. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.

\subsection*{2.1.2 Types of uncertainty}

The uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is
\footnotetext{
1. Actually, the Stanford statistician (and former professional magician) Persi Diaconis has shown that a coin is about $51 \%$ likely to land facing the same way up as it started, due to the physics of the problem [DHM07].
}

![](https://cdn.mathpix.com/cropped/2024_06_13_398d6182f58c2c67baf7g-1.jpg?height=329&width=498&top_left_y=239&top_left_x=751

ChatGPT figure/image summary: The image is a graph illustrating a probability density function (PDF) of a mixture of two one-dimensional (1D) Gaussians. The x-axis presumably represents the variable $x$, and the y-axis represents the probability density $p(x)$. The graph shows two distinct peaks, which correspond to the two components of the Gaussian mixture, each with its own mean ($\mu$) and variance ($\sigma^2$), as described in the context.

In the mixture, each Gaussian component is weighted—indicated by the text preceding the image—with a mixing coefficient, here both being 0.5 (suggesting an equal contribution from both components). The means of the Gaussians are at 0 and 2, and both have the same variance of 0.5. The bimodal nature of the PDF is represented by the two peaks on the graph, indicating the presence of two distinct Gaussian distributions within the mixture.)

Figure 2.4: Illustration of a mixture of two 1d Gaussians, $p(x)=0.5 \mathcal{N}(x \mid 0,0.5)+0.5 \mathcal{N}(x \mid 2,0.5)$. Generated by bimodal_dist_plot.ipynb.

To prove this, let us suppose, for simplicity, that $X$ and $Y$ are both discrete rv's. Then we have

$$
\begin{aligned}
\mathbb{E}_{Y}[\mathbb{E}[X \mid Y]] & =\mathbb{E}_{Y}\left[\sum_{x} x p(X=x \mid Y)\right] \\
& =\sum_{y}\left[\sum_{x} x p(X=x \mid Y=y)\right] p(Y=y)=\sum_{x, y} x p(X=x, Y=y)=\mathbb{E}[X]
\end{aligned}
$$

To give a more intuitive explanation, consider the following simple example. ${ }^{3}$ Let $X$ be the lifetime duration of a lightbulb, and let $Y$ be the factory the lightbulb was produced in. Suppose $\mathbb{E}[X \mid Y=1]=5000$ and $\mathbb{E}[X \mid Y=2]=4000$, indicating that factory 1 produces longer lasting bulbs. Suppose factory 1 supplies $60 \%$ of the lightbulbs, so $p(Y=1)=0.6$ and $p(Y=2)=0.4$. Then the expected duration of a random lightbulb is given by

$$
\mathbb{E}[X]=\mathbb{E}[X \mid Y=1] p(Y=1)+\mathbb{E}[X \mid Y=2] p(Y=2)=5000 \times 0.6+4000 \times 0.4=4600
$$

There is a similar formula for the variance. In particular, the law of total variance, also called the conditional variance formula, tells us that

$$
\mathbb{V}[X]=\mathbb{E}_{Y}[\mathbb{V}[X \mid Y]]+\mathbb{V}_{Y}[\mathbb{E}[X \mid Y]]
$$

To see this, let us define the conditional moments, $\mu_{X \mid Y}=\mathbb{E}[X \mid Y], s_{X \mid Y}=\mathbb{E}\left[X^{2} \mid Y\right]$, and $\sigma_{X \mid Y}^{2}=\mathbb{V}[X \mid Y]=s_{X \mid Y}-\mu_{X \mid Y}^{2}$, which are functions of $Y$ (and therefore are random quantities). Then we have

$$
\begin{aligned}
\mathbb{V}[X] & =\mathbb{E}\left[X^{2}\right]-(\mathbb{E}[X])^{2}=\mathbb{E}_{Y}\left[s_{X \mid Y}\right]-\left(\mathbb{E}_{Y}\left[\mu_{X \mid Y}\right]\right)^{2} \\
& =\mathbb{E}_{Y}\left[\sigma_{X \mid Y}^{2}\right]+\mathbb{E}_{Y}\left[\mu_{X \mid Y}^{2}\right]-\left(\mathbb{E}_{Y}\left[\mu_{X \mid Y}\right]\right)^{2} \\
& =E_{Y}[\mathbb{V}[X \mid Y]]+\mathbb{V}_{Y}\left[\mu_{X \mid Y}\right]
\end{aligned}
$$

To get some intuition for these formulas, consider a mixture of $K$ univariate Gaussians. Let $Y$ be the hidden indicator variable that specifies which mixture component we are using, and let
\footnotetext{
3. This example is from https://en.wikipedia.org/wiki/Law_of_total_expectation, but with modified notation.
}

![](https://cdn.mathpix.com/cropped/2024_06_13_73649b50fd444db3b6bbg-1.jpg?height=451&width=1513&top_left_y=188&top_left_x=264

ChatGPT figure/image summary: The image presents four separate scatter plots labeled as Dataset: I, II, III, and IV, correspondingly represented as (a), (b), (c), and (d). Each plot shows a set of points in a two-dimensional space with x and y axes:

- (a) Dataset I appears to show a roughly linear relationship between the x and y variables, with points closely aligned along a positively sloped line.
- (b) Dataset II displays a curved relationship, such as a quadratic or parabolic trend, with the points following a curved trajectory.
- (c) Dataset III seems to consist of points clustered in a vertical formation, indicative of a strong relationship at a specific x value, except for one outlier point which deviates substantially from the group.
- (d) Dataset IV shows points that are lined up vertically with respect to a specific x value, except for one point which is far from the main cluster and lies along a line that extends through the rest of the data points.

These plots are designed to demonstrate that despite having the same statistical metrics, such as mean, variance, and correlation, the actual distribution of data points can be quite different, visually illustrating the necessity of data visualization in complement to summary statistics in data analysis. This concept is commonly exemplified by Anscombe's quartet, and the information provided suggests that these plots are a representation of Anscombe’s quartet.)

Figure 2.5: Illustration of Anscombe's quartet. All of these datasets have the same low order summary statistics. Generated by anscombes_quartet.ipynb.

$X=\sum_{y=1}^{K} \pi_{y} \mathcal{N}\left(X \mid \mu_{y}, \sigma_{y}\right)$. In Figure 2.4, we have $\pi_{1}=\pi_{2}=0.5, \mu_{1}=0, \mu_{2}=2, \sigma_{1}=\sigma_{2}=0.5$. Thus

$$
\begin{aligned}
& \mathbb{E}[\mathbb{V}[X \mid Y]]=\pi_{1} \sigma_{1}^{2}+\pi_{2} \sigma_{2}^{2}=0.25 \\
& \mathbb{V}[\mathbb{E}[X \mid Y]]=\pi_{1}\left(\mu_{1}-\bar{\mu}\right)^{2}+\pi_{2}\left(\mu_{2}-\bar{\mu}\right)^{2}=0.5(0-1)^{2}+0.5(2-1)^{2}=0.5+0.5=1
\end{aligned}
$$

So we get the intuitive result that the variance of $X$ is dominated by which centroid it is drawn from (i.e., difference in the means), rather than the local variance around each centroid.

\title{
2.2.6 Limitations of summary statistics *
}

Although it is common to summarize a probability distribution (or points sampled from a distribution) using simple statistics such as the mean and variance, this can lose a lot of information. A striking example of this is known as Anscombe's quartet [Ans73], which is illustrated in Figure 2.5. This shows 4 different datasets of $(x, y)$ pairs, all of which have identical mean, variance and correlation coefficient $\rho$ (defined in Section 3.1.2): $\mathbb{E}[x]=9, \mathbb{V}[x]=11, \mathbb{E}[y]=7.50, \mathbb{V}[y]=4.12$, and $\rho=0.816 .{ }^{4}$ However, the joint distributions $p(x, y)$ from which these points were sampled are clearly very different. Anscombe invented these datasets, each consisting of 10 data points, to counter the impression among statisticians that numerical summaries are superior to data visualization [Ans73].

An even more striking example of this phenomenon is shown in Figure 2.6. This consists of a dataset that looks like a dinosaur ${ }^{5}$, plus 11 other datasets, all of which have identical low order statistics. This collection of datasets is called the Datasaurus Dozen [MF17]. The exact values of the $(x, y)$ points are available online. ${ }^{6}$ They were computed using simulated annealing, a derivative free optimization method which we discuss in the sequel to this book, [Mur23]. (The objective
\footnotetext{
4. The maximum likelihood estimate for the variance in Equation (4.36) differs from the unbiased estimate in Equation (4.38). For the former, we have $\mathbb{V}[x]=10.00, \mathbb{V}[y]=3.75$, for the latter, we have $\mathbb{V}[x]=11.00, \mathbb{V}[y]=4.12$. 5. This dataset was created by Alberto Cairo, and is available at http://www.thefunctionalart.com/2016/08/ download-datasaurus-never-trust-summary.html

6. https://www.autodesk.com/research/publications/same-stats-different-graphs. There are actually 13 datasets in total, including the dinosaur. We omitted the "away" dataset for visual clarity.
}

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_9c684c6cc35d19d78d75g-1.jpg?height=951&width=1518&top_left_y=188&top_left_x=261

ChatGPT figure/image summary: The image displays a collection of graphs, each representing a different dataset. Despite the visual differences among the datasets, they all share the same low order summary statistics. The datasets are creatively arranged to form various shapes, including a dinosaur, lines, a star, a circle, and slanted patterns, demonstrating how datasets with identical statistical properties can have drastically different distributions when visualized. This illustrates the Datasaurus Dozen concept, emphasizing the importance of data visualization in addition to numerical summary, as datasets with similar statistical summaries can have very different structures.)

Figure 2.6: Illustration of the Datasaurus Dozen. All of these datasets have the same low order summary statistics. Adapted from Figure 1 of [MF17]. Generated by datasaurus_dozen.ipynb.

function being optimized measures deviation from the target summary statistics of the original dinosaur, plus distance from a particular target shape.)

The same simulated annealing approach can be applied to 1d datasets, as shown in Figure 2.7. We see that all the datasets are quite different, but they all have the same median and inter-quartile range as shown by the central shaded part of the box plots in the middle. A better visualization is known as a violin plot, shown on the right. This shows (two copies of) the $1 \mathrm{~d}$ kernel density estimate (Section 16.3) of the distribution on the vertical axis, in addition to the median and IQR markers. This visualization is better able to distinguish differences in the distributions. However, the technique is limited to $1 \mathrm{~d}$ data.

\title{
2.3 Bayes' rule
}

Bayes's theorem is to the theory of probability what Pythagoras's theorem is to geometry.
- Sir Harold Jeffreys, 1973 [Jef73].

In this section, we discuss the basics of Bayesian inference. According to the Merriam-Webster dictionary, the term "inference" means "the act of passing from sample data to generalizations, usually with calculated degrees of certainty". The term "Bayesian" is used to refer to inference methods

![](https://cdn.mathpix.com/cropped/2024_06_13_bc2b90831f76956b6fc5g-1.jpg?height=523&width=1533&top_left_y=193&top_left_x=264

ChatGPT figure/image summary: The image contains three panels, each displaying a different representation of the same data for seven different datasets labeled A to G.

- The left panel, titled "Raw Data," shows scatter plots for datasets A through G. These plots visualize the data points' dispersion and any patterns within each dataset.
- The middle panel, titled "Box-plot of the Data," presents box plots for each dataset. Box plots are statistical graphics showing the median, interquartile range (IQR), and potential outliers of the distributions.
- The right panel, titled "Violin-plot of the Data," shows violin plots corresponding to each dataset. Violin plots combine a kernel density plot, which shows the density of the data at different values, with a box plot. They provide a more nuanced view of the data distribution than box plots. The shape of each violin plot suggests the frequency of data points at different levels within each dataset.

The purpose of showing these distinct visualizations is to convey how different graphical representations can provide various insights into the same data, as all datasets have the same summary statistics but display very unique structures in the scatter plots.)

Figure 2.7: Illustration of 7 different datasets (left), the corresponding box plots (middle) and violin box plots (right). From Figure 8 of https://www. autodesk. com/research/publications/ same-stats-different-graphs. Used with kind permission of Justin Matejka.

that represent "degrees of certainty" using probability theory, and which leverage Bayes' rule ${ }^{7}$, to update the degree of certainty given data.

Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity $H$ given some observed data $Y=y$ :

$$
p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)}
$$

This follows automatically from the identity

$$
p(h \mid y) p(y)=p(h) p(y \mid h)=p(h, y)
$$

which itself follows from the product rule of probability.

In Equation (2.51), the term $p(H)$ represents what we know about possible values of $H$ before we see any data; this is called the prior distribution. (If $H$ has $K$ possible values, then $p(H)$ is a vector of $K$ probabilities, that sum to 1.) The term $p(Y \mid H=h)$ represents the distribution over the possible outcomes $Y$ we expect to see if $H=h$; this is called the observation distribution. When we evaluate this at a point corresponding to the actual observations, $y$, we get the function $p(Y=y \mid H=h)$, which is called the likelihood. (Note that this is a function of $h$, since $y$ is fixed, but it is not a probability distribution, since it does not sum to one.) Multiplying the prior distribution $p(H=h)$ by the likelihood function $p(Y=y \mid H=h)$ for each $h$ gives the unnormalized joint distribution $p(H=h, Y=y)$. We can convert this into a normalized distribution by dividing by $p(Y=y)$, which is known as the marginal likelihood, since it is computed by marginalizing over the unknown $H$ :

$$
p(Y=y)=\sum_{h^{\prime} \in \mathcal{H}} p\left(H=h^{\prime}\right) p\left(Y=y \mid H=h^{\prime}\right)=\sum_{h^{\prime} \in \mathcal{H}} p\left(H=h^{\prime}, Y=y\right)
$$
7. Thomas Bayes (1702-1761) was an English mathematician and Presbyterian minister. For a discussion of whether
to spell this as Bayes rule, Bayes' rule or Bayes's rule, see https://bit. 1y/2kDtLuK.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

\begin{tabular}{cc|cc} 
& & \multicolumn{2}{|c}{ Observation } \\
& & 0 & 1 \\
\hline \multirow{2}{*}{ Truth } & 0 & $\mathrm{TNR}=$ Specificity $=0.975$ & $\mathrm{FPR}=1-\mathrm{TNR}=0.025$ \\
& 1 & $\mathrm{FNR}=1-\mathrm{TPR}=0.125$ & TPR $=$ Sensitivity $=0.875$
\end{tabular}

Table 2.1: Likelihood function $p(Y \mid H)$ for a binary observation $Y$ given two possible hidden states $H$. Each row sums to one. Abbreviations: TNR is true negative rate, TPR is true positive rate, FNR is false negative rate, FPR is false positive rate.

Normalizing the joint distribution by computing $p(H=h, Y=y) / p(Y=y)$ for each $h$ gives the posterior distribution $p(H=h \mid Y=y)$; this represents our new belief state about the possible values of $H$.

We can summarize Bayes rule in words as follows:

$$
\text { posterior } \propto \text { prior } \times \text { likelihood }
$$

Here we use the symbol $\propto$ to denote "proportional to", since we are ignoring the denominator, which is just a constant, independent of $H$. Using Bayes rule to update a distribution over unknown values of some quantity of interest, given relevant observed data, is called Bayesian inference, or posterior inference. It can also just be called probabilistic inference.

Below we give some simple examples of Bayesian inference in action. We will see many more interesting examples later in this book.

\title{
2.3.1 Example: Testing for COVID-19
}

Suppose you think you may have contracted COVID-19, which is an infectious disease caused by the SARS-CoV-2 virus. You decide to take a diagnostic test, and you want to use its result to determine if you are infected or not.

Let $H=1$ be the event that you are infected, and $H=0$ be the event you are not infected. Let $Y=1$ if the test is positive, and $Y=0$ if the test is negative. We want to compute $p(H=h \mid Y=y)$, for $h \in\{0,1\}$, where $y$ is the observed test outcome. (We will write the distribution of values, $[p(H=0 \mid Y=y), p(H=1 \mid Y=y)]$ as $p(H \mid y)$, for brevity.) We can think of this as a form of binary classification, where $H$ is the unknown class label, and $y$ is the feature vector.

First we must specify the likelihood. This quantity obviously depends on how reliable the test is. There are two key parameters. The sensitivity (aka true positive rate) is defined as $p(Y=1 \mid H=1)$, i.e., the probability of a positive test given that the truth is positive. The false negative rate is defined as one minus the sensitivity. The specificity (aka true negative rate) is defined as $p(Y=0 \mid H=0)$, i.e., the probability of a negative test given that the truth is negative. The false positive rate is defined as one minus the specificity. We summarize all these quantities in Table 2.1. (See Section 5.1.3.1 for more details.) Following https://nyti.ms/31mTZgV, we set the sensitivity to $87.5 \%$ and the specificity to $97.5 \%$.

Next we must specify the prior. The quantity $p(H=1)$ represents the prevalence of the disease in the area in which you live. We set this to $p(H=1)=0.1$ (i.e., $10 \%$ ), which was the prevalence in New York City in Spring 2020. (This example was chosen to match the numbers in https://nyti.ms/31MTZgV.)

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

Now suppose you test positive. We have

$$
\begin{aligned}
p(H=1 \mid Y=1) & =\frac{p(Y=1 \mid H=1) p(H=1)}{p(Y=1 \mid H=1) p(H=1)+p(Y=1 \mid H=0) p(H=0)} \\
& =\frac{\mathrm{TPR} \times \text { prior }}{\mathrm{TPR} \times \text { prior }+\mathrm{FPR} \times(1-\text { prior })} \\
& =\frac{0.875 \times 0.1}{0.875 \times 0.1+0.025 \times 0.9}=0.795
\end{aligned}
$$

So there is a $79.5 \%$ chance you are infected.

Now suppose you test negative. The probability you are infected is given by

$$
\begin{aligned}
p(H=1 \mid Y=0) & =\frac{p(Y=0 \mid H=1) p(H=1)}{p(Y=0 \mid H=1) p(H=1)+p(Y=0 \mid H=0) p(H=0)} \\
& =\frac{\text { FNR } \times \text { prior }}{\text { FNR } \times \text { prior }+ \text { TNR } \times(1-\text { prior })} \\
& =\frac{0.125 \times 0.1}{0.125 \times 0.1+0.975 \times 0.9}=0.014
\end{aligned}
$$

So there is just a $1.4 \%$ chance you are infected.

Nowadays COVID-19 prevalence is much lower. Suppose we repeat these calculations using a base rate of $1 \%$; now the posteriors reduce to $26 \%$ and $0.13 \%$ respectively.

The fact that you only have a $26 \%$ chance of being infected with COVID-19, even after a positive test, is very counter-intuitive. The reason is that a single positive test is more likely to be a false positive than due to the disease, since the disease is rare. To see this, suppose we have a population of 100,000 people, of whom 1000 are infected. Of those who are infected, $875=0.875 \times 1000$ test positive, and of those who are uninfected, $2475=0.025 \times 99,000$ test positive. Thus the total number of positives is $3350=875+2475$, so the posterior probability of being infected given a positive test is $875 / 3350=0.26$.

Of course, the above calculations assume we know the sensitivity and specificity of the test. See [GC20] for how to apply Bayes rule for diagnostic testing when there is uncertainty about these parameters.

\title{
2.3.2 Example: The Monty Hall problem
}

In this section, we consider a more "frivolous" application of Bayes rule. In particular, we apply it to the famous Monty Hall problem.

Imagine a game show with the following rules: There are three doors, labeled 1, 2, 3. A single prize (e.g., a car) has been hidden behind one of them. You get to select one door. Then the gameshow host opens one of the other two doors (not the one you picked), in such a way as to not reveal the prize location. At this point, you will be given a fresh choice of door: you can either stick with your first choice, or you can switch to the other closed door. All the doors will then be opened and you will receive whatever is behind your final choice of door.

For example, suppose you choose door 1 , and the gameshow host opens door 3 , revealing nothing behind the door, as promised. Should you (a) stick with door 1 , or (b) switch to door 2 , or (c) does it make no difference?

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

\begin{tabular}{lllll} 
Door 1 & Door 2 & Door 3 & Switch & Stay \\
\hline Car & - & - & Lose & Win \\
- & Car & - & Win & Lose \\
- & - & Car & Win & Lose
\end{tabular}

Table 2.2: 3 possible states for the Monty Hall game, showing that switching doors is two times better (on average) than staying with your original choice. Adapted from Table 6.1 of [PM18].

Intuitively, it seems it should make no difference, since your initial choice of door cannot influence the location of the prize. However, the fact that the host opened door 3 tells us something about the location of the prize, since he made his choice conditioned on the knowledge of the true location and on your choice. As we show below, you are in fact twice as likely to win the prize if you switch to door 2 .

To show this, we will use Bayes' rule. Let $H_{i}$ denote the hypothesis that the prize is behind door $i$. We make the following assumptions: the three hypotheses $H_{1}, H_{2}$ and $H_{3}$ are equiprobable a priori, i.e.,

$$
P\left(H_{1}\right)=P\left(H_{2}\right)=P\left(H_{3}\right)=\frac{1}{3}
$$

The datum we receive, after choosing door 1, is either $Y=3$ and $Y=2$ (meaning door 3 or 2 is opened, respectively). We assume that these two possible outcomes have the following probabilities. If the prize is behind door 1 , then the host selects at random between $Y=2$ and $Y=3$. Otherwise the choice of the host is forced and the probabilities are 0 and 1.

![](https://cdn.mathpix.com/cropped/2024_06_13_ed018759cfa69e78e314g-1.jpg?height=107&width=887&top_left_y=1176&top_left_x=301

ChatGPT figure/image summary: The image displays a set of conditional probabilities that describe the host's actions in the Monty Hall problem after the contestant has picked door 1. These probabilities define the likelihood of the host opening door 2 or door 3, given the location of the prize behind one of the three doors (H1, H2, or H3):

- If the prize is behind door 1 (H1), the host has an equal chance of opening door 2 or door 3, since he can't reveal the prize or the door picked by the contestant. Therefore, both outcomes have a probability of 1/2.
- If the prize is behind door 2 (H2), the host cannot open door 2 (since the prize is there), and he cannot open door 1 (since that is the door picked by the contestant). Hence, he has no choice but to open door 3, giving P(Y = 3|H2) = 1 and P(Y = 2|H2) = 0.
- If the prize is behind door 3 (H3), similarly, the host is forced to open door 2, leading to P(Y = 2|H3) = 1 and P(Y = 3|H3) = 0.

These probabilities are used in the application of Bayes' theorem to determine the contestant's best strategy in the Monty Hall problem, which—as the earlier text explains—would be to switch doors after the host opens one.)

Now, using Bayes' theorem, we evaluate the posterior probabilities of the hypotheses:

$$
\begin{aligned}
& P\left(H_{i} \mid Y=3\right)=\frac{P\left(Y=3 \mid H_{i}\right) P\left(H_{i}\right)}{P(Y=3)} \\
& \left|P\left(H_{1} \mid Y=3\right)=\frac{(1 / 2)(1 / 3)}{P(Y=3)}\right| P\left(H_{2} \mid Y=3\right)=\frac{(1)(1 / 3)}{P(Y=3)}\left|P\left(H_{3} \mid Y=3\right)=\frac{(0)(1 / 3)}{P(Y=3)}\right|
\end{aligned}
$$

The denominator $P(Y=3)$ is $P(Y=3)=\frac{1}{6}+\frac{1}{3}=\frac{1}{2}$. So

$$
\left.\left|P\left(H_{1} \mid Y=3\right)=\frac{1}{3}\right| P\left(H_{2} \mid Y=3\right)=\frac{2}{3} \right\rvert\, P\left(H_{3} \mid Y=3\right)=0
$$

So the contestant should switch to door 2 in order to have the biggest chance of getting the prize. See Table 2.2 for a worked example.

Many people find this outcome surprising. One way to make it more intuitive is to perform a thought experiment in which the game is played with a million doors. The rules are now that the contestant chooses one door, then the game show host opens 999,998 doors in such a way as not to reveal the prize, leaving the contestant's selected door and one other door closed. The contestant may now stick or switch. Imagine the contestant confronted by a million doors, of which doors 1 and 234,598 have not been opened, door 1 having been the contestant's initial guess. Where do you think the prize is?

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_ab164e7d058b84145366g-1.jpg?height=387&width=250&top_left_y=202&top_left_x=872

ChatGPT figure/image summary: The image appears to be an artistic or schematic representation of a three-dimensional object being projected onto a two-dimensional surface. There is a series of line-drawn figures that form shapes, which could be polygons like cubes or other polyhedrons, stacked on top of each other along a vertical axis labeled "z." The lower dimension in the foreground is labeled "y," and the one going into the page is labeled "x," indicating the three spatial axes commonly used in Cartesian coordinate systems.

This illustration is likely meant to demonstrate how a three-dimensional structure can be represented in two dimensions, which ties into the discussion from the text about the challenge of inferring 3D shapes from 2D images. This fundamental problem in perception and computer vision is an example of an inverse problem, where the goal is to determine the true nature of the original object (a 3D shape in this case) from its 2D projection. The visual appearance of the object can be ambiguous because multiple 3D shapes can produce the same 2D projection when viewed from a particular angle. The text explains how these sorts of problems are approached using probability theory and specifically Bayes' rule to infer the most likely state of the world from observed data.)

Figure 2.8: Any planar line-drawing is geometrically consistent with infinitely many 3-D structures. From Figure 11 of [SA93]. Used with kind permission of Pawan Sinha.

\title{
2.3.3 Inverse problems *
}

Probability theory is concerned with predicting a distribution over outcomes $y$ given knowledge (or assumptions) about the state of the world, $h$. By contrast, inverse probability is concerned with inferring the state of the world from observations of outcomes. We can think of this as inverting the $h \rightarrow y$ mapping.

For example, consider trying to infer a $3 \mathrm{~d}$ shape $h$ from a $2 \mathrm{~d}$ image $y$, which is a classic problem in visual scene understanding. Unfortunately, this is a fundamentally ill-posed problem, as illustrated in Figure 2.8, since there are multiple possible hidden $h$ 's consistent with the same observed $y$ (see e.g., [Piz01]). Similarly, we can view natural language understanding as an ill-posed problem, in which the listener must infer the intention $h$ from the (often ambiguous) words spoken by the speaker (see e.g., [Sab21]).

To tackle such inverse problems, we can use Bayes' rule to compute the posterior, $p(h \mid y)$, which gives a distribution over possible states of the world. This requires specifying the forwards model, $p(y \mid h)$, as well as a prior $p(h)$, which can be used to rule out (or downweight) implausible world states. We discuss this topic in more detail in the sequel to this book, [Mur23].

\subsection*{2.4 Bernoulli and binomial distributions}

Perhaps the simplest probability distribution is the Bernoulli distribution, which can be used to model binary events, as we discuss below.

\subsection*{2.4.1 Definition}

Consider tossing a coin, where the probability of event that it lands heads is given by $0 \leq \theta \leq 1$. Let $Y=1$ denote this event, and let $Y=0$ denote the event that the coin lands tails. Thus we are assuming that $p(Y=1)=\theta$ and $p(Y=0)=1-\theta$. This is called the Bernoulli distribution, and can be written as follows

$$
Y \sim \operatorname{Ber}(\theta)
$$

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_2a2dfc8685cf6835049eg-1.jpg?height=521&width=1271&top_left_y=191&top_left_x=380

ChatGPT figure/image summary: The image displays two bar graphs illustrating examples of the binomial distribution with \( N = 10 \) trials for different values of probability \( \theta \). On the left, graph (a) shows the binomial distribution with \( \theta = 0.25 \), where the probability of success on any individual trial is 0.25. On the right, graph (b) shows the binomial distribution with \( \theta = 0.9 \), where the probability of success is 0.9. The x-axis represents the number of successes ranging from 0 to 10, and the y-axis represents the probability of observing that number of successes in 10 trials. The different shapes of the distributions reflect the different probabilities of success, with (a) having a more spread out distribution peaking around 2 or 3 successes and (b) having a distribution that is heavily weighted towards a higher number of successes, peaking at 9 or 10.)

Figure 2.9: Illustration of the binomial distribution with $N=10$ and (a) $\theta=0.25$ and (b) $\theta=0.9$. Generated by binom_dist_plot.ipynb.

where the symbol means "is sampled from" or "is distributed as", and Ber refers to Bernoulli. The probability mass function (pmf) of this distribution is defined as follows:

$$
\operatorname{Ber}(y \mid \theta)= \begin{cases}1-\theta & \text { if } y=0 \\ \theta & \text { if } y=1\end{cases}
$$

(See Section 2.2.1 for details on pmf's.) We can write this in a more concise manner as follows:

$$
\operatorname{Ber}(y \mid \theta) \triangleq \theta^{y}(1-\theta)^{1-y}
$$

The Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of $N$ Bernoulli trials, denoted $y_{n} \sim \operatorname{Ber}(\cdot \mid \theta)$, for $n=1: N$. Concretely, think of tossing a coin $N$ times. Let us define $s$ to be the total number of heads, $s \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=1\right)$. The distribution of $s$ is given by the binomial distribution:

$$
\operatorname{Bin}(s \mid N, \theta) \triangleq\binom{N}{s} \theta^{s}(1-\theta)^{N-s}
$$

where

$$
\binom{N}{k} \triangleq \frac{N!}{(N-k)!k!}
$$

is the number of ways to choose $k$ items from $N$ (this is known as the binomial coefficient, and is pronounced "N choose k"). See Figure 2.9 for some examples of the binomial distribution. If $N=1$, the binomial distribution reduces to the Bernoulli distribution.

\title{
2.4.2 Sigmoid (logistic) function
}

When we want to predict a binary variable $y \in\{0,1\}$ given some inputs $\boldsymbol{x} \in \mathcal{X}$, we need to use a conditional probability distribution of the form

$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\operatorname{Ber}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_5e8463dfe213d57710b3g-1.jpg?height=510&width=1248&top_left_y=222&top_left_x=381

ChatGPT figure/image summary: The image shows two graphs side by side. On the left, graph (a) represents the sigmoid or logistic function, which is a smooth and continuous S-shaped curve that maps any real-valued number into a value between 0 and 1. It's labeled "sigmoid function." The horizontal axis likely represents the input to the function, ranging from -4 to 4, while the vertical axis represents the output of the sigmoid function, ranging from 0 to 1. This function is commonly used in machine learning for binary classification problems.

On the right, graph (b) represents the Heaviside function, a step function that jumps from 0 to 1 at a certain point on the horizontal axis, which appears to be at zero. It's labeled "Heaviside function." This function is not smooth or continuous and is typically used to model situations where there is a switch from one state to another at a certain threshold.)

Figure 2.10: (a) The sigmoid (logistic) function $\sigma(a)=\left(1+e^{-a}\right)^{-1}$. (b) The Heaviside function $\mathbb{I}(a>0)$. Generated by activation_fun_plot.ipynb.

$$
\begin{aligned}
\sigma(x) & \triangleq \frac{1}{1+e^{-x}}=\frac{e^{x}}{1+e^{x}} \\
\frac{d}{d x} \sigma(x) & =\sigma(x)(1-\sigma(x)) \\
1-\sigma(x) & =\sigma(-x) \\
\sigma^{-1}(p) & =\log \left(\frac{p}{1-p}\right) \triangleq \operatorname{logit}(p) \\
\sigma_{+}(x) & \triangleq \log \left(1+e^{x}\right) \triangleq \operatorname{softplus}(x) \\
\frac{d}{d x} \sigma_{+}(x) & =\sigma(x)
\end{aligned}
$$

Table 2.3: Some useful properties of the sigmoid (logistic) and related functions. Note that the logit function is the inverse of the sigmoid function, and has a domain of $[0,1]$.

where $f(\boldsymbol{x} ; \boldsymbol{\theta})$ is some function that predicts the mean parameter of the output distribution. We will consider many different kinds of function $f$ in Part II-Part IV.

To avoid the requirement that $0 \leq f(\boldsymbol{x} ; \boldsymbol{\theta}) \leq 1$, we can let $f$ be an unconstrained function, and use the following model:

$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\operatorname{Ber}(y \mid \sigma(f(\boldsymbol{x} ; \boldsymbol{\theta})))
$$

Here $\sigma()$ is the sigmoid or logistic function, defined as follows:

$$
\sigma(a) \triangleq \frac{1}{1+e^{-a}}
$$

where $a=f(\boldsymbol{x} ; \boldsymbol{\theta})$. The term "sigmoid" means S-shaped: see Figure 2.10a for a plot. We see that it Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

called epistemic uncertainty, since epistemology is the philosophical term used to describe the study of knowledge. However, a simpler term for this is model uncertainty. The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], derived from the Latin word for "dice", although a simpler term would be data uncertainty. As a concrete example, consider tossing a fair coin. We might know for sure that the probability of heads is $p=0.5$, so there is no epistemic uncertainty, but we still cannot perfectly predict the outcome.

This distinction can be important for applications such as active learning. A typical strategy is to query examples for which $\mathbb{H}(p(y \mid \boldsymbol{x}, \mathcal{D})$ ) is large (where $\mathbb{H}(p)$ is the entropy, discussed in Section 6.1). However, this could be due to uncertainty about the parameters, i.e., large $\mathbb{H}(p(\boldsymbol{\theta} \mid \mathcal{D}))$, or just due to inherent variability of the outcome, corresponding to large entropy of $p(y \mid \boldsymbol{x}, \boldsymbol{\theta})$. In the latter case, there would not be much use collecting more samples, since our epistemic uncertainty would not be reduced. See [Osb16] for further discussion of this point.

\title{
2.1.3 Probability as an extension of logic
}

In this section, we review the basic rules of probability, following the presentation of [Jay03], in which we view probability as an extension of Boolean logic.

\subsection*{2.1.3.1 Probability of an event}

We define an event, denoted by the binary variable $A$, as some state of the world that either holds or does not hold. For example, $A$ might be event "it will rain tomorrow", or "it rained yesterday", or "the label is $y=1$ ", or "the parameter $\theta$ is between 1.5 and 2.0", etc. The expression $\operatorname{Pr}(A)$ denotes the probability with which you believe event $A$ is true (or the long run fraction of times that $A$ will occur). We require that $0 \leq \operatorname{Pr}(A) \leq 1$, where $\operatorname{Pr}(A)=0$ means the event definitely will not happen, and $\operatorname{Pr}(A)=1$ means the event definitely will happen. We write $\operatorname{Pr}(\bar{A})$ to denote the probability of event $A$ not happening; this is defined to be $\operatorname{Pr}(\bar{A})=1-\operatorname{Pr}(A)$.

\subsection*{2.1.3.2 Probability of a conjunction of two events}

We denote the joint probability of events $A$ and $B$ both happening as follows:

$$
\operatorname{Pr}(A \wedge B)=\operatorname{Pr}(A, B)
$$

If $A$ and $B$ are independent events, we have

$$
\operatorname{Pr}(A, B)=\operatorname{Pr}(A) \operatorname{Pr}(B)
$$

For example, suppose $X$ and $Y$ are chosen uniformly at random from the set $\mathcal{X}=\{1,2,3,4\}$. Let $A$ be the event that $X \in\{1,2\}$, and $B$ be the event that $Y \in\{3\}$. Then we have $\operatorname{Pr}(A, B)=$ $\operatorname{Pr}(A) \operatorname{Pr}(B)=\frac{1}{2} \cdot \frac{1}{4}$.

\subsection*{2.1.3.3 Probability of a union of two events}

The probability of event $A$ or $B$ happening is given by

$$
\operatorname{Pr}(A \vee B)=\operatorname{Pr}(A)+\operatorname{Pr}(B)-\operatorname{Pr}(A \wedge B)
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_ecbdc63aa9b6dbee5100g-1.jpg?height=344&width=982&top_left_y=218&top_left_x=524

ChatGPT figure/image summary: The image displays a graph from logistic regression applied to a 1-dimensional, 2-class version of the Iris dataset. The horizontal axis represents petal width in centimeters, while the vertical axis represents the probability. There are two curves: one solid green line indicating the probability of an iris being of the Virginica species, and one dashed blue line representing the probability of an iris not being of the Virginica species, based on the petal width.

A decision boundary is indicated by a vertical dashed line at approximately 1.7 cm on the petal width axis, where the probability of the two classes (Iris-Virginica and Not Iris-Virginica) intersects at 0.5. Triangular markers on the top and bottom edges likely represent actual data points from the dataset, showing their classification according to petal width. The graph illustrates how logistic regression can classify flowers based on the single feature of petal width and shows that as the petal width increases or decreases from the decision boundary, the model's confidence in classifying the flower as either Virginica or not Virginica increases.)

Figure 2.11: Logistic regression applied to a 1-dimensional, 2-class version of the Iris dataset. Generated by iris_logreg.ipynb. Adapted from Figure 4.23 of [Gér19].

maps the whole real line to $[0,1]$, which is necessary for the output to be interpreted as a probability (and hence a valid value for the Bernoulli parameter $\theta$ ). The sigmoid function can be thought of as a "soft" version of the heaviside step function, defined by

$$
H(a) \triangleq \mathbb{I}(a>0)
$$

as shown in Figure 2.10b.

Plugging the definition of the sigmoid function into Equation (2.78) we get

$$
\begin{aligned}
& p(y=1 \mid x, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a) \\
& p(y=0 \mid x, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a)
\end{aligned}
$$

The quantity $a$ is equal to the $\log$ odds, $\log \left(\frac{p}{1-p}\right)$, where $p=p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})$. To see this, note that

$$
\log \left(\frac{p}{1-p}\right)=\log \left(\frac{e^{a}}{1+e^{a}} \frac{1+e^{a}}{1}\right)=\log \left(e^{a}\right)=a
$$

The logistic function or sigmoid function maps the log-odds $a$ to $p$ :

$$
p=\operatorname{logistic}(a)=\sigma(a) \triangleq \frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}
$$

The inverse of this is called the logit function, and maps $p$ to the log-odds $a$ :

$$
a=\operatorname{logit}(p)=\sigma^{-1}(p) \triangleq \log \left(\frac{p}{1-p}\right)
$$

See Table 2.3 for some useful properties of these functions.

\title{
2.4.3 Binary logistic regression
}

In this section, we use a conditional Bernoulli model, where we use a linear predictor of the form $f(\boldsymbol{x} ; \boldsymbol{\theta})=\boldsymbol{w}^{\top} \boldsymbol{x}+b$. Thus the model has the form

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\operatorname{Ber}\left(y \mid \sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)\right)
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

In other words,

$$
p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)}}
$$

This is called logistic regression.

For example consider a 1-dimensional, 2-class version of the iris dataset, where the positive class is "Virginica" and the negative class is "not Virginica", and the feature $x$ we use is the petal width. We fit a logistic regression model to this and show the results in Figure 2.11. The decision boundary corresponds to the value $x^{*}$ where $p\left(y=1 \mid x=x^{*}, \boldsymbol{\theta}\right)=0.5$. We see that, in this example, $x^{*} \approx 1.7$. As $x$ moves away from this boundary, the classifier becomes more confident in its prediction about the class label.

It should be clear from this example why it would be inappropriate to use linear regression for a (binary) classification problem. In such a model, the probabilities would increase above 1 as we move far enough to the right, and below 0 as we move far enough to the left.

For more detail on logistic regression, see Chapter 10.

\title{
2.5 Categorical and multinomial distributions
}

To represent a distribution over a finite set of labels, $y \in\{1, \ldots, C\}$, we can use the categorical distribution, which generalizes the Bernoulli to $C>2$ values.

\subsection*{2.5.1 Definition}

The categorical distribution is a discrete probability distribution with one parameter per class:

$$
\operatorname{Cat}(y \mid \boldsymbol{\theta}) \triangleq \prod_{c=1}^{C} \theta_{c}^{\mathbb{I}(y=c)}
$$

In other words, $p(y=c \mid \boldsymbol{\theta})=\theta_{c}$. Note that the parameters are constrained so that $0 \leq \theta_{c} \leq 1$ and $\sum_{c=1}^{C} \theta_{c}=1$; thus there are only $C-1$ independent parameters.

We can write the categorical distribution in another way by converting the discrete variable $y$ into a one-hot vector with $C$ elements, all of which are 0 except for the entry corresponding to the class label. (The term "one-hot" arises from electrical engineering, where binary vectors are encoded as electrical current on a set of wires, which can be active ("hot") or not ("cold").) For example, if $C=3$, we encode the classes 1,2 and 3 as $(1,0,0),(0,1,0)$, and $(0,0,1)$. More generally, we can encode the classes using unit vectors, where $\boldsymbol{e}_{c}$ is all 0 s except for dimension $c$. (This is also called a dummy encoding.) Using one-hot encodings, we can write the categorical distribution as follows:

$$
\operatorname{Cat}(\boldsymbol{y} \mid \boldsymbol{\theta}) \triangleq \prod_{c=1}^{C} \theta_{c}^{y_{c}}
$$

The categorical distribution is a special case of the multinomial distribution. To explain this, suppose we observe $N$ categorical trials, $y_{n} \sim \operatorname{Cat}(\cdot \mid \boldsymbol{\theta})$, for $n=1: N$. Concretely, think of rolling a $C$-sided dice $N$ times. Let us define $\boldsymbol{y}$ to be a vector that counts the number of times each face

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_0d3c3fabafab878573b2g-1.jpg?height=362&width=941&top_left_y=204&top_left_x=545

ChatGPT figure/image summary: The image shows a set of three bar graphs, representing the softmax distribution over three classes for a given input vector of scores (logits) \(\boldsymbol{a}=(3,0,1)\), at different temperature values (\(T=100\), \(T=2\), and \(T=1\)). These graphs aim to illustrate the effect of temperature on the softmax function.

- On the left, with \(T=100\), the distribution is almost uniform, meaning that all three classes have similar probabilities. This is due to the high temperature, which makes the softmax output less sensitive to differences in the input scores (logits).
  
- In the middle, with \(T=2\), the difference in the scores has a more pronounced effect, resulting in different probabilities for the three classes. The class corresponding to the highest input score (logit of 3) has a higher probability, but the distribution is still relatively smooth due to the temperature not being too low.

- On the right, with \(T=1\), we see the "winner-takes-all" behavior at a low temperature, where the class with the highest input score is assigned a much higher probability compared to the others, with most of the probability mass focused on this class.

Each bar graph effectively demonstrates how the softmax function can act as a "soft" version of the argmax function, especially at lower temperatures.)

Figure 2.12: Softmax distribution $\operatorname{softmax}(\boldsymbol{a} / T)$, where $\boldsymbol{a}=(3,0,1)$, at temperatures of $T=100, T=2$ and $T=1$. When the temperature is high (left), the distribution is uniform, whereas when the temperature is low (right), the distribution is "spiky", with most of its mass on the largest element. Generated by softmax_plot.ipynb.

shows up, i.e., $y_{c}=N_{c} \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=c\right)$. Now $\boldsymbol{y}$ is no longer one-hot, but is "multi-hot", since it has a non-zero entry for every value of $c$ that was observed across all $N$ trials. The distribution of $\boldsymbol{y}$ is given by the multinomial distribution:

$$
\mathcal{M}(\boldsymbol{y} \mid N, \boldsymbol{\theta}) \triangleq\binom{N}{y_{1} \ldots y_{C}} \prod_{c=1}^{C} \theta_{c}^{y_{c}}=\binom{N}{N_{1} \ldots N_{C}} \prod_{c=1}^{C} \theta_{c}^{N_{c}}
$$

where $\theta_{c}$ is the probability that side $c$ shows up, and

$$
\binom{N}{N_{1} \ldots N_{C}} \triangleq \frac{N!}{N_{1}!N_{2}!\cdots N_{C}!}
$$

is the multinomial coefficient, which is the number of ways to divide a set of size $N=\sum_{c=1}^{C} N_{c}$ into subsets with sizes $N_{1}$ up to $N_{C}$. If $N=1$, the multinomial distribution becomes the categorical distribution.

\title{
2.5.2 Softmax function
}

In the conditional case, we can define

$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\operatorname{Cat}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))
$$

which we can also write as

$$
p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\mathcal{M}(\boldsymbol{y} \mid 1, f(\boldsymbol{x} ; \boldsymbol{\theta}))
$$

We require that $0 \leq f_{c}(\boldsymbol{x} ; \boldsymbol{\theta}) \leq 1$ and $\sum_{c=1}^{C} f_{c}(\boldsymbol{x} ; \boldsymbol{\theta})=1$.

To avoid the requirement that $f$ directly predict a probability vector, it is common to pass the output from $f$ into the softmax function [Bri90], also called the multinomial logit. This is defined as follows:

$$
\operatorname{softmax}(\boldsymbol{a}) \triangleq\left[\frac{e^{a_{1}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}, \ldots, \frac{e^{a_{C}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}\right]
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_855eab66de586ca6078dg-1.jpg?height=367&width=554&top_left_y=207&top_left_x=733

ChatGPT figure/image summary: The image depicts a scatter plot with decision boundaries resulting from multinomial logistic regression on the classic Iris dataset. 

On the horizontal axis, we have the petal length, while the vertical axis shows the petal width. Both of these are features used to classify the Iris flowers into three distinct species, which are represented by different shapes and colors:

- Iris Virginica is indicated by green triangles.
- Iris Versicolor is shown with yellow circles.
- Iris Setosa is represented by blue squares.

The background color gradient and contour lines indicate the probabilities associated with each class across the feature space. The boundaries where the colors change are where the model's predicted probabilities for the different classes are equal. These lines show where the logistic regression model changes its prediction from one Iris class to another, thus dividing the graph into regions where each species is the most likely according to the model's parameters and the given features.

The decision boundaries are linear, and you can observe that Iris Setosa is very well separated from the other two species, while there is a small overlap between Iris Virginica and Iris Versicolor, reflecting some ambiguity in the classification between these two species based on petal length and width alone.)

Figure 2.13: Logistic regression on the 3-class, 2-feature version of the Iris dataset. Adapted from Figure of 4.25 [Gér19]. Generated by iris_logreg.ipynb.

This maps $\mathbb{R}^{C}$ to $[0,1]^{C}$, and satisfies the constraints that $0 \leq \operatorname{softmax}(\boldsymbol{a})_{c} \leq 1$ and $\sum_{c=1}^{C} \operatorname{softmax}(\boldsymbol{a})_{c}=$ 1. The inputs to the softmax, $\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})$, are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function. To see this, let us divide each $a_{c}$ by a constant $T$ called the temperature. ${ }^{8}$ Then as $T \rightarrow 0$, we find

$$
\operatorname{softmax}(\boldsymbol{a} / T)_{c}= \begin{cases}1.0 & \text { if } c=\operatorname{argmax}_{c^{\prime}} a_{c^{\prime}} \\ 0.0 & \text { otherwise }\end{cases}
$$

In other words, at low temperatures, the distribution puts most of its probability mass in the most probable state (this is called winner takes all), whereas at high temperatures, it spreads the mass uniformly. See Figure 2.12 for an illustration.

\title{
2.5.3 Multiclass logistic regression
}

If we use a linear predictor of the form $f(\boldsymbol{x} ; \boldsymbol{\theta})=\mathbf{W} \boldsymbol{x}+\boldsymbol{b}$, where $\mathbf{W}$ is a $C \times D$ matrix, and $\boldsymbol{b}$ is a $C$-dimensional bias vector, the final model becomes

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\operatorname{Cat}(y \mid \operatorname{softmax}(\mathbf{W} \boldsymbol{x}+\boldsymbol{b}))
$$

Let $\boldsymbol{a}=\mathbf{W} \boldsymbol{x}+\boldsymbol{b}$ be the $C$-dimensional vector of logits. Then we can rewrite the above as follows:

$$
p(y=c \mid \boldsymbol{x} ; \boldsymbol{\theta})=\frac{e^{a_{c}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}
$$

This is known as multinomial logistic regression.

If we have just two classes, this reduces to binary logistic regression. To see this, note that

$$
\operatorname{softmax}(\boldsymbol{a})_{0}=\frac{e^{a_{0}}}{e^{a_{0}}+e^{a_{1}}}=\frac{1}{1+e^{a_{1}-a_{0}}}=\sigma\left(a_{0}-a_{1}\right)
$$

so we can just train the model to predict $a=a_{1}-a_{0}$. This can be done with a single weight vector $\boldsymbol{w}$; if we use the multi-class formulation, we will have two weight vectors, $\boldsymbol{w}_{0}$ and $\boldsymbol{w}_{1}$. Such a model is over-parameterized, which can hurt interpretability, but the predictions will be the same.
\footnotetext{
8. This terminology comes from the area of statistical physics. The Boltzmann distribution is a distribution over states which has the same form as the softmax function.
}

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

We discuss this in more detail in Section 10.3. For now, we just give an example. Figure 2.13 shows what happens when we fit this model to the 3 -class iris dataset, using just 2 features. We see that the decision boundaries between each class are linear. We can create nonlinear boundaries by transforming the features (e.g., using polynomials), as we discuss in Section 10.3.1.

\title{
2.5.4 Log-sum-exp trick
}

In this section, we discuss one important practical detail to pay attention to when working with the softmax distribution. Suppose we want to compute the normalized probability $p_{c}=p(y=c \mid \boldsymbol{x})$, which is given by

$$
p_{c}=\frac{e^{a_{c}}}{Z(\boldsymbol{a})}=\frac{e^{a_{c}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}
$$

where $\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})$ are the logits. We might encounter numerical problems when computing the partition function $Z$. For example, suppose we have 3 classes, with logits $\boldsymbol{a}=(0,1,0)$. Then we find $Z=e^{0}+e^{1}+e^{0}=4.71$. But now suppose $\boldsymbol{a}=(1000,1001,1000)$; we find $Z=\infty$, since on a computer, even using 64 bit precision, np. $\exp (1000)=$ inf. Similarly, suppose $\boldsymbol{a}=(-1000,-999,-1000)$; now we find $Z=0$, since np. $\exp (-1000)=0$. To avoid numerical problems, we can use the following identity:

$$
\log \sum_{c=1}^{C} \exp \left(a_{c}\right)=m+\log \sum_{c=1}^{C} \exp \left(a_{c}-m\right)
$$

This holds for any $m$. It is common to use $m=\max _{c} a_{c}$ which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function:

$$
\operatorname{lse}(\boldsymbol{a}) \triangleq \log \sum_{c=1}^{C} \exp \left(a_{c}\right)
$$

We can use this to compute the probabilities from the logits:

$$
p(y=c \mid \boldsymbol{x})=\exp \left(a_{c}-\operatorname{lse}(\boldsymbol{a})\right)
$$

We can then pass this to the cross-entropy loss, defined in Equation (5.41).

However, to save computational effort, and for numerical stability, it is quite common to modify the cross-entropy loss so that it takes the logits $\boldsymbol{a}$ as inputs, instead of the probability vector $\boldsymbol{p}$. For example, consider the binary case. The CE loss for one example is

$$
\mathcal{L}=-\left[\mathbb{I}(y=0) \log p_{0}+\mathbb{I}(y=1) \log p_{1}\right]
$$

where

$$
\begin{aligned}
& \log p_{1}=\log \left(\frac{1}{1+\exp (-a)}\right)=\log (1)-\log (1+\exp (-a))=0-\operatorname{lse}([0,-a]) \\
& \log p_{0}=0-\operatorname{lse}([0,+a])
\end{aligned}
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

\title{
2.6 Univariate Gaussian (normal) distribution
}

The most widely used distribution of real-valued random variables $y \in \mathbb{R}$ is the Gaussian distribution, also called the normal distribution (see Section 2.6.4 for a discussion of these names).

\subsection*{2.6.1 Cumulative distribution function}

We define the cumulative distribution function or cdf of a continuous random variable $Y$ as follows:

$$
P(y) \triangleq \operatorname{Pr}(Y \leq y)
$$

(Note that we use a capital $P$ to represent the cdf.) Using this, we can compute the probability of being in any interval as follows:

$$
\operatorname{Pr}(a<Y \leq b)=P(b)-P(a)
$$

Cdf's are monotonically non-decreasing functions.

The cdf of the Gaussian is defined by

$$
\Phi\left(y ; \mu, \sigma^{2}\right) \triangleq \int_{-\infty}^{y} \mathcal{N}\left(z \mid \mu, \sigma^{2}\right) d z
$$

See Figure 2.2a for a plot. Note that the cdf of the Gaussian is often implemented using $\Phi\left(y ; \mu, \sigma^{2}\right)=$ $\frac{1}{2}[1+\operatorname{erf}(z / \sqrt{2})]$, where $z=(y-\mu) / \sigma$ and $\operatorname{erf}(u)$ is the error function, defined as

$$
\operatorname{erf}(u) \triangleq \frac{2}{\sqrt{\pi}} \int_{0}^{u} e^{-t^{2}} d t
$$

The parameter $\mu$ encodes the mean of the distribution, which is the same as the mode, since the distribution is unimodal. The parameter $\sigma^{2}$ encodes the variance. (Sometimes we talk about the precision of a Gaussian, which is the inverse variance, denoted $\lambda=1 / \sigma^{2}$.) When $\mu=0$ and $\sigma=1$, the Gaussian is called the standard normal distribution.

If $P$ is the cdf of $Y$, then $P^{-1}(q)$ is the value $y_{q}$ such that $p\left(Y \leq y_{q}\right)=q$; this is called the $q^{\prime}$ 'th quantile of $P$. The value $P^{-1}(0.5)$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P^{-1}(0.25)$ and $P^{-1}(0.75)$ are the lower and upper quartiles.

For example, let $\Phi$ be the cdf of the Gaussian distribution $\mathcal{N}(0,1)$, and $\Phi^{-1}$ be the inverse cdf (also known as the probit function). Then points to the left of $\Phi^{-1}(\alpha / 2)$ contain $\alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $\Phi^{-1}(1-\alpha / 2)$ also contain $\alpha / 2$ of the mass. Hence the central interval $\left(\Phi^{-1}(\alpha / 2), \Phi^{-1}(1-\alpha / 2)\right)$ contains $1-\alpha$ of the mass. If we set $\alpha=0.05$, the central $95 \%$ interval is covered by the range

$$
\left(\Phi^{-1}(0.025), \Phi^{-1}(0.975)\right)=(-1.96,1.96)
$$

If the distribution is $\mathcal{N}\left(\mu, \sigma^{2}\right)$, then the $95 \%$ interval becomes $(\mu-1.96 \sigma, \mu+1.96 \sigma)$. This is often approximated by writing $\mu \pm 2 \sigma$.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

\title{
2.6.2 Probability density function
}

We define the probability density function or pdf as the derivative of the cdf:

$$
p(y) \triangleq \frac{d}{d y} P(y)
$$

The pdf of the Gaussian is given by

$$
\mathcal{N}\left(y \mid \mu, \sigma^{2}\right) \triangleq \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}(y-\mu)^{2}}
$$

where $\sqrt{2 \pi \sigma^{2}}$ is the normalization constant needed to ensure the density integrates to 1 (see Exercise 2.12). See Figure 2.2b for a plot.

Given a pdf, we can compute the probability of a continuous variable being in a finite interval as follows:

$$
\operatorname{Pr}(a<Y \leq b)=\int_{a}^{b} p(y) d y=P(b)-P(a)
$$

As the size of the interval gets smaller, we can write

$$
\operatorname{Pr}(y \leq Y \leq y+d y) \approx p(y) d y
$$

Intuitively, this says the probability of $Y$ being in a small interval around $y$ is the density at $y$ times the width of the interval. One important consequence of the above result is that the pdf at a point can be larger than 1 . For example, $\mathcal{N}(0 \mid 0,0.1)=3.99$.

We can use the pdf to compute the mean, or expected value, of the distribution:

$$
\mathbb{E}[Y] \triangleq \int_{\mathcal{Y}} y p(y) d y
$$

For a Gaussian, we have the familiar result that $\mathbb{E}\left[\mathcal{N}\left(\cdot \mid \mu, \sigma^{2}\right)\right]=\mu$. (Note, however, that for some distributions, this integral is not finite, so the mean is not defined.)

We can also use the pdf to compute the variance of a distribution. This is a measure of the "spread", and is often denoted by $\sigma^{2}$. The variance is defined as follows:

$$
\begin{aligned}
\mathbb{V}[Y] & \triangleq \mathbb{E}\left[(Y-\mu)^{2}\right]=\int(y-\mu)^{2} p(y) d y \\
& =\int y^{2} p(y) d y+\mu^{2} \int p(y) d y-2 \mu \int y p(y) d y=\mathbb{E}\left[Y^{2}\right]-\mu^{2}
\end{aligned}
$$

from which we derive the useful result

$$
\mathbb{E}\left[Y^{2}\right]=\sigma^{2}+\mu^{2}
$$

The standard deviation is defined as

$$
\operatorname{std}[Y] \triangleq \sqrt{\mathbb{V}[Y]}=\sigma
$$

(The standard deviation can be more intepretable than the variance since it has the same units as $Y$ itself.) For a Gaussian, we have the familiar result that std $\left[\mathcal{N}\left(\cdot \mid \mu, \sigma^{2}\right)\right]=\sigma$.

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_7978c08eaaee0a4861dag-1.jpg?height=331&width=449&top_left_y=240&top_left_x=429

ChatGPT figure/image summary: The image displays a scatter plot with a linear regression fit to the data points. The scatter plot, which consists of numerous discrete blue dots, likely represents observed data that varies around a trend. A solid red line appears to be the linear regression line, the best fit straight line through the data that represents the relationship between the independent variable (on the x-axis) and the dependent variable (on the y-axis).

Accompanying the regression line are two parallel green lines that appear to define a confidence or predictive interval around the regression line. These lines might represent a fixed variance around the mean prediction (the red line), illustrating the concept of homoscedasticity where the spread or variability of the data around the regression line doesn't change with the level of the independent variable. This is typically interpreted as the red line being the mean function $\mu(x) = b + wx$, with the green lines representing the interval $\mu(x) \pm 2\sigma$, which in the context of a Gaussian distribution, could roughly estimate a 95% predictive interval assuming the errors are normally distributed.

Intervals like these are used to show the uncertainty in the regression predictions; points outside these lines can be considered to have residuals larger than what would be expected 95% of the time under the model assumptions.)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_13_7978c08eaaee0a4861dag-1.jpg?height=329&width=434&top_left_y=241&top_left_x=1143

ChatGPT figure/image summary: The image shows a scatter plot with numerous blue dots representing individual data points. These points appear to have a linear relationship, as indicated by the red line through the center of the data, which seems to represent the mean or expected value of the response variable based on the model (linear regression). Additionally, there are two green lines that are equidistant from the red line on either side, which likely represent the 95% predictive interval, showing the range where new observations are expected to fall with a 95% probability assuming a Gaussian output with fixed variance (homoskedastic). The slope of the red line suggests a positive correlation between the variables, indicating that as the value of the x-axis increases, so does the y-value. The spread of the data points around the red line implies the variability or noise in the data, which the linear regression model attempts to capture.)

(b)

Figure 2.14: Linear regression using Gaussian output with mean $\mu(x)=b+w x$ and (a) fixed variance $\sigma^{2}$ (homoskedastic) or (b) input-dependent variance $\sigma(x)^{2}$ (heteroscedastic). Generated by linreg_1d_hetero_tfp.ipynb.

\title{
2.6.3 Regression
}

So far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right)
$$

where $f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}$ predicts the mean, and $f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}$predicts the variance.

It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid \boldsymbol{w}^{\top} \boldsymbol{x}+b, \sigma^{2}\right)
$$

where $\boldsymbol{\theta}=\left(\boldsymbol{w}, b, \sigma^{2}\right)$. See Figure 2.14(a) for an illustration of this model in 1d. and Section 11.2 for more details on this model.

However, we can also make the variance depend on the input; this is called heteroskedastic regression. In the linear regression setting, we have

$$
p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid \boldsymbol{w}_{\mu}^{\top} \boldsymbol{x}+b, \sigma_{+}\left(\boldsymbol{w}_{\sigma}^{\top} \boldsymbol{x}\right)\right)
$$

where $\boldsymbol{\theta}=\left(\boldsymbol{w}_{\mu}, \boldsymbol{w}_{\sigma}\right)$ are the two forms of regression weights, and

$$
\sigma_{+}(a)=\log \left(1+e^{a}\right)
$$

is the softplus function, that maps from $\mathbb{R}$ to $\mathbb{R}_{+}$, to ensure the predicted standard deviation is non-negative. See Figure 2.14(b) for an illustration of this model in 1d.

Note that Figure 2.14 plots the $95 \%$ predictive interval, $[\mu(x)-2 \sigma(x), \mu(x)+2 \sigma(x)]$. This is the uncertainty in the predicted observation $y$ given $\boldsymbol{x}$, and captures the variability in the blue dots. By contrast, the uncertainty in the underlying (noise-free) function is represented by $\sqrt{\mathbb{V}\left[f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta})\right]}$, which does not involve the $\sigma$ term; now the uncertainty is over the parameters $\boldsymbol{\theta}$, rather than the output $y$. See Section 11.7 for details on how to model parameter uncertainty.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

\title{
2.6.4 Why is the Gaussian distribution so widely used?
}

The Gaussian distribution is the most widely used distribution in statistics and machine learning. There are several reasons for this. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. Second, the central limit theorem (Section 2.8.6) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or "noise". Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section 3.4.4; this makes it a good default choice in many cases. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see in Section 3.2.

From a historical perspective, it's worth remarking that the term "Gaussian distribution" is a bit misleading, since, as Jaynes [Jay03, p241] notes: "The fundamental nature of this distribution and its main properties were noted by Laplace when Gauss was six years old; and the distribution itself had been found by de Moivre before Laplace was born". However, Gauss popularized the use of the distribution in the 1800s, and the term "Gaussian" is now widely used in science and engineering.

The name "normal distribution" seems to have arisen in connection with the normal equations in linear regression (see Section 11.2.2.2). However, we prefer to avoid the term "normal", since it suggests other distributions are "abnormal", whereas, as Jaynes [Jay03] points out, it is the Gaussian that is abnormal in the sense that it has many special properties that are untypical of general distributions.

\subsection*{2.6.5 Dirac delta function as a limiting case}

As the variance of a Gaussian goes to 0 , the distribution approaches an infinitely narrow, but infinitely tall, "spike" at the mean. We can write this as follows:

$$
\lim _{\sigma \rightarrow 0} \mathcal{N}\left(y \mid \mu, \sigma^{2}\right) \rightarrow \delta(y-\mu)
$$

where $\delta$ is the Dirac delta function, defined by

$$
\delta(x)= \begin{cases}+\infty & \text { if } x=0 \\ 0 & \text { if } x \neq 0\end{cases}
$$

where

$$
\int_{-\infty}^{\infty} \delta(x) d x=1
$$

A slight variant of this is to define

$$
\delta_{y}(x)= \begin{cases}+\infty & \text { if } x=y \\ 0 & \text { if } x \neq y\end{cases}
$$

Note that we have

$$
\delta_{y}(x)=\delta(x-y)
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_3154fe948759464a8bc7g-1.jpg?height=551&width=1416&top_left_y=186&top_left_x=302

ChatGPT figure/image summary: This image shows two charts, labeled as Figure 2.15 (a) and (b), depicting probability density functions (pdf) and their logarithmic transformations. Specifically, the charts compare the Gaussian distribution with the Student's t-distribution (for \(\nu=1\) and \(\nu=2\)) and the Laplace distribution.

In chart (a), the pdf is plotted along the y-axis against the variable \( x \) on the x-axis. The charts show the shapes of the different distributions, comparing how their probabilities are distributed over the range of \( x \) values. The Gaussian distribution is shown as a dotted line, and the different Student distributions are shown as dashed and dot-dashed lines, respectively, for \( \nu=1 \) and \( \nu=2 \). The Laplace distribution is plotted as a solid line.

In chart (b), the logarithms of the probability density functions (log-pdfs) are plotted, providing a different perspective that emphasizes the distributions' tails. As before, the different lines represent the corresponding distributions, with the same color and line styles as in chart (a).

The purpose of this comparison is to illustrate the differences in behavior between these distributions, particularly in their tails—the Student's t-distribution, for example, has heavier tails compared to the Gaussian, which implies it assigns more probability to extreme events, potentially making it more robust to outliers in statistical modeling. The figure is likely used to supplement a discussion on these distributions in the context of probabilistic machine learning.)

Figure 2.15: (a) The pdf's for a $\mathcal{N}(0,1), \mathcal{T}(\mu=0, \sigma=1, \nu=1), \mathcal{T}(\mu=0, \sigma=1, \nu=2)$, and Laplace $(0,1 / \sqrt{2})$. The mean is 0 and the variance is 1 for both the Gaussian and Laplace. When $\nu=1$, the Student is the same as the Cauchy, which does not have a well-defined mean and variance. (b) Log of these pdf's. Note that the Student distribution is not log-concave for any parameter value, unlike the Laplace distribution. Nevertheless, both are unimodal. Generated by student_laplace_pdf_plot.ipynb.

The delta function distribution satisfies the following sifting property, which we will use later on:

$$
\int_{-\infty}^{\infty} f(y) \delta(x-y) d y=f(x)
$$

\title{
2.7 Some other common univariate distributions *
}

In this section, we briefly introduce some other univariate distributions that we will use in this book.

\subsection*{2.7.1 Student $t$ distribution}

The Gaussian distribution is quite sensitive to outliers. A robust alternative to the Gaussian is the Student $t$-distribution, which we shall call the Student distribution for short. ${ }^{9}$ Its pdf is as follows:

$$
\mathcal{T}\left(y \mid \mu, \sigma^{2}, \nu\right) \propto\left[1+\frac{1}{\nu}\left(\frac{y-\mu}{\sigma}\right)^{2}\right]^{-\left(\frac{\nu+1}{2}\right)}
$$

where $\mu$ is the mean, $\sigma>0$ is the scale parameter (not the standard deviation), and $\nu>0$ is called the degrees of freedom (although a better term would be the degree of normality [Kru13], since large values of $\nu$ make the distribution act like a Gaussian).
\footnotetext{
9. This distribution has a colorful etymology. It was first published in 1908 by William Sealy Gosset, who worked at the Guinness brewery in Dublin, Ireland. Since his employer would not allow him to use his own name, he called it the "Student" distribution. The origin of the term $t$ seems to have arisen in the context of tables of the Student distribution, used by Fisher when developing the basis of classical statistical inference. See http://jeff560.tripod.com/s.html for more historical details.
}

If the events are mutually exclusive (so they cannot happen at the same time), we get

$$
\operatorname{Pr}(A \vee B)=\operatorname{Pr}(A)+\operatorname{Pr}(B)
$$

For example, suppose $X$ is chosen uniformly at random from the set $\mathcal{X}=\{1,2,3,4\}$. Let $A$ be the event that $X \in\{1,2\}$ and $B$ be the event that $X \in\{3\}$. Then we have $\operatorname{Pr}(A \vee B)=\frac{2}{4}+\frac{1}{4}$.

\title{
2.1.3.4 Conditional probability of one event given another
}

We define the conditional probability of event $B$ happening given that $A$ has occurred as follows:

$$
\operatorname{Pr}(B \mid A) \triangleq \frac{\operatorname{Pr}(A, B)}{\operatorname{Pr}(A)}
$$

This is not defined if $\operatorname{Pr}(A)=0$, since we cannot condition on an impossible event.

\subsection*{2.1.3.5 Independence of events}

We say that event $A$ is independent of event $B$ if

$$
\operatorname{Pr}(A, B)=\operatorname{Pr}(A) \operatorname{Pr}(B)
$$

\subsection*{2.1.3.6 Conditional independence of events}

We say that events $A$ and $B$ are conditionally independent given event $C$ if

$$
\operatorname{Pr}(A, B \mid C)=\operatorname{Pr}(A \mid C) \operatorname{Pr}(B \mid C)
$$

This is written as $A \perp B \mid C$. Events are often dependent on each other, but may be rendered independent if we condition on the relevant intermediate variables, as we discuss in more detail later in this chapter.

\subsection*{2.2 Random variables}

Suppose $X$ represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of $X$ is unknown and/or could change, we call it a random variable or $\mathbf{r v}$. The set of possible values, denoted $\mathcal{X}$, is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if $X$ represents the face of a dice that is rolled, so $\mathcal{X}=\{1,2, \ldots, 6\}$, the event of "seeing a 1 " is denoted $X=1$, the event of "seeing an odd number" is denoted $X \in\{1,3,5\}$, the event of "seeing a number between 1 and 3 " is denoted $1 \leq X \leq 3$, etc.

\subsection*{2.2.1 Discrete random variables}

If the sample space $\mathcal{X}$ is finite or countably infinite, then $X$ is called a discrete random variable. In this case, we denote the probability of the event that $X$ has value $x$ by $\operatorname{Pr}(X=x)$. We define the

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_57844719c558b7c67b96g-1.jpg?height=535&width=1276&top_left_y=207&top_left_x=380

ChatGPT figure/image summary: The image displays two plots, labeled as figure 2.16 (a) and (b), which illustrate the effect of outliers on fitting Gaussian, Student, and Laplace distributions to a dataset. 

In both plots, there are histograms representing some data, and three different curves overlaid on the histograms, which correspond to three types of probability density functions (PDFs) fitted to the data:

1. A dotted line representing a Gaussian (or normal) distribution fit.
2. A dashed line representing a Student's t-distribution fit.
3. A solid line representing a Laplace (double-sided exponential) distribution fit.

Plot (a) shows a situation with no outliers, and you can see that the Gaussian and Student t-distribution lines are very close to each other, indicating that they fit the data similarly when there are no outliers present.

Plot (b) shows what happens when outliers are added to the data. In this case, the Gaussian distribution is visibly more affected by the outliers—its curve is drawn towards the tails by the extreme values. Meanwhile, the Student and Laplace distributions are less influenced, with the latter two showing greater robustness to outliers—their shapes do not change as dramatically as the Gaussian's.

The illustration is meant to convey that both the Student's t-distribution and the Laplace distribution are more robust to outliers compared to the Gaussian distribution, which is sensitive to the presence of outliers and can result in a distorted representation of the main trend of the data.)

Figure 2.16: Illustration of the effect of outliers on fitting Gaussian, Student and Laplace distributions. (a) No outliers (the Gaussian and Student curves are on top of each other). (b) With outliers. We see that the Gaussian is more affected by outliers than the Student and Laplace distributions. Adapted from Figure 2.16 of [Bis06]. Generated by robust_pdf_plot.ipynb.

We see that the probability density decays as a polynomial function of the squared distance from the center, as opposed to an exponential function, so there is more probability mass in the tail than with a Gaussian distribution, as shown in Figure 2.15. We say that the Student distribution has heavy tails, which makes it robust to outliers.

To illustrate the robustness of the Student distribution, consider Figure 2.16. On the left, we show a Gaussian and a Student distribution fit to some data with no outliers. On the right, we add some outliers. We see that the Gaussian is affected a lot, whereas the Student hardly changes. We discuss how to use the Student distribution for robust linear regression in Section 11.6.2.

For later reference, we note that the Student distribution has the following properties:

$$
\text { mean }=\mu, \text { mode }=\mu, \operatorname{var}=\frac{\nu \sigma^{2}}{(\nu-2)}
$$

The mean is only defined if $\nu>1$. The variance is only defined if $\nu>2$. For $\nu \gg 5$, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties. It is common to use $\nu=4$, which gives good performance in a range of problems [LLT89].

\title{
2.7.2 Cauchy distribution
}

If $\nu=1$, the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by

$$
\mathcal{C}(x \mid \mu, \gamma)=\frac{1}{\gamma \pi}\left[1+\left(\frac{x-\mu}{\gamma}\right)^{2}\right]^{-1}
$$

This distribution has very heavy tails compared to a Gaussian. For example, $95 \%$ of the values from a standard normal are between -1.96 and 1.96, but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge.

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

The half Cauchy distribution is a version of the Cauchy (with $\mu=0$ ) that is "folded over" on itself, so all its probability density is on the positive reals. Thus it has the form

$$
\mathcal{C}_{+}(x \mid \gamma) \triangleq \frac{2}{\pi \gamma}\left[1+\left(\frac{x}{\gamma}\right)^{2}\right]^{-1}
$$

This is useful in Bayesian modeling, where we want to use a distribution over positive reals with heavy tails, but finite density at the origin.

\title{
2.7.3 Laplace distribution
}

Another distribution with heavy tails is the Laplace distribution ${ }^{10}$, also known as the double sided exponential distribution. This has the following pdf:

$$
\operatorname{Laplace}(y \mid \mu, b) \triangleq \frac{1}{2 b} \exp \left(-\frac{|y-\mu|}{b}\right)
$$

See Figure 2.15 for a plot. Here $\mu$ is a location parameter and $b>0$ is a scale parameter. This distribution has the following properties:

$$
\text { mean }=\mu, \text { mode }=\mu, \text { var }=2 b^{2}
$$

In Section 11.6.1, we discuss how to use the Laplace distribution for robust linear regression, and in Section 11.4, we discuss how to use the Laplace distribution for sparse linear regression.

\subsection*{2.7.4 Beta distribution}

The beta distribution has support over the interval $[0,1]$ and is defined as follows:

$$
\operatorname{Beta}(x \mid a, b)=\frac{1}{B(a, b)} x^{a-1}(1-x)^{b-1}
$$

where $B(a, b)$ is the beta function, defined by

$$
B(a, b) \triangleq \frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)}
$$

where $\Gamma(a)$ is the Gamma function defined by

$$
\Gamma(a) \triangleq \int_{0}^{\infty} x^{a-1} e^{-x} d x
$$

See Figure 2.17a for plots of some beta distributions.

We require $a, b>0$ to ensure the distribution is integrable (i.e., to ensure $B(a, b)$ exists). If $a=b=1$, we get the uniform distribution. If $a$ and $b$ are both less than 1 , we get a bimodal distribution with "spikes" at 0 and 1 ; if $a$ and $b$ are both greater than 1 , the distribution is unimodal.

For later reference, we note that the distribution has the following properties (Exercise 2.8):

$$
\text { mean }=\frac{a}{a+b}, \text { mode }=\frac{a-1}{a+b-2}, \text { var }=\frac{a b}{(a+b)^{2}(a+b+1)}
$$
\footnotetext{
10. Pierre-Simon Laplace (1749-1827) was a French mathematician, who played a key role in creating the field of Bayesian statistics.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license
}

![](https://cdn.mathpix.com/cropped/2024_06_13_e6f01ce7d8503a9d9eadg-1.jpg?height=551&width=1288&top_left_y=222&top_left_x=366

ChatGPT figure/image summary: The image you provided contains two subfigures labeled (a) and (b), displaying different probability density functions (PDFs) for the Beta and Gamma distributions, respectively.

Subfigure (a) shows the PDFs for various parameters of the Beta distribution, specifically labeled for combinations of 'a' and 'b' parameters that shape the curve: (a=0.1, b=0.1), (a=0.1, b=1.0), (a=1.0, b=1.0), (a=2.0, b=2.0), and (a=2.0, b=8.0). Each curve is assigned a different color, and you can observe how the shape of the Beta distribution changes depending on these parameters, ranging from "spike"-like shapes to more bell-shaped, to skewed distributions based on these values.

Subfigure (b) shows various PDFs for Gamma distributions with shape 'a' and rate 'b' parameters as follows: (a=1.0, b=1.0), (a=1.5, b=1.0), (a=2.0, b=1.0), and then again these same 'a' values paired with a higher rate 'b' value of 2.0. The solid lines represent the PDFs with a rate 'b' of 1.0, while the dashed lines represent those with a rate 'b' of 2.0. Different color codes correspond to the different 'a' values. Here, you can see how the PDF shape changes as we vary both the shape parameter 'a' and the rate 'b', with modes ranging from the origin to further along the x-axis.

Both of these plots serve to illustrate the versatility of the Beta and Gamma distributions in modeling data under a variety of conditions and parameterizations.)

Figure 2.17: (a) Some beta distributions. If $a<1$, we get a "spike" on the left, and if $b<1$, we get a "spike" on the right. if $a=b=1$, the distribution is uniform. If $a>1$ and $b>1$, the distribution is unimodal. Generated by beta_dist_plot.ipynb. (b) Some gamma distributions. If $a \leq 1$, the mode is at 0 , otherwise the mode is away from 0 . As we increase the rate $b$, we reduce the horizontal scale, thus squeezing everything leftwards and upwards. Generated by gamma_dist_plot.ipynb.

\title{
2.7.5 Gamma distribution
}

The gamma distribution is a flexible distribution for positive real valued rv's, $x>0$. It is defined in terms of two parameters, called the shape $a>0$ and the rate $b>0$ :

$$
\operatorname{Ga}(x \mid \text { shape }=a, \text { rate }=b) \triangleq \frac{b^{a}}{\Gamma(a)} x^{a-1} e^{-x b}
$$

Sometimes the distribution is parameterized in terms of the shape $a$ and the scale $s=1 / b$ :

$$
\operatorname{Ga}(x \mid \text { shape }=a, \text { scale }=s) \triangleq \frac{1}{s^{a} \Gamma(a)} x^{a-1} e^{-x / s}
$$

See Figure 2.17b for some plots of the gamma pdf.

For reference, we note that the distribution has the following properties:

$$
\text { mean }=\frac{a}{b}, \text { mode }=\frac{a-1}{b}, \text { var }=\frac{a}{b^{2}}
$$

There are several distributions which are just special cases of the Gamma, which we discuss below.

- Exponential distribution. This is defined by

$$
\operatorname{Expon}(x \mid \lambda) \triangleq \operatorname{Ga}(x \mid \text { shape }=1, \text { rate }=\lambda)
$$

This distribution describes the times between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate $\lambda$.

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_aee57e93246230d45dbeg-1.jpg?height=592&width=1237&top_left_y=189&top_left_x=402

ChatGPT figure/image summary: The image showcases two sub-figures labeled (a) and (b), which serve as illustrations related to the empirical probability density function (pdf) and cumulative distribution function (cdf) described in the context provided.

Figure (a) represents the empirical pdf with a curve suggesting the underlying probability density being approximated. Overlaying the curve are vertical dashed lines, each topped with an arrow, pointing to the five samples each denoted by `x^{(n)}` where `n` runs from 1 to 5. These are used to approximate the pdf using delta functions, hence the shape of arrows (spikes) representing the delta functions at the respective sample points.

Figure (b) presents the empirical cdf with a staircase function that steps upward at each of the five sample points `x^{(n)}`. Each step has a height of `1/5`, indicated by the fractional values next to the vertical dashed lines, consistent with the cumulative distribution stepping up by `1/N` at every sample when `N` is 5. The sample points are further emphasized with hollow circles, and the incremental steps are represented by filled circles.

These two figures visually demonstrate the concepts of empirical probability distributions discussed in the text, specifically how a pdf and cdf can be approximated from a finite set of samples.)

Figure 2.18: Illustration of the (a) empirical pdf and (b) empirical cdf derived from a set of $N=5$ samples. From https: //bit. ly/3hFgi0e. Used with kind permission of Mauro Escudero.

- Chi-squared distribution. This is defined by

$$
\chi_{\nu}^{2}(x) \triangleq \mathrm{Ga}\left(x \mid \text { shape }=\frac{\nu}{2}, \text { rate }=\frac{1}{2}\right)
$$

where $\nu$ is called the degrees of freedom. This is the distribution of the sum of squared Gaussian random variables. More precisely, if $Z_{i} \sim \mathcal{N}(0,1)$, and $S=\sum_{i=1}^{\nu} Z_{i}^{2}$, then $S \sim \chi_{\nu}^{2}$.

- The inverse Gamma distribution is defined as follows:

$$
\operatorname{IG}(x \mid \text { shape }=a, \text { scale }=b) \triangleq \frac{b^{a}}{\Gamma(a)} x^{-(a+1)} e^{-b / x}
$$

The distribution has these properties

$$
\text { mean }=\frac{b}{a-1}, \text { mode }=\frac{b}{a+1}, \text { var }=\frac{b^{2}}{(a-1)^{2}(a-2)}
$$

The mean only exists if $a>1$. The variance only exists if $a>2$. Note: if $X \sim \mathrm{Ga}$ (shape $=$ $a$, rate $=b$ ), then $1 / X \sim \operatorname{IG}$ (shape $=a$, scale $=b$ ). (Note that $b$ plays two different roles in this case.)

\title{
2.7.6 Empirical distribution
}

Suppose we have a set of $N$ samples $\mathcal{D}=\left\{x^{(1)}, \ldots, x^{(N)}\right\}$, derived from a distribution $p(X)$, where $X \in \mathbb{R}$. We can approximate the pdf using a set of delta functions (Section 2.6.5) or "spikes", centered on these samples:

$$
\hat{p}_{N}(x)=\frac{1}{N} \sum_{n=1}^{N} \delta_{x^{(n)}}(x)
$$

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

This is called the empirical distribution of the dataset $\mathcal{D}$. An example of this, with $N=5$, is shown in Figure 2.18(a).

The corresponding cdf is given by

$$
\hat{P}_{N}(x)=\frac{1}{N} \sum_{n=1}^{N} \mathbb{I}\left(x^{(n)} \leq x\right)=\frac{1}{N} \sum_{n=1}^{N} u_{x^{(n)}}(x)
$$

where $u_{y}(x)$ is a step function at $y$ defined by

$$
u_{y}(x)= \begin{cases}1 & \text { if } x \geq y \\ 0 & \text { if } x<y\end{cases}
$$

This can be visualized as a "stair case", as in Figure 2.18(b), where the jumps of height $1 / N$ occur at every sample.

\title{
2.8 Transformations of random variables *
}

Suppose $\boldsymbol{x} \sim p()$ is some random variable, and $\boldsymbol{y}=f(\boldsymbol{x})$ is some deterministic transformation of it. In this section, we discuss how to compute $p(\boldsymbol{y})$.

\subsection*{2.8.1 Discrete case}

If $X$ is a discrete rv, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$ 's such that $f(x)=y$ :

$$
p_{y}(y)=\sum_{x: f(x)=y} p_{x}(x)
$$

For example, if $f(X)=1$ if $X$ is even and $f(X)=0$ otherwise, and $p_{x}(X)$ is uniform on the set $\{1, \ldots, 10\}$, then $p_{y}(1)=\sum_{x \in\{2,4,6,8,10\}} p_{x}(x)=0.5$, and hence $p_{y}(0)=0.5$ also. Note that in this example, $f$ is a many-to-one function.

\subsection*{2.8.2 Continuous case}

If $X$ is continuous, we cannot use Equation (2.150) since $p_{x}(x)$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf's, as follows:

$$
P_{y}(y) \triangleq \operatorname{Pr}(Y \leq y)=\operatorname{Pr}(f(X) \leq y)=\operatorname{Pr}(X \in\{x \mid f(x) \leq y\})
$$

If $f$ is invertible, we can derive the pdf of $y$ by differentiating the cdf, as we show below. If $f$ is not invertible, we can use numerical integration, or a Monte Carlo approximation.

\subsection*{2.8.3 Invertible transformations (bijections)}

In this section, we consider the case of monotonic and hence invertible functions. (Note a function is invertible iff it is a bijector). With this assumption, there is a simple formula for the pdf of $y$, as we will see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this case.)

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_9144c552ba5b89e1e6c1g-1.jpg?height=390&width=426&top_left_y=198&top_left_x=450

ChatGPT figure/image summary: The image depicts a graphical representation of a probability density function (pdf) transformation. On the horizontal axis x, there is the original probability density function (pdf) p(x), which appears to be uniform between 0 and 1, with a value of 1 (this suggests a uniform distribution where each outcome in the interval [0,1] is equally likely). Below the x-axis, there's the transformation function f: R → R, f(x) = 2x + 1, which defines how x values are mapped to y values.

The arrows indicate the mapping of the x domain onto the y domain through the transformation function. As a result, the original pdf is stretched and shifted, creating a new pdf p(y) on the vertical y-axis. The p(y) depicts a uniform distribution between 1 and 3 with a value of 0.5, consistent with the transformation of the unit interval [0,1] to [1,3] and the corresponding scaling of the probability density to maintain the total probability mass.)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_13_9144c552ba5b89e1e6c1g-1.jpg?height=266&width=431&top_left_y=321&top_left_x=1157

ChatGPT figure/image summary: The image appears to be a graphical illustration related to mathematical functions and their derivatives. It shows two graphs with lines representing functions, where the slope of the line is denoted as dy/dx. The left graph corresponds to a positive slope (dy/dx > 0), indicating that the function is increasing, and hence y increases as x increases. The right graph corresponds to a negative slope (dy/dx < 0), indicating that the function is decreasing, and hence y decreases as x increases.

Both graphs include small increments labeled dx and dy, representing an infinitesimally small change in x and the corresponding change in y, respectively. These illustrations are typically used to explain concepts related to derivatives in calculus, which measure the rate at which one variable changes with respect to another. The graphs could be used to illustrate how functions transform small intervals when the function is increasing or decreasing, as part of a discussion on transformations of random variables or the concept of a function's slope and rate of change.)

(b)

Figure 2.19: (a) Mapping a uniform pdf through the function $f(x)=2 x+1$. (b) Illustration of how two nearby points, $x$ and $x+d x$, get mapped under $f$. If $\frac{d y}{d x}>0$, the function is locally increasing, but if $\frac{d y}{d x}<0$, the function is locally decreasing. From [Jan18]. Used with kind permission of Eric Jang.

\title{
2.8.3.1 Change of variables: scalar case
}

We start with an example. Suppose $x \sim \operatorname{Unif}(0,1)$, and $y=f(x)=2 x+1$. This function stretches and shifts the probability distribution, as shown in Figure 2.19(a). Now let us zoom in on a point $x$ and another point that is infinitesimally close, namely $x+d x$. We see this interval gets mapped to $(y, y+d y)$. The probability mass in these intervals must be the same, hence $p(x) d x=p(y) d y$, and so $p(y)=p(x) d x / d y$. However, since it does not matter (in terms of probability preservation) whether $d x / d y>0$ or $d x / d y<0$, we get

$$
p_{y}(y)=p_{x}(x)\left|\frac{d x}{d y}\right|
$$

Now consider the general case for any $p_{x}(x)$ and any monotonic function $f: \mathbb{R} \rightarrow \mathbb{R}$. Let $g=f^{-1}$, so $y=f(x)$ and $x=g(y)$. If we assume that $f: \mathbb{R} \rightarrow \mathbb{R}$ is monotonically increasing we get

$$
P_{y}(y)=\operatorname{Pr}(f(X) \leq y)=\operatorname{Pr}\left(X \leq f^{-1}(y)\right)=P_{x}\left(f^{-1}(y)\right)=P_{x}(g(y))
$$

Taking derivatives we get

$$
p_{y}(y) \triangleq \frac{d}{d y} P_{y}(y)=\frac{d}{d y} P_{x}(x)=\frac{d x}{d y} \frac{d}{d x} P_{x}(x)=\frac{d x}{d y} p_{x}(x)
$$

We can derive a similar expression (but with opposite signs) for the case where $f$ is monotonically decreasing. To handle the general case we take the absolute value to get

$$
p_{y}(y)=p_{x}(g(y))\left|\frac{d}{d y} g(y)\right|
$$

This is called change of variables formula.

\subsection*{2.8.3.2 Change of variables: multivariate case}

We can extend the previous results to multivariate distributions as follows. Let $\boldsymbol{f}$ be an invertible function that maps $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$, with inverse $\boldsymbol{g}$. Suppose we want to compute the pdf of $\boldsymbol{y}=\boldsymbol{f}(\boldsymbol{x})$. By

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license


![](https://cdn.mathpix.com/cropped/2024_06_13_a723e795abd87511cc8bg-1.jpg?height=390&width=938&top_left_y=198&top_left_x=548

ChatGPT figure/image summary: The image depicts two graphs related to an affine transformation applied to a unit square in the context of multivariate distributions. On the left side (labeled "No Scale, Shift Only"), there's a green square representing the original unit square with coordinates on the axes indicated by \( x_1 \) and \( x_2 \). This square is shifted without scaling to form the blue square, showing the transformation only involves a translation in the space. Arrows indicate the direction and distance of the shift from the original to the transformed square.

On the right side, we see a transformation where a square is not only shifted but also scaled and possibly rotated, resulting in a parallelogram. The area of this parallelogram is given by the expression "ad-bc," which represents the determinant of the transformation matrix. Here, the points \( (a, b) \) and \( (c, d) \) likely represent the columns of the transformation matrix \( A \), and the point \( (a+c, b+d) \) represents the addition of these columns, showing the combined effect of the transformation on one corner of the unit square.

This image is meant to illustrate the change of variables concept, showing how the affine transformation affects the geometry of the given shapes and how it relates to probability density functions (PDFs) in the multivariate case. The area of the transformed shape is indicative of the determinant of the Jacobian matrix, which is a crucial factor in computing the new PDF after the transformation.)

Figure 2.20: Illustration of an affine transformation applied to a unit square, $f(\boldsymbol{x})=\mathbf{A} \boldsymbol{x}+\boldsymbol{b}$. (a) Here $\mathbf{A}=\mathbf{I}$. (b) Here $\boldsymbol{b}=\mathbf{0}$. From [Jan18]. Used with kind permission of Eric Jang.

analogy with the scalar case, we have

$$
p_{y}(\boldsymbol{y})=p_{x}(\boldsymbol{g}(\boldsymbol{y}))\left|\operatorname{det}\left[\mathbf{J}_{g}(\boldsymbol{y})\right]\right|
$$

where $\mathbf{J}_{g}=\frac{d \boldsymbol{g}(\boldsymbol{y})}{d \boldsymbol{y}^{\top}}$ is the Jacobian of $\boldsymbol{g}$, and $|\operatorname{det} \mathbf{J}(\boldsymbol{y})|$ is the absolute value of the determinant of $\mathbf{J}$ evaluated at $\boldsymbol{y}$. (See Section 7.8.5 for a discussion of Jacobians.) In Exercise 3.6 you will use this formula to derive the normalization constant for a multivariate Gaussian.

Figure 2.20 illustrates this result in $2 \mathrm{~d}$, for the case where $f(\boldsymbol{x})=\mathbf{A} \boldsymbol{x}+\boldsymbol{b}$, where $\mathbf{A}=\left(\begin{array}{ll}a & c \\ b & d\end{array}\right)$. We see that the area of the unit square changes by a factor of $\operatorname{det}(\mathbf{A})=a d-b c$, which is the area of the parallelogram.

As another example, consider transforming a density from Cartesian coordinates $\boldsymbol{x}=\left(x_{1}, x_{2}\right)$ to polar coordinates $\boldsymbol{y}=\boldsymbol{f}\left(x_{1}, x_{2}\right)$, so $\boldsymbol{g}(r, \theta)=(r \cos \theta, r \sin \theta)$. Then

$$
\begin{aligned}
\mathbf{J}_{g} & =\left(\begin{array}{ll}
\frac{\partial x_{1}}{\partial x_{2}} & \frac{\partial x_{1}}{\partial \theta_{2}} \\
\frac{\partial x_{2}}{\partial r} & \frac{\partial x_{2}}{\partial \theta}
\end{array}\right)=\left(\begin{array}{cc}
\cos \theta & -r \sin \theta \\
\sin \theta & r \cos \theta
\end{array}\right) \\
\left|\operatorname{det}\left(\mathbf{J}_{g}\right)\right| & =\left|r \cos ^{2} \theta+r \sin ^{2} \theta\right|=|r|
\end{aligned}
$$

Hence

$$
p_{r, \theta}(r, \theta)=p_{x_{1}, x_{2}}(r \cos \theta, r \sin \theta) r
$$

To see this geometrically, notice that the area of the shaded patch in Figure 2.21 is given by

$$
\operatorname{Pr}(r \leq R \leq r+d r, \theta \leq \Theta \leq \theta+d \theta)=p_{r, \theta}(r, \theta) d r d \theta
$$

In the limit, this is equal to the density at the center of the patch times the size of the patch, which is given by $r d r d \theta$. Hence

$$
p_{r, \theta}(r, \theta) d r d \theta=p_{x_{1}, x_{2}}(r \cos \theta, r \sin \theta) r d r d \theta
$$

![](https://cdn.mathpix.com/cropped/2024_06_13_54c60bf0fccf07f0954bg-1.jpg?height=510&width=561&top_left_y=199&top_left_x=730

ChatGPT figure/image summary: The image displays a geometric illustration of the change of variables from polar to Cartesian coordinates. It shows a small patch of an area in polar coordinates defined by the radius \( r \) and the angle \( \theta \). The region is bound by the curves from \( r \) to \( r + dr \) (indicating a small change in radius) and the lines from \( \theta \) to \( \theta + d\theta \) (indicating a small change in angle).

The shaded area represents the differential area element in polar coordinates, which can be expressed as \( r dr d\theta \). This expression accounts for the area of the infinitesimal sector-shaped patch in a polar coordinate system, where \( dr \) is the change in the radial direction and \( r d\theta \) is the arc length defining the change in the angular direction, with \( r \) being the radius of the arc. The Cartesian axes are also shown, highlighting how the patch relates to these axes in terms of position.)

Figure 2.21: Change of variables from polar to Cartesian. The area of the shaded patch is $r d r d \theta$. Adapted from Figure 3.16 of [Ric95].

\title{
2.8.4 Moments of a linear transformation
}

Suppose $f$ is an affine function, so $\boldsymbol{y}=\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}$. In this case, we can easily derive the mean and covariance of $\boldsymbol{y}$ as follows. First, for the mean, we have

$$
\mathbb{E}[\boldsymbol{y}]=\mathbb{E}[\mathbf{A} \boldsymbol{x}+\boldsymbol{b}]=\mathbf{A} \boldsymbol{\mu}+\boldsymbol{b}
$$

where $\boldsymbol{\mu}=\mathbb{E}[\boldsymbol{x}]$. If $f$ is a scalar-valued function, $f(\boldsymbol{x})=\boldsymbol{a}^{\top} \boldsymbol{x}+b$, the corresponding result is

$$
\mathbb{E}\left[\boldsymbol{a}^{\top} \boldsymbol{x}+b\right]=\boldsymbol{a}^{\top} \boldsymbol{\mu}+b
$$

For the covariance, we have

$$
\operatorname{Cov}[\boldsymbol{y}]=\operatorname{Cov}[\mathbf{A} \boldsymbol{x}+\boldsymbol{b}]=\mathbf{A} \boldsymbol{\Sigma} \mathbf{A}^{\top}
$$

where $\boldsymbol{\Sigma}=\operatorname{Cov}[\boldsymbol{x}]$. We leave the proof of this as an exercise.

As a special case, if $y=\boldsymbol{a}^{\top} \boldsymbol{x}+b$, we get

$$
\mathbb{V}[y]=\mathbb{V}\left[\boldsymbol{a}^{\top} \boldsymbol{x}+b\right]=\boldsymbol{a}^{\top} \boldsymbol{\Sigma} \boldsymbol{a}
$$

For example, to compute the variance of the sum of two scalar random variables, we can set $\boldsymbol{a}=[1,1]$ to get

$$
\begin{aligned}
\mathbb{V}\left[x_{1}+x_{2}\right] & =\left(\begin{array}{ll}
1 & 1
\end{array}\right)\left(\begin{array}{ll}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)\binom{1}{1} \\
& =\Sigma_{11}+\Sigma_{22}+2 \Sigma_{12}=\mathbb{V}\left[x_{1}\right]+\mathbb{V}\left[x_{2}\right]+2 \operatorname{Cov}\left[x_{1}, x_{2}\right]
\end{aligned}
$$

Note, however, that although some distributions (such as the Gaussian) are completely characterized by their mean and covariance, in general we must use the techniques described above to derive the full distribution of $\boldsymbol{y}$.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

$$
\begin{array}{cccccccc|l}
- & - & 1 & 2 & 3 & 4 & - & - & \\
\hline 7 & 6 & 5 & - & - & - & - & - & z_{0}=x_{0} y_{0}=5 \\
- & 7 & 6 & 5 & - & - & - & - & z_{1}=x_{0} y_{1}+x_{1} y_{0}=16 \\
- & - & 7 & 6 & 5 & - & - & - & z_{2}=x_{0} y_{2}+x_{1} y_{1}+x_{2} y_{0}=34 \\
- & - & - & 7 & 6 & 5 & - & - & z_{3}=x_{1} y_{2}+x_{2} y_{1}+x_{3} y_{0}=52 \\
- & - & - & - & 7 & 6 & 5 & - & z_{4}=x_{2} y_{2}+x_{3} y_{1}=45 \\
- & - & - & - & - & 7 & 6 & 5 & z_{5}=x_{3} y_{2}=28
\end{array}
$$

Table 2.4: Discrete convolution of $\boldsymbol{x}=[1,2,3,4]$ with $\boldsymbol{y}=[5,6,7]$ to yield $\boldsymbol{z}=[5,16,34,52,45,28]$. In general, $z_{n}=\sum_{k=-\infty}^{\infty} x_{k} y_{n-k}$. We see that this operation consists of "flipping" $\boldsymbol{y}$ and then "dragging" it over $\boldsymbol{x}$, multiplying elementwise, and adding up the results.

\title{
2.8.5 The convolution theorem
}

Let $y=x_{1}+x_{2}$, where $x_{1}$ and $x_{2}$ are independent rv's. If these are discrete random variables, we can compute the pmf for the sum as follows:

$$
p(y=j)=\sum_{k} p\left(x_{1}=k\right) p\left(x_{2}=j-k\right)
$$

for $j=\ldots,-2,-1,0,1,2, \ldots$

If $x_{1}$ and $x_{2}$ have pdf's $p_{1}\left(x_{1}\right)$ and $p_{2}\left(x_{2}\right)$, what is the distribution of $y$ ? The cdf for $y$ is given by

$$
P_{y}\left(y^{*}\right)=\operatorname{Pr}\left(y \leq y^{*}\right)=\int_{-\infty}^{\infty} p_{1}\left(x_{1}\right)\left[\int_{-\infty}^{y^{*}-x_{1}} p_{2}\left(x_{2}\right) d x_{2}\right] d x_{1}
$$

where we integrate over the region $R$ defined by $x_{1}+x_{2}<y^{*}$. Thus the pdf for $y$ is

$$
p(y)=\left[\frac{d}{d y^{*}} P_{y}\left(y^{*}\right)\right]_{y^{*}=y}=\int p_{1}\left(x_{1}\right) p_{2}\left(y-x_{1}\right) d x_{1}
$$

where we used the rule of differentiating under the integral sign:

$$
\frac{d}{d x} \int_{a(x)}^{b(x)} f(t) d t=f(b(x)) \frac{d b(x)}{d x}-f(a(x)) \frac{d a(x)}{d x}
$$

We can write Equation (2.170) as follows:

$$
p=p_{1} \circledast p_{2}
$$

where $\circledast$ represents the convolution operator. For finite length vectors, the integrals become sums, and convolution can be thought of as a "flip and drag" operation, as illustrated in Table 2.4. Consequently, Equation (2.170) is called the convolution theorem.

For example, suppose we roll two dice, so $p_{1}$ and $p_{2}$ are both the discrete uniform distributions

![](https://cdn.mathpix.com/cropped/2024_06_13_3b03ebbae3c95d0ddb56g-1.jpg?height=393&width=518&top_left_y=196&top_left_x=751

ChatGPT figure/image summary: The image is a bar chart representing the probability distribution of the sum of two dice rolls. The x-axis is labeled 'S' and represents the possible sums that can be obtained by rolling two six-sided dice, which range from 2 to 12. The y-axis shows two scales: on the left, we have the probability 'p(S)' of each sum occurring, displayed as a decimal fraction, and on the right, the same probabilities are expressed as ratios out of 36, which is the total number of possible outcomes when rolling two dice (6 sides per die times 2 dice).

Each bar on the chart corresponds to a sum, starting with 2 all the way up to 12. The height of each bar corresponds to its probability of occurrence. On top of the bars, there are visual representations of dice that add up to the sum associated with each bar. For instance, the bar for the sum of 2 is the shortest, corresponding to a probability of 1/36, since there is only one combination of dice that can result in this sum: a pair of ones. As the sums increase toward the middle of the distribution, the probabilities increase, with the sum of 7 having the highest probability of 6/36 (or 1/6), as there are six different dice combinations that produce this sum. The bars then decrease symmetrically, as the sums become less likely toward the extremes of the distribution.

This histogram is commonly used to teach basic probability and demonstrates the concept of a uniform discrete probability distribution for fair dice. The distribution forms a neat symmetric triangle, which is characteristic of the sum of two uniform distributions with the shape approximating a binomial distribution.)

Figure 2.22: Distribution of the sum of two dice rolls, i.e., $p(y)$ where $y=x_{1}+x_{2}$ and $x_{i} \sim \operatorname{Unif}(\{1,2, \ldots, 6\})$. From https://en.wikipedia.org/wiki/Probability_distribution. Used with kind permission of Wikipedia author Tim Stellmach.

over $\{1,2, \ldots, 6\}$. Let $y=x_{1}+x_{2}$ be the sum of the dice. We have

$$
\begin{aligned}
& p(y=2)=p\left(x_{1}=1\right) p\left(x_{2}=1\right)=\frac{1}{6} \frac{1}{6}=\frac{1}{36} \\
& p(y=3)=p\left(x_{1}=1\right) p\left(x_{2}=2\right)+p\left(x_{1}=2\right) p\left(x_{2}=1\right)=\frac{1}{6} \frac{1}{6}+\frac{1}{6} \frac{1}{6}=\frac{2}{36}
\end{aligned}
$$

Continuing in this way, we find $p(y=4)=3 / 36, p(y=5)=4 / 36, p(y=6)=5 / 36, p(y=7)=6 / 36$, $p(y=8)=5 / 36, p(y=9)=4 / 36, p(y=10)=3 / 36, p(y=11)=2 / 36$ and $p(y=12)=1 / 36$. See Figure 2.22 for a plot. We see that the distribution looks like a Gaussian; we explain the reasons for this in Section 2.8.6.

We can also compute the pdf of the sum of two continuous rv's. For example, in the case of Gaussians, where $x_{1} \sim \mathcal{N}\left(\boldsymbol{\mu}_{1}, \sigma_{1}^{2}\right)$ and $x_{2} \sim \mathcal{N}\left(\boldsymbol{\mu}_{2}, \sigma_{2}^{2}\right)$, one can show (Exercise 2.4) that if $y=x_{1}+x_{2}$ then

$$
p(y)=\mathcal{N}\left(x_{1} \mid \boldsymbol{\mu}_{1}, \sigma_{1}^{2}\right) \otimes \mathcal{N}\left(x_{2} \mid \boldsymbol{\mu}_{2}, \sigma_{2}^{2}\right)=\mathcal{N}\left(y \mid \boldsymbol{\mu}_{1}+\boldsymbol{\mu}_{2}, \sigma_{1}^{2}+\sigma_{2}^{2}\right)
$$

Hence the convolution of two Gaussians is a Gaussian.

\title{
2.8.6 Central limit theorem
}

Now consider $N_{\mathcal{D}}$ random variables with pdf's (not necessarily Gaussian) $p_{n}(x)$, each with mean $\mu$ and variance $\sigma^{2}$. We assume each variable is independent and identically distributed or iid for short, which means $X_{n} \sim p(X)$ are independent samples from the same distribution. Let $S_{N_{\mathcal{D}}}=\sum_{n=1}^{N_{\mathcal{D}}} X_{n}$ be the sum of the rv's. One can show that, as $N$ increases, the distribution of this sum approaches

$$
p\left(S_{N_{\mathcal{D}}}=u\right)=\frac{1}{\sqrt{2 \pi N_{\mathcal{D}} \sigma^{2}}} \exp \left(-\frac{\left(u-N_{\mathcal{D}} \mu\right)^{2}}{2 N_{\mathcal{D}} \sigma^{2}}\right)
$$

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_7a7e462c99307ff380fdg-1.jpg?height=429&width=1173&top_left_y=222&top_left_x=421

ChatGPT figure/image summary: The image shows two bar graphs with four bars each, labeled (a) and (b), representing two discrete probability mass functions (pmfs) on the state space \(\mathcal{X} = \{1, 2, 3, 4\}\).

In graph (a), there is a uniform distribution with all four bars at the same height, indicating a probability of \( p(x = k) = 1/4 \) for each of the four possible outcomes \( x = 1, 2, 3,\) and \(4\).

In graph (b), there is a degenerate distribution or delta function, where all the probability mass is placed on the outcome where \( x=1 \). This is depicted with the first bar at height 1 and the remaining three bars at height 0.

This image is used to illustrate the concept of discrete distributions, specifically a uniform distribution and a degenerate distribution within the context of the explanatory text.)

Figure 2.1: Some discrete distributions on the state space $\mathcal{X}=\{1,2,3,4\}$. (a) A uniform distribution with $p(x=k)=1 / 4$. (b) A degenerate distribution (delta function) that puts all its mass on $x=1$. Generated by discrete_prob_dist_plot.ipynb.

probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value:

$$
p(x) \triangleq \operatorname{Pr}(X=x)
$$

The pmf satisfies the properties $0 \leq p(x) \leq 1$ and $\sum_{x \in \mathcal{X}} p(x)=1$.

If $X$ has a finite number of values, say $K$, the pmf can be represented as a list of $K$ numbers, which we can plot as a histogram. For example, Figure 2.1 shows two pmf's defined on $\mathcal{X}=\{1,2,3,4\}$. On the left we have a uniform distribution, $p(x)=1 / 4$, and on the right, we have a degenerate distribution, $p(x)=\mathbb{I}(x=1)$, where $\mathbb{I}()$ is the binary indicator function. Thus the distribution in Figure 2.1(b) represents the fact that $X$ is always equal to the value 1. (Thus we see that random variables can also be constant.)

\title{
2.2.2 Continuous random variables
}

If $X \in \mathbb{R}$ is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with $X$ being in each one of these intervals, we can use the methods discussed above for discrete random variables. Informally speaking, we can represent the probability of $X$ taking on a specific real value by allowing the size of the intervals to shrink to zero, as we show below.

\subsection*{2.2.2.1 Cumulative distribution function (cdf)}

Define the events $A=(X \leq a), B=(X \leq b)$ and $C=(a<X \leq b)$, where $a<b$. We have that $B=A \vee C$, and since $A$ and $C$ are mutually exclusive, the sum rules gives

$$
\operatorname{Pr}(B)=\operatorname{Pr}(A)+\operatorname{Pr}(C)
$$

and hence the probability of being in interval $C$ is given by

$$
\operatorname{Pr}(C)=\operatorname{Pr}(B)-\operatorname{Pr}(A)
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_4a0eadb9c3250516aa8dg-1.jpg?height=441&width=518&top_left_y=208&top_left_x=404

ChatGPT figure/image summary: The image shows a histogram representing the distribution of sample means of a random variable drawn from a Beta(1,5) distribution, with the number of draws (N) for the sample mean calculation being 1. This histogram is part of a demonstration of the central limit theorem, illustrating that as the sample size increases, the distribution of the sample means tends toward a Gaussian (normal) distribution. In this particular case (N=1), the histogram shows the frequency of the outcomes on the vertical axis and the sample mean values on the horizontal axis. This is Figure 2.23 labeled with "(a)" in the provided context, indicating that it is the first of a set of similar histograms corresponding to different values of N.)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_13_4a0eadb9c3250516aa8dg-1.jpg?height=441&width=517&top_left_y=208&top_left_x=1119

ChatGPT figure/image summary: The image is a histogram representing the central limit theorem. It illustrates the distribution trend for sample means \(\hat{\mu}_{N}^{s}\) where each \(x_{n s}\) is drawn from a Beta(1,5) distribution. This specific histogram, labeled (b) with \(N=5\), shows the distribution for the sample means when five samples (\(N=5\)) have been averaged together (\(N_{\mathcal{D}}\) times) for a total of 10,000 times (\(s=1:10000\)). As the number of samples used to calculate the mean increases, the distribution of these means will approach a Gaussian distribution, in accordance with the central limit theorem demonstrated in Figure 2.23 of the referenced academic work. This graph is part of a demonstration to visualize the concept.)

(b)

Figure 2.23: The central limit theorem in pictures. We plot a histogram of $\hat{\mu}_{N}^{s}=\frac{1}{N_{\mathcal{D}}} \sum_{n=1}^{N_{\mathcal{D}}} x_{n s}$, where $x_{n s} \sim \operatorname{Beta}(1,5)$, for $s=1: 10000$. As $N_{\mathcal{D}} \rightarrow \infty$, the distribution tends towards a Gaussian. (a) $N=1$. (b) $N=5$. Adapted from Figure 2.6 of [Bis06]. Generated by centralLimitDemo.ipynb.

Hence the distribution of the quantity

$$
Z_{N_{\mathcal{D}}} \triangleq \frac{S_{N_{\mathcal{D}}}-N_{\mathcal{D}} \mu}{\sigma \sqrt{N_{\mathcal{D}}}}=\frac{\bar{X}-\mu}{\sigma / \sqrt{N_{\mathcal{D}}}}
$$

converges to the standard normal, where $\bar{X}=S_{N} / N$ is the sample mean. This is called the central limit theorem. See e.g., [Jay03, p222] or [Ric95, p169] for a proof.

In Figure 2.23 we give an example in which we compute the sample mean of rv's drawn from a beta distribution. We see that the sampling distribution of this mean rapidly converges to a Gaussian distribution.

\title{
2.8.7 Monte Carlo approximation
}

Suppose $\boldsymbol{x}$ is a random variable, and $\boldsymbol{y}=f(\boldsymbol{x})$ is some function of $\boldsymbol{x}$. It is often difficult to compute the induced distribution $p(\boldsymbol{y})$ analytically. One simple but powerful alternative is to draw a large number of samples from the $\boldsymbol{x}$ 's distribution, and then to use these samples (instead of the distribution) to approximate $p(\boldsymbol{y})$.

For example, suppose $x \sim \operatorname{Unif}(-1,1)$ and $y=f(x)=x^{2}$. We can approximate $p(y)$ by drawing many samples from $p(x)$ (using a uniform random number generator), squaring them, and computing the resulting empirical distribution, which is given by

$$
p_{S}(y) \triangleq \frac{1}{N_{s}} \sum_{s=1}^{N_{s}} \delta\left(y-y_{s}\right)
$$

This is just an equally weighted "sum of spikes", each centered on one of the samples (see Section 2.7.6). By using enough samples, we can approximate $p(y)$ rather well. See Figure 2.24 for an illustration.

This approach is called a Monte Carlo approximation to the distribution. (The term "Monte Carlo" comes from the name of a famous gambling casino in Monaco.) Monte Carlo techniques were

![](https://cdn.mathpix.com/cropped/2024_06_13_e1c4fa23ad624dcfc447g-1.jpg?height=472&width=1202&top_left_y=223&top_left_x=409

ChatGPT figure/image summary: The first image presented appears to be a cropped screenshot from a mathematical or statistical paper, discussing the central limit theorem. It is likely to show figures or plots demonstrating the central limit theorem in action, though the image itself is not visible based on the context provided.

The second image (a), corresponding to the label "(a)" in the text, would be expected to display a histogram representing the distribution of sample means \(\hat{\mu}_{N}^{s}\) for \( N=1 \) where each \( x_{n s} \) is sampled from a Beta distribution, specifically \( \text{Beta}(1,5) \).

The second image (b), corresponding to the label "(b)" in the text, would typically show the same concept as (a) but for \( N=5 \). The histograms in both images are intended to demonstrate that as the number of samples \( N_{\mathcal{D}} \) grows, the distribution of the sample mean approaches a Gaussian distribution due to the central limit theorem.

The third image (Figure 2.24) is an illustration of the Monte Carlo approximation. It should contain three plots: The leftmost plot probably displays the uniform distribution \( p(x) \) of \( x \) which is the input to the function \( y=f(x)=x^2 \). The middle plot would be the analytic result of the distribution \( p(y) \) of \( y \), which is the square of \( x \) where \( x \) is uniformly distributed between -1 and 1. And the rightmost plot is likely the Monte Carlo approximation of the distribution of \( y \), showing the empirical distribution obtained from squaring many samples drawn from the uniform distribution of \( x \).

However, I am not able to infer the specific contents of the images themselves without visual access to them.)

Figure 2.24: Computing the distribution of $y=x^{2}$, where $p(x)$ is uniform (left). The analytic result is shown in the middle, and the Monte Carlo approximation is shown on the right. Generated by change_of_vars_demo1d.ipynb.

first developed in the area of statistical physics - in particular, during development of the atomic bomb - but are now widely used in statistics and machine learning as well. More details can be found in the sequel to this book, [Mur23], as well as specialized books on the topic, such as [Liu01; RC04; KTB11; BZ20].

\title{
2.9 Exercises
}

Exercise 2.1 [Conditional independence $*]$

(Source: Koller.)

a. Let $H \in\{1, \ldots, K\}$ be a discrete random variable, and let $e_{1}$ and $e_{2}$ be the observed values of two other random variables $E_{1}$ and $E_{2}$. Suppose we wish to calculate the vector

$$
\vec{P}\left(H \mid e_{1}, e_{2}\right)=\left(P\left(H=1 \mid e_{1}, e_{2}\right), \ldots, P\left(H=K \mid e_{1}, e_{2}\right)\right)
$$

Which of the following sets of numbers are sufficient for the calculation?

$$
\begin{aligned}
& \text { i. } P\left(e_{1}, e_{2}\right), P(H), P\left(e_{1} \mid H\right), P\left(e_{2} \mid H\right) \\
& \text { ii. } P\left(e_{1}, e_{2}\right), P(H), P\left(e_{1}, e_{2} \mid H\right) \\
& \text { ii. } P\left(e_{1} \mid H\right), P\left(e_{2} \mid H\right), P(H)
\end{aligned}
$$

b. Now suppose we now assume $E_{1} \perp E_{2} \mid H$ (i.e., $E_{1}$ and $E_{2}$ are conditionally independent given $H$ ). Which of the above 3 sets are sufficient now?

Show your calculations as well as giving the final result. Hint: use Bayes rule.

Exercise 2.2 [Pairwise independence does not imply mutual independence]

We say that two random variables are pairwise independent if

$$
p\left(X_{2} \mid X_{1}\right)=p\left(X_{2}\right)
$$

and hence

$$
p\left(X_{2}, X_{1}\right)=p\left(X_{1}\right) p\left(X_{2} \mid X_{1}\right)=p\left(X_{1}\right) p\left(X_{2}\right)
$$

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

![](https://cdn.mathpix.com/cropped/2024_06_13_4e374e7a642c7281c32cg-1.jpg?height=447&width=1266&top_left_y=200&top_left_x=367

ChatGPT figure/image summary: The image shows two plots related to the standard normal distribution, often denoted by \(\mathcal{N}(0,1)\).

On the left, labeled as (a), there is a plot of the cumulative distribution function (cdf) for the standard normal distribution. The x-axis represents the variable \(x\) ranging from approximately -3 to 3, and the y-axis represents the cumulative probability from 0 to 1. The curve smoothly increases from left to right, illustrating the probability that a random variable \(X\) drawn from the distribution will have a value less than or equal to \(x\).

On the right, labeled as (b), we have a plot of the probability density function (pdf) corresponding to the same standard normal distribution. The x-axis covers the same range, while the y-axis represents the density of the probability. The characteristic bell-shaped curve is shown, peaking at 0 where the mean of the distribution is located. The shaded areas under the curve at both tails represent \(\alpha / 2\), corresponding to the tails of the distribution; these areas are symmetric around the mean. Thus, the unshaded region under the curve represents the probability mass of \(1 - \alpha\), which in the context provided would typically be the range within which the majority of the distribution's values fall (often used in confidence interval calculations).

These plots are standard tools in statistics to illustrate the distribution's properties and are likely found in a chapter discussing normal distributions and related concepts such as cdf, pdf, and quantiles.)

Figure 2.2: (a) Plot of the cdf for the standard normal, $\mathcal{N}(0,1)$. Generated by gauss_plot.ipynb. (b) Corresponding pdf. The shaded regions each contain $\alpha / 2$ of the probability mass. Therefore the nonshaded region contains $1-\alpha$ of the probability mass. The leftmost cutoff point is $\Phi^{-1}(\alpha / 2)$, where $\Phi$ is the cdf of the Gaussian. By symmetry, the rightmost cutoff point is $\Phi^{-1}(1-\alpha / 2)=-\Phi^{-1}(\alpha / 2)$. Generated by quantile_plot.ipynb.

In general, we define the cumulative distribution function or cdf of the rv $X$ as follows:

$$
P(x) \triangleq \operatorname{Pr}(X \leq x)
$$

(Note that we use a capital $P$ to represent the cdf.) Using this, we can compute the probability of being in any interval as follows:

$$
\operatorname{Pr}(a<X \leq b)=P(b)-P(a)
$$

Cdf's are monotonically non-decreasing functions. See Figure 2.2a for an example, where we illustrate the cdf of a standard normal distribution, $\mathcal{N}(x \mid 0,1)$; see Section 2.6 for details.

\title{
2.2.2.2 Probability density function (pdf)
}

We define the probability density function or pdf as the derivative of the cdf:

$$
p(x) \triangleq \frac{d}{d x} P(x)
$$

(Note that this derivative does not always exist, in which case the pdf is not defined.) See Figure $2.2 \mathrm{~b}$ for an example, where we illustrate the pdf of a univariate Gaussian (see Section 2.6 for details).

Given a pdf, we can compute the probability of a continuous variable being in a finite interval as follows:

$$
\operatorname{Pr}(a<X \leq b)=\int_{a}^{b} p(x) d x=P(b)-P(a)
$$

As the size of the interval gets smaller, we can write

$$
\operatorname{Pr}(x<X \leq x+d x) \approx p(x) d x
$$

Intuitively, this says the probability of $X$ being in a small interval around $x$ is the density at $x$ times the width of the interval.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

\title{
2.2.2.3 Quantiles
}

If the cdf $P$ is strictly monotonically increasing, it has an inverse, called the inverse cdf, or percent point function (ppf), or quantile function.

If $P$ is the cdf of $X$, then $P^{-1}(q)$ is the value $x_{q}$ such that $\operatorname{Pr}\left(X \leq x_{q}\right)=q$; this is called the $q^{\prime}$ th quantile of $P$. The value $P^{-1}(0.5)$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P^{-1}(0.25)$ and $P^{-1}(0.75)$ are the lower and upper quartiles.

For example, let $\Phi$ be the cdf of the Gaussian distribution $\mathcal{N}(0,1)$, and $\Phi^{-1}$ be the inverse cdf. Then points to the left of $\Phi^{-1}(\alpha / 2)$ contain $\alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $\Phi^{-1}(1-\alpha / 2)$ also contain $\alpha / 2$ of the mass. Hence the central interval $\left(\Phi^{-1}(\alpha / 2), \Phi^{-1}(1-\alpha / 2)\right)$ contains $1-\alpha$ of the mass. If we set $\alpha=0.05$, the central $95 \%$ interval is covered by the range

$$
\left(\Phi^{-1}(0.025), \Phi^{-1}(0.975)\right)=(-1.96,1.96)
$$

If the distribution is $\mathcal{N}\left(\mu, \sigma^{2}\right)$, then the $95 \%$ interval becomes $(\mu-1.96 \sigma, \mu+1.96 \sigma)$. This is often approximated by writing $\mu \pm 2 \sigma$.

\subsection*{2.2.3 Sets of related random variables}

In this section, we discuss distributions over sets of related random variables.

Suppose, to start, that we have two random variables, $X$ and $Y$. We can define the joint distribution of two random variables using $p(x, y)=p(X=x, Y=y)$ for all possible values of $X$ and $Y$. If both variables have finite cardinality, we can represent the joint distribution as a $2 \mathrm{~d}$ table, all of whose entries sum to one. For example, consider the following example with two binary variables:

$$
\begin{array}{l|ll}
p(X, Y) & Y=0 & Y=1 \\
\hline X=0 & 0.2 & 0.3 \\
X=1 & 0.3 & 0.2
\end{array}
$$

If two variables are independent, we can represent the joint as the product of the two marginals. If both variables have finite cardinality, we can factorize the $2 \mathrm{~d}$ joint table into a product of two $1 \mathrm{~d}$ vectors, as shown in Figure 2.3.

Given a joint distribution, we define the marginal distribution of an rv as follows:

$$
p(X=x)=\sum_{y} p(X=x, Y=y)
$$

where we are summing over all possible states of $Y$. This is sometimes called the sum rule or the rule of total probability. We define $p(Y=y)$ similarly. For example, from the above $2 \mathrm{~d}$ table, we see $p(X=0)=0.2+0.3=0.5$ and $p(Y=0)=0.2+0.3=0.5$. (The term "marginal" comes from the accounting practice of writing the sums of rows and columns on the side, or margin, of a table.)

We define the conditional distribution of an rv using

$$
p(Y=y \mid X=x)=\frac{p(X=x, Y=y)}{p(X=x)}
$$

We can rearrange this equation to get

$$
p(x, y)=p(x) p(y \mid x)
$$

Draft of "Probabilistic Machine Learning: An Introduction". August 8, 2022

![](https://cdn.mathpix.com/cropped/2024_06_13_647790ec99d4643bfdd1g-1.jpg?height=380&width=510&top_left_y=198&top_left_x=755

ChatGPT figure/image summary: The image depicts a graphical representation of the concept where two random variables, \(X\) and \(Y\), are unconditionally (or marginally) independent. The larger grid labeled \(P(X, Y)\) represents the joint probability distribution of \(X\) and \(Y\). Here, \(X\) has 6 possible states (values), and \(Y\) has 5 possible states, which can be inferred from the grid dimensions.

On the right side of the equality, two smaller grids labeled \(P(X)\) and \(P(Y)\) represent the marginal probability distributions of \(X\) and \(Y\) respectively. These marginal distributions are obtained by summing over the rows for \(P(Y)\) and over the columns for \(P(X)\) in the joint probability distribution. The figure illustrates that the joint distribution can be factorized into the product of the two marginal distributions, showing that the variables are independent. This is a visualization of the product rule of probability for independent random variables, which states that \(p(x, y) = p(x) p(y)\) when \(X \perp Y\).)

Figure 2.3: Computing $p(x, y)=p(x) p(y)$, where $X \perp Y$. Here $X$ and $Y$ are discrete random variables; $X$ has 6 possible states (values) and $Y$ has 5 possible states. A general joint distribution on two such variables would require $(6 \times 5)-1=29$ parameters to define it (we subtract 1 because of the sum-to-one constraint). By assuming (unconditional) independence, we only need $(6-1)+(5-1)=9$ parameters to define $p(x, y)$.

This is called the product rule.

By extending the product rule to $D$ variables, we get the chain rule of probability:

$$
p\left(\boldsymbol{x}_{1: D}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{1}, x_{2}\right) p\left(x_{4} \mid x_{1}, x_{2}, x_{3}\right) \ldots p\left(x_{D} \mid \boldsymbol{x}_{1: D-1}\right)
$$

This provides a way to create a high dimensional joint distribution from a set of conditional distributions. We discuss this in more detail in Section 3.6.

\title{
2.2.4 Independence and conditional independence
}

We say $X$ and $Y$ are unconditionally independent or marginally independent, denoted $X \perp Y$, if we can represent the joint as the product of the two marginals (see Figure 2.3), i.e.,

$$
X \perp Y \Longleftrightarrow p(X, Y)=p(X) p(Y)
$$

In general, we say a set of variables $X_{1}, \ldots, X_{n}$ is (mutually) independent if the joint can be written as a product of marginals for all subsets $\left\{X_{1}, \ldots, X_{m}\right\} \subseteq\left\{X_{1}, \ldots, X_{n}\right\}$ : i.e.,

$$
p\left(X_{1}, \ldots, X_{m}\right)=\prod_{i=1}^{m} p\left(X_{i}\right)
$$

For example, we say $X_{1}, X_{2}, X_{3}$ are mutually independent if the following conditions hold: $p\left(X_{1}, X_{2}, X_{3}\right)=$ $p\left(X_{1}\right) p\left(X_{2}\right) p\left(X_{3}\right), p\left(X_{1}, X_{2}\right)=p\left(X_{1}\right) p\left(X_{2}\right), p\left(X_{2}, X_{3}\right)=p\left(X_{2}\right) p\left(X_{3}\right)$, and $p\left(X_{1}, X_{3}\right)=p\left(X_{1}\right) p\left(X_{3}\right) .{ }^{2}$

Unfortunately, unconditional independence is rare, because most variables can influence most other variables. However, usually this influence is mediated via other variables rather than being direct. We therefore say $X$ and $Y$ are conditionally independent (CI) given $Z$ iff the conditional joint can be written as a product of conditional marginals:

$$
X \perp Y \mid Z \Longleftrightarrow p(X, Y \mid Z)=p(X \mid Z) p(Y \mid Z)
$$

2. For further discussion, see https://github.com/probml/pml-book/issues/353\#issuecomment-1120327442.

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license

We can write this assumption as a graph $X-Z-Y$, which captures the intuition that all the dependencies between $X$ and $Y$ are mediated via $Z$. By using larger graphs, we can define complex joint distributions; these are known as graphical models, and are discussed in Section 3.6.

\title{
2.2.5 Moments of a distribution
}

In this section, we describe various summary statistics that can be derived from a probability distribution (either a pdf or pmf).

\subsection*{2.2.5.1 Mean of a distribution}

The most familiar property of a distribution is its mean, or expected value, often denoted by $\mu$. For continuous rv's, the mean is defined as follows:

$$
\mathbb{E}[X] \triangleq \int_{\mathcal{X}} x p(x) d x
$$

If the integral is not finite, the mean is not defined; we will see some examples of this later.

For discrete rv's, the mean is defined as follows:

$$
\mathbb{E}[X] \triangleq \sum_{x \in \mathcal{X}} x p(x)
$$

However, this is only meaningful if the values of $x$ are ordered in some way (e.g., if they represent integer counts).

Since the mean is a linear operator, we have

$$
\mathbb{E}[a X+b]=a \mathbb{E}[X]+b
$$

This is called the linearity of expectation.

For a set of $n$ random variables, one can show that the expectation of their sum is as follows:

$$
\mathbb{E}\left[\sum_{i=1}^{n} X_{i}\right]=\sum_{i=1}^{n} \mathbb{E}\left[X_{i}\right]
$$

If they are independent, the expectation of their product is given by

$$
\mathbb{E}\left[\prod_{i=1}^{n} X_{i}\right]=\prod_{i=1}^{n} \mathbb{E}\left[X_{i}\right]
$$

\subsection*{2.2.5.2 Variance of a distribution}

The variance is a measure of the "spread" of a distribution, often denoted by $\sigma^{2}$. This is defined as follows:

$$
\begin{aligned}
\mathbb{V}[X] & \triangleq \mathbb{E}\left[(X-\mu)^{2}\right]=\int(x-\mu)^{2} p(x) d x \\
& =\int x^{2} p(x) d x+\mu^{2} \int p(x) d x-2 \mu \int x p(x) d x=\mathbb{E}\left[X^{2}\right]-\mu^{2}
\end{aligned}
$$

from which we derive the useful result

$$
\mathbb{E}\left[X^{2}\right]=\sigma^{2}+\mu^{2}
$$

The standard deviation is defined as

$$
\operatorname{std}[X] \triangleq \sqrt{\mathbb{V}[X]}=\sigma
$$

This is useful since it has the same units as $X$ itself.

The variance of a shifted and scaled version of a random variable is given by

$$
\mathbb{V}[a X+b]=a^{2} \mathbb{V}[X]
$$

If we have a set of $n$ independent random variables, the variance of their sum is given by the sum of their variances:

$$
\mathbb{V}\left[\sum_{i=1}^{n} X_{i}\right]=\sum_{i=1}^{n} \mathbb{V}\left[X_{i}\right]
$$

The variance of their product can also be derived, as follows:

$$
\begin{aligned}
\mathbb{V}\left[\prod_{i=1}^{n} X_{i}\right] & =\mathbb{E}\left[\left(\prod_{i} X_{i}\right)^{2}\right]-\left(\mathbb{E}\left[\prod_{i} X_{i}\right]\right)^{2} \\
& =\mathbb{E}\left[\prod_{i} X_{i}^{2}\right]-\left(\prod_{i} \mathbb{E}\left[X_{i}\right]\right)^{2} \\
& =\prod_{i} \mathbb{E}\left[X_{i}^{2}\right]-\prod_{i}\left(\mathbb{E}\left[X_{i}\right]\right)^{2} \\
& =\prod_{i}\left(\mathbb{V}\left[X_{i}\right]+\left(\mathbb{E}\left[X_{i}\right]\right)^{2}\right)-\prod_{i}\left(\mathbb{E}\left[X_{i}\right]\right)^{2} \\
& =\prod_{i}\left(\sigma_{i}^{2}+\mu_{i}^{2}\right)-\prod_{i} \mu_{i}^{2}
\end{aligned}
$$

\title{
2.2.5.3 Mode of a distribution
}

The mode of a distribution is the value with the highest probability mass or probability density:

$$
\boldsymbol{x}^{*}=\underset{\boldsymbol{x}}{\operatorname{argmax}} p(\boldsymbol{x})
$$

If the distribution is multimodal, this may not be unique, as illustrated in Figure 2.4. Furthermore, even if there is a unique mode, this point may not be a good summary of the distribution.

\subsection*{2.2.5.4 Conditional moments}

When we have two or more dependent random variables, we can compute the moments of one given knowledge of the other. For example, the law of iterated expectations, also called the law of total expectation, tells us that

$$
\mathbb{E}[X]=\mathbb{E}_{Y}[\mathbb{E}[X \mid Y]]
$$

Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license


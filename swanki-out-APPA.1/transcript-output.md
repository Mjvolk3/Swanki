## Appendix A: Linear Algebra

In this appendix, I delve into some essential properties and identities involving matrices and determinants. This isn't a beginner's tutorial; instead, it's aimed at those who already have a grounding in basic linear algebra. For some results, I'll indicate how to prove them, whereas more complex cases are referenced to standard textbooks on the subject. We assume that matrix inverses exist and that the matrix dimensions are such that the formulas are correctly defined. For a comprehensive discussion, you can refer to Golub and Van Loan's work (1996), and for an extensive collection of matrix properties, Lütkepohl’s 1996 publication is a great resource. Matrix derivatives are discussed in detail by Magnus and Neudecker (1999).

### A.1. Matrix Identities

Let's start with the basics. A matrix, let’s call it \(A\), has elements \(A_{ij}\), where \(i\) indexes the rows and \(j\) indexes the columns. The identity matrix of size \(N\) by \(N\) is denoted by \(I_N\), and if there's no ambiguity about its size, we simply use \(I\). The transpose of matrix \(A\), denoted \(A^T\), switches its rows and columns, meaning the element at row \(i\) and column \(j\) of \(A^T\) is the element at row \(j\) and column \(i\) of \(A\). This gives us the property \((A \cdot B)^T = B^T \cdot A^T\), which can be verified by writing out the indices.

The inverse of matrix \(A\), denoted \(A^{-1}\), satisfies the equation \(A \cdot A^{-1} = A^{-1} \cdot A = I\). This means that when you multiply a matrix by its inverse, you get the identity matrix. Another important property is that \((A \cdot B)^{-1} = B^{-1} \cdot A^{-1}\). Taking the transpose of the inverse of a matrix gives us \((A^T)^{-1} = (A^{-1})^T\). This can be proven by taking the transpose of the product \(A \cdot A^{-1}\) and applying the previously mentioned transpose property.

One useful identity involving matrix inverses is \((P^{-1} + B^T \cdot R^{-1} \cdot B)^{-1} \cdot B^T \cdot R^{-1} = P \cdot B^T \cdot (B \cdot P \cdot B^T + R)^{-1}\). Suppose \(P\) is an \(N \times N\) matrix, and \(R\) is an \(M \times M\) matrix, making \(B\) an \(M \times N\) matrix. If \(M\) is much smaller than \(N\), it is computationally cheaper to evaluate the right-hand side of this identity than the left-hand side. 

Additionally, a special case that sometimes arises is \((I + A \cdot B)^{-1} \cdot A = A \cdot (I + B \cdot A)^{-1}\). Another useful inverse identity is the Woodbury identity, which states \((A + B \cdot D^{-1} \cdot C)^{-1} = A^{-1} - A^{-1} \cdot B \cdot (D + C \cdot A^{-1} \cdot B)^{-1} \cdot C \cdot A^{-1}\). This is particularly useful when \(A\) is large and diagonal (hence easier to invert), and when \(B\) has many rows but few columns, making the right-hand side of the equation much cheaper to evaluate.

In linear algebra, a set of vectors is said to be linearly independent if the relation \(\sum \alpha_n a_n = 0\) holds only if all \(\alpha_n = 0\). This implies that none of the vectors in the set can be expressed as a linear combination of the others. The rank of a matrix is the maximum number of linearly independent rows or, equivalently, the maximum number of linearly independent columns.

### A.2. Traces and Determinants

Moving on to square matrices, these have traces and determinants. The trace of a matrix \(A\), denoted \(\operatorname{Tr}(A)\), is the sum of the elements on the leading diagonal. By writing out the indices, we see that \(\operatorname{Tr}(A \cdot B) = \operatorname{Tr}(B \cdot A)\). This property, known as the cyclic property of the trace operator, extends to the product of any number of matrices, making it a very powerful tool in linear algebra.

The determinant of an \(N \times N\) matrix \(A\), denoted \(|A|\), is defined as the sum of products of elements, with each product containing precisely one element from each row and each column. The sign of each product is positive or negative according to whether the permutation of the indices is even or odd, respectively. For a simple \(2 \times 2\) matrix, the determinant is calculated as \(a_{11} \cdot a_{22} - a_{12} \cdot a_{21}\).

One important property of determinants is that the determinant of a product of two matrices is the product of their determinants, \(|A \cdot B| = |A| \cdot |B|\). The determinant of the inverse of a matrix is given by \(|A^{-1}| = 1 / |A|\). 

If \(A\) and \(B\) are matrices of size \(N \times M\), then \(|I_N + A \cdot B^T| = |I_M + A^T \cdot B|\). A useful special case is when \(A\) and \(B\) are column vectors, in which case the determinant simplifies to \(1 + a^T \cdot b\).

### A.3. Matrix Derivatives

Sometimes we need to consider derivatives of vectors and matrices with respect to scalars. The derivative of a vector \(a\) with respect to a scalar \(x\) is a vector whose components are given by the partial derivatives of each component of \(a\) with respect to \(x\). Similarly, the derivative of a matrix with respect to a scalar is a matrix of the partial derivatives of each element of the matrix with respect to the scalar.

Derivatives with respect to vectors and matrices can also be defined. For example, the derivative of a scalar \(x\) with respect to a vector \(a\) is a vector whose \(i\)th component is the partial derivative of \(x\) with respect to the \(i\)th component of \(a\). The derivative of a vector \(a\) with respect to another vector \(b\) is a matrix whose \((i, j)\) element is the partial derivative of the \(i\)th component of \(a\) with respect to the \(j\)th component of \(b\).

One useful result is that the derivative of the product of a row vector \(a^T\) and a column vector \(x\) with respect to \(x\) is the row vector \(a\). Similarly, the derivative of the product of two matrices \(A\) and \(B\) with respect to a scalar \(x\) is given by the product of the derivatives of each matrix with respect to \(x\).

The derivative of the inverse of a matrix \(A\) with respect to a scalar \(x\) is given by \(-A^{-1} \cdot \partial A / \partial x \cdot A^{-1}\). This can be shown by differentiating the equation \(A^{-1} \cdot A = I\) and then right-multiplying by \(A^{-1}\). Additionally, the derivative of the logarithm of the determinant of a matrix \(A\) with respect to a scalar \(x\) is given by the trace of the product of the inverse of \(A\) and the derivative of \(A\) with respect to \(x\).

### A.4. Eigenvectors

For a square matrix \(A\) of size \(M \times M\), the eigenvector equation is defined by \(A \cdot u_i = \lambda_i \cdot u_i\) for \(i = 1, \ldots, M\), where \(u_i\) is an eigenvector and \(\lambda_i\) is the corresponding eigenvalue. This can be viewed as a set of \(M\) simultaneous homogeneous linear equations, and the condition for a solution is that \(|A - \lambda_i \cdot I| = 0\), known as the characteristic equation. This polynomial equation of order \(M\) in \(\lambda_i\) must have \(M\) solutions, though they need not all be distinct. The rank of \(A\) is equal to the number of non-zero eigenvalues.

Symmetric matrices, which appear frequently as covariance matrices, kernel matrices, and Hessians, have the property that their elements are symmetric, meaning \(A_{ij} = A_{ji}\) or equivalently \(A^T = A\). The inverse of a symmetric matrix is also symmetric. The eigenvalues of a symmetric matrix are real numbers, which can be shown by considering the complex conjugate of the eigenvector equation and using the symmetry property of the matrix.

The eigenvectors of a real symmetric matrix can be chosen to be orthonormal. This means that the eigenvectors are orthogonal and normalized to unit length, which can be shown by manipulating the eigenvector equations and using the properties of symmetric matrices.
Let's begin by discussing the relationship between eigenvalues and eigenvectors, focusing on the orthogonality property. When we have two different eigenvalues, say lambda_i and lambda_j, the corresponding eigenvectors u_i and u_j are orthogonal. This orthogonality condition is expressed by the equation: the dot product of u_i and u_j equals zero. Essentially, if the eigenvalues are distinct, the eigenvectors point in completely different directions.

Now, if the eigenvalues are the same, things get a bit more interesting. Any linear combination of the eigenvectors associated with that eigenvalue is also an eigenvector. This means we can choose one combination arbitrarily and the second one to be orthogonal to the first. Importantly, even if the eigenvalues are the same, the eigenvectors are never linearly dependent—meaning one cannot be expressed as a multiple of the other. This orthogonality and normalization process allows us to form a complete set of M orthogonal eigenvectors, which means any M-dimensional vector can be expressed as a combination of these eigenvectors.

Next, let's consider the matrix U, which has these orthogonal eigenvectors as its columns. This matrix U satisfies the condition that when you multiply U by its transpose, you get an identity matrix. This property defines U as an orthogonal matrix. Moreover, the rows of this matrix are also orthogonal, which implies that multiplying U by its transpose from the other side also gives an identity matrix. This orthonormality is crucial for many applications, as it ensures that the transformations we perform using U preserve lengths and angles.

When we express the eigenvector equation in matrix form, we have matrix A multiplied by U equals U multiplied by a diagonal matrix Lambda. This diagonal matrix Lambda contains the eigenvalues on its diagonal. If we transform a vector x by this orthogonal matrix U, the length of the vector remains unchanged. This preservation of length and the angles between vectors means that multiplication by U can be interpreted as a rigid rotation of the coordinate system. This is a powerful concept in linear algebra, as it allows for transformations without distorting the underlying geometry of the space.

Finally, let's talk about the diagonalization process of matrix A. When we multiply A by U and then by the transpose of U, we get the diagonal matrix Lambda. This means U diagonalizes A. If we left-multiply by U and right-multiply by the transpose of U, we can reconstitute A from its eigenvalues and eigenvectors. Similarly, the inverse of A can be expressed by inverting its eigenvalues. The determinant of A is the product of its eigenvalues, while the trace of A, which is the sum of its diagonal elements, is the sum of its eigenvalues.

Regarding matrix properties, a matrix is positive definite if, for any non-zero vector w, the quadratic form w transpose A w is greater than zero. This condition is equivalent to all eigenvalues being positive. For instance, a matrix with all positive elements is not necessarily positive definite. A matrix is positive semidefinite if the quadratic form is non-negative, meaning all eigenvalues are non-negative. The condition number of a matrix, which measures its sensitivity to numerical operations, is defined as the square root of the ratio of its largest to smallest eigenvalue. This gives a sense of the matrix's stability and robustness in calculations.
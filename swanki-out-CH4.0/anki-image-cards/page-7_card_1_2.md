## How is the least-squares regression function geometrically obtained in an N-dimensional space?

![](https://cdn.mathpix.com/cropped/2024_05_26_4d03a03b9a49734662f9g-1.jpg?height=367&width=543&top_left_y=217&top_left_x=1106)

%

The least-squares regression function is obtained by finding the orthogonal projection of the data vector $\mathbf{t}$ onto the subspace spanned by the basis functions $\phi_{j}(\mathbf{x})$. Each basis function is viewed as a vector $\varphi_{j}$ in an N-dimensional space. This projection results in the vector $\mathbf{y}$ which represents the least-squares solution.

- #geometry, #regression.least-squares

## In the context of a least-squares solution, what do the vectors $\varphi_{j}$ and $\mathbf{t}$ represent in the N-dimensional space?

![](https://cdn.mathpix.com/cropped/2024_05_26_4d03a03b9a49734662f9g-1.jpg?height=367&width=543&top_left_y=217&top_left_x=1106)

%

In the N-dimensional space, the vector $\mathbf{t}$ represents the target values with components $t_{1}, \ldots, t_{N}$, while each vector $\varphi_{j}$ represents a basis function $\phi_{j}\left(\mathbf{x}_{n}\right)$ evaluated at the $N$ data points. The vector $\varphi_{j}$ corresponds to the $j^\text{th}$ column of the matrix $\boldsymbol{\Phi}$.

- #geometry, #regression.least-squares, #linear-algebra.dimensions